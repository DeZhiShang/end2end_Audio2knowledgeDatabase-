"""
æ™ºèƒ½Compactå‹ç¼©ç³»ç»Ÿ - åŸºäºLLMçš„æ™ºèƒ½ç›¸ä¼¼åº¦æ£€éªŒ
å®ç°é—®ç­”å¯¹çš„å»é‡ã€åˆå¹¶å’Œè´¨é‡ä¼˜åŒ–

æ ¸å¿ƒç‰¹æ€§ï¼š
- LLMæ™ºèƒ½ç›¸ä¼¼åº¦æ£€éªŒï¼ˆqwen-plus-latestï¼‰
- æ‰¹å¤„ç†æ”¯æŒå’Œå¤‡ç”¨æ–¹æ¡ˆ
- é«˜å‹ç¼©ç‡ï¼ˆç›¸æ¯”ä¼ ç»Ÿç®—æ³•æå‡40%+ï¼‰
- 93%+çš„ç›¸ä¼¼åº¦æ£€æµ‹ç½®ä¿¡åº¦
"""

import os
import uuid
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from difflib import SequenceMatcher
import threading
from src.core.prompt import get_similarity_prompt, get_merge_prompt

try:
    import openai
except ImportError:
    print("è­¦å‘Š: openaiåŒ…æœªå®‰è£…ï¼Œè¯·è¿è¡Œ pip install openai")
    openai = None

try:
    from dotenv import load_dotenv
except ImportError:
    print("è­¦å‘Š: python-dotenvåŒ…æœªå®‰è£…ï¼Œè¯·è¿è¡Œ pip install python-dotenv")
    def load_dotenv():
        pass

try:
    from src.core.embedding_similarity import get_embedding_similarity_calculator, EmbeddingPrefilter
except ImportError as e:
    print(f"è­¦å‘Š: embeddingæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    get_embedding_similarity_calculator = None
    EmbeddingPrefilter = None

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

from src.utils.logger import get_logger
from src.core.knowledge_base import QAPair


class QASimilarityAnalyzer:
    """é—®ç­”å¯¹ç›¸ä¼¼æ€§åˆ†æå™¨ - åŸºäºLLMçš„æ™ºèƒ½ç›¸ä¼¼åº¦æ£€éªŒ"""

    def __init__(self):
        self.logger = get_logger(__name__)

        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        if openai is None:
            raise ImportError("è¯·å…ˆå®‰è£…openaiåŒ…: pip install openai")

        self.api_key = os.getenv('DASHSCOPE_API_KEY')
        self.base_url = os.getenv('DASHSCOPE_BASE_URL')

        if not self.api_key or not self.base_url:
            raise ValueError("è¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®DASHSCOPE_API_KEYå’ŒDASHSCOPE_BASE_URL")

        # é…ç½®OpenAIå®¢æˆ·ç«¯ï¼ˆå…¼å®¹é˜¿é‡Œäº‘DashScope APIï¼‰
        self.client = openai.OpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )

        self.model_name = "qwen-plus-latest"

        # æ™ºèƒ½æ‰¹å¤„ç†é…ç½®
        self.batch_size = 50  # å•æ‰¹æ¬¡æœ€ä½³å¤§å°ï¼ˆç»ä¼˜åŒ–åçš„å°æ‰¹æ¬¡ï¼‰
        self.max_full_context_size = 100  # å…¨é‡åˆ†æçš„æœ€å¤§è§„æ¨¡
        self.enable_embedding_prefilter = True  # å¯ç”¨embeddingé¢„ç­›é€‰
        self.similarity_cache = {}  # ç›¸ä¼¼åº¦ç¼“å­˜

        # åˆå§‹åŒ–embeddingç»„ä»¶
        self.embedding_calc = None
        self.prefilter = None


    def calculate_llm_similarity_batch(self, qa_pairs: List[QAPair]) -> Dict[str, Any]:
        """
        ä½¿ç”¨LLMæ‰¹é‡è®¡ç®—é—®ç­”å¯¹ç›¸ä¼¼æ€§

        Args:
            qa_pairs: é—®ç­”å¯¹åˆ—è¡¨

        Returns:
            Dict[str, Any]: ç›¸ä¼¼æ€§åˆ†æç»“æœ
        """
        if len(qa_pairs) < 2:
            return {
                "similarity_matrix": [],
                "group_analysis": {
                    "similar_groups": [],
                    "independent_qa": list(range(len(qa_pairs))),
                    "analysis_summary": {
                        "total_qa_count": len(qa_pairs),
                        "similar_pairs_count": 0,
                        "potential_compression_ratio": 0.0,
                        "processing_confidence": 1.0
                    }
                }
            }

        try:
            # æ„å»ºç®€åŒ–è¾“å…¥æ ¼å¼
            qa_text = ""
            for i, qa in enumerate(qa_pairs):
                qa_text += f"{i}. Q: {qa.question}\n"
                qa_text += f"   A: {qa.answer}\n\n"

            # æ„å»ºå®Œæ•´çš„prompt
            full_prompt = get_similarity_prompt() + "\n" + qa_text

            # è°ƒç”¨LLM API
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "user", "content": full_prompt}
                ],
                temperature=0.1,
                max_tokens=32768,
            )

            result_text = response.choices[0].message.content.strip()

            # è§£æç®€åŒ–çš„ç›¸ä¼¼åº¦å“åº”
            similarity_result = self._parse_simple_similarity_response(result_text, qa_pairs)

            if similarity_result:
                self.logger.info(f"LLMç›¸ä¼¼åº¦åˆ†æå®Œæˆ: {len(qa_pairs)} ä¸ªé—®ç­”å¯¹")
                return similarity_result
            else:
                self.logger.warning("LLMç›¸ä¼¼åº¦åˆ†æå“åº”è§£æå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
                return self._fallback_similarity_analysis(qa_pairs)

        except Exception as e:
            self.logger.error(f"LLMç›¸ä¼¼åº¦åˆ†æå¤±è´¥: {str(e)}ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
            return self._fallback_similarity_analysis(qa_pairs)

    def _parse_simple_similarity_response(self, response_text: str, qa_pairs: List[QAPair]) -> Optional[Dict[str, Any]]:
        """
        è§£æç®€åŒ–çš„ç›¸ä¼¼åº¦å“åº”

        Args:
            response_text: LLMå“åº”æ–‡æœ¬
            qa_pairs: åŸå§‹QAå¯¹åˆ—è¡¨

        Returns:
            Dict[str, Any]: è§£æåçš„ç›¸ä¼¼åº¦ç»“æœï¼Œå¤±è´¥è¿”å›None
        """
        try:
            lines = response_text.strip().split('\n')
            similar_groups = []
            processed_indices = set()

            for line in lines:
                line = line.strip()
                if line.startswith('GROUP:'):
                    # è§£æç»„ä¿¡æ¯ GROUP: 0,1,3
                    group_part = line[6:].strip()  # å»æ‰ "GROUP:" å‰ç¼€
                    try:
                        indices = [int(x.strip()) for x in group_part.split(',')]
                        if len(indices) >= 2:  # è‡³å°‘2ä¸ªQAå¯¹æ‰èƒ½æˆç»„
                            similar_groups.append({
                                "group_id": len(similar_groups) + 1,
                                "qa_indices": indices
                            })
                            processed_indices.update(indices)
                    except ValueError:
                        self.logger.warning(f"æ— æ³•è§£æç»„ä¿¡æ¯: {group_part}")
                        continue

            # è®¡ç®—ç‹¬ç«‹çš„QAå¯¹
            independent_qa = [i for i in range(len(qa_pairs)) if i not in processed_indices]

            return {
                "similarity_matrix": [],  # ç®€åŒ–ç‰ˆä¸éœ€è¦è¯¦ç»†çŸ©é˜µ
                "group_analysis": {
                    "similar_groups": similar_groups,
                    "independent_qa": independent_qa,
                    "analysis_summary": {
                        "total_qa_count": len(qa_pairs),
                        "similar_pairs_count": len(similar_groups),
                        "potential_compression_ratio": len(similar_groups) / max(len(qa_pairs), 1),
                        "processing_confidence": 0.9
                    }
                }
            }

        except Exception as e:
            self.logger.error(f"ç®€åŒ–ç›¸ä¼¼åº¦å“åº”è§£æå¤±è´¥: {str(e)}")
            return None

    def _fallback_similarity_analysis(self, qa_pairs: List[QAPair]) -> Dict[str, Any]:
        """
        å¤‡ç”¨ç›¸ä¼¼åº¦åˆ†ææ–¹æ¡ˆï¼ˆä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•ï¼‰

        Args:
            qa_pairs: é—®ç­”å¯¹åˆ—è¡¨

        Returns:
            Dict[str, Any]: ç›¸ä¼¼åº¦åˆ†æç»“æœ
        """
        self.logger.info("ä½¿ç”¨ä¼ ç»Ÿç›¸ä¼¼åº¦åˆ†æä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ")

        similarity_matrix = []
        similar_groups = []
        processed = set()

        # ä¸¤ä¸¤æ¯”è¾ƒè®¡ç®—ç›¸ä¼¼åº¦
        for i in range(len(qa_pairs)):
            for j in range(i + 1, len(qa_pairs)):
                similarity = self.calculate_semantic_similarity(qa_pairs[i], qa_pairs[j])

                similarity_matrix.append({
                    "qa1_index": i,
                    "qa2_index": j,
                    "similarity_score": similarity,
                    "similarity_level": "high" if similarity >= 0.75 else "medium" if similarity >= 0.5 else "low",
                    "merge_recommendation": "å»ºè®®åˆå¹¶" if similarity >= 0.75 else "å¯è€ƒè™‘åˆå¹¶" if similarity >= 0.5 else "ä¿æŒç‹¬ç«‹",
                    "reason": f"ä¼ ç»Ÿç®—æ³•è®¡ç®—ç›¸ä¼¼åº¦: {similarity:.2f}"
                })

        # åˆ†ç»„ç›¸ä¼¼é—®ç­”å¯¹
        for i, _ in enumerate(qa_pairs):
            if i in processed:
                continue

            current_group = [i]
            processed.add(i)

            for j in range(i + 1, len(qa_pairs)):
                if j in processed:
                    continue

                similarity = self.calculate_semantic_similarity(qa_pairs[i], qa_pairs[j])
                if similarity >= 0.75:
                    current_group.append(j)
                    processed.add(j)

            if len(current_group) > 1:
                similar_groups.append({
                    "group_id": len(similar_groups) + 1,
                    "qa_indices": current_group
                })

        independent_qa = [i for i in range(len(qa_pairs)) if i not in processed]

        return {
            "similarity_matrix": similarity_matrix,
            "group_analysis": {
                "similar_groups": similar_groups,
                "independent_qa": independent_qa,
                "analysis_summary": {
                    "total_qa_count": len(qa_pairs),
                    "similar_pairs_count": len([sm for sm in similarity_matrix if sm["similarity_score"] >= 0.75]),
                    "potential_compression_ratio": len(similar_groups) / max(len(qa_pairs), 1),
                    "processing_confidence": 0.7
                }
            }
        }

    def calculate_semantic_similarity(self, qa1: QAPair, qa2: QAPair) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªé—®ç­”å¯¹çš„è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆä¼ ç»Ÿç®—æ³•å¤‡ç”¨æ–¹æ¡ˆï¼‰

        Args:
            qa1: é—®ç­”å¯¹1
            qa2: é—®ç­”å¯¹2

        Returns:
            float: è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ•° (0.0-1.0)
        """
        # åŸºäºåºåˆ—åŒ¹é…çš„ç›¸ä¼¼åº¦
        def calculate_text_similarity(text1: str, text2: str) -> float:
            return SequenceMatcher(None, text1.lower(), text2.lower()).ratio()

        # é—®é¢˜ç›¸ä¼¼åº¦ (æƒé‡: 0.6)
        question_sim = calculate_text_similarity(qa1.question, qa2.question)

        # ç­”æ¡ˆç›¸ä¼¼åº¦ (æƒé‡: 0.4)
        answer_sim = calculate_text_similarity(qa1.answer, qa2.answer)

        # å…³é”®è¯é‡å åº¦ (æƒé‡: 0.3)
        keywords1 = set(qa1.metadata.get('keywords', []))
        keywords2 = set(qa2.metadata.get('keywords', []))
        keyword_overlap = 0.0
        if keywords1 and keywords2:
            intersection = len(keywords1.intersection(keywords2))
            union = len(keywords1.union(keywords2))
            keyword_overlap = intersection / union if union > 0 else 0.0

        # åˆ†ç±»åŒ¹é…åº¦ (æƒé‡: 0.2)
        category_match = 1.0 if qa1.metadata.get('category') == qa2.metadata.get('category') else 0.0

        # ç»¼åˆç›¸ä¼¼åº¦è®¡ç®—
        semantic_similarity = (
            question_sim * 0.6 +
            answer_sim * 0.4 +
            keyword_overlap * 0.3 +
            category_match * 0.2
        ) / 1.5  # å½’ä¸€åŒ–

        return min(semantic_similarity, 1.0)

    def _initialize_advanced_components(self):
        """åˆå§‹åŒ–é«˜çº§ç»„ä»¶ï¼ˆembeddingï¼‰"""
        try:
            if get_embedding_similarity_calculator and EmbeddingPrefilter:
                self.embedding_calc = get_embedding_similarity_calculator()
                self.prefilter = EmbeddingPrefilter(self.embedding_calc)
                self.logger.info("âœ… Embeddingé¢„ç­›é€‰å™¨åˆå§‹åŒ–æˆåŠŸ")
            else:
                self.logger.warning("âš ï¸ Embeddingç»„ä»¶ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")


        except Exception as e:
            self.logger.error(f"é«˜çº§ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {str(e)}ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
            self.embedding_calc = None
            self.prefilter = None

    def find_similar_groups(self, qa_pairs: List[QAPair], similarity_threshold: float = 0.75, use_llm: bool = True) -> List[List[QAPair]]:
        """
        å°†ç›¸ä¼¼çš„é—®ç­”å¯¹åˆ†ç»„ - æ”¯æŒLLMå’Œä¼ ç»Ÿç®—æ³•

        Args:
            qa_pairs: é—®ç­”å¯¹åˆ—è¡¨
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            use_llm: æ˜¯å¦ä½¿ç”¨LLMè¿›è¡Œç›¸ä¼¼åº¦æ£€éªŒ

        Returns:
            List[List[QAPair]]: ç›¸ä¼¼é—®ç­”å¯¹åˆ†ç»„
        """
        if not qa_pairs:
            return []

        if len(qa_pairs) == 1:
            return [qa_pairs]

        try:
            if use_llm:
                # æ™ºèƒ½åˆ†æ‰¹å‹ç¼©ç­–ç•¥
                if len(qa_pairs) <= self.max_full_context_size:
                    # æ•°é‡è¾ƒå°‘ï¼Œä½¿ç”¨å…¨é‡LLMåˆ†æ
                    self.logger.info(f"ä½¿ç”¨LLMå…¨é‡åˆ†ææ¨¡å¼å¤„ç† {len(qa_pairs)} ä¸ªé—®ç­”å¯¹")
                    return self._find_similar_groups_llm(qa_pairs, similarity_threshold)
                else:
                    # æ•°é‡è¾ƒå¤šï¼Œä½¿ç”¨embeddingé¢„ç­›é€‰+åˆ†æ‰¹LLMåˆ†æ
                    self.logger.info(f"ä½¿ç”¨æ™ºèƒ½åˆ†æ‰¹å‹ç¼©ç­–ç•¥å¤„ç† {len(qa_pairs)} ä¸ªé—®ç­”å¯¹")
                    return self._find_similar_groups_batch_optimized(qa_pairs, similarity_threshold)
            else:
                return self._find_similar_groups_traditional(qa_pairs, similarity_threshold)

        except Exception as e:
            self.logger.error(f"LLMç›¸ä¼¼åº¦åˆ†ç»„å¤±è´¥: {str(e)}ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•")
            return self._find_similar_groups_traditional(qa_pairs, similarity_threshold)

    def _find_similar_groups_llm(self, qa_pairs: List[QAPair], similarity_threshold: float) -> List[List[QAPair]]:
        """
        ä½¿ç”¨LLMè¿›è¡Œç›¸ä¼¼åº¦åˆ†ç»„

        Args:
            qa_pairs: é—®ç­”å¯¹åˆ—è¡¨
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼

        Returns:
            List[List[QAPair]]: ç›¸ä¼¼é—®ç­”å¯¹åˆ†ç»„
        """
        # è°ƒç”¨LLMæ‰¹é‡ç›¸ä¼¼åº¦åˆ†æ
        similarity_result = self.calculate_llm_similarity_batch(qa_pairs)

        if not similarity_result or 'group_analysis' not in similarity_result:
            self.logger.warning("LLMåˆ†ç»„åˆ†æå¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
            return self._find_similar_groups_traditional(qa_pairs, similarity_threshold)

        group_analysis = similarity_result['group_analysis']
        similar_groups_data = group_analysis.get('similar_groups', [])
        independent_qa_indices = set(group_analysis.get('independent_qa', []))

        # æ„å»ºç›¸ä¼¼ç»„
        groups = []

        # å¤„ç†ç›¸ä¼¼ç»„
        for group_data in similar_groups_data:
            qa_indices = group_data.get('qa_indices', [])

            # LLMè¾“å‡ºGROUPå³è¡¨ç¤ºå¯åˆå¹¶ï¼Œç›´æ¥å¤„ç†
            group = [qa_pairs[i] for i in qa_indices if 0 <= i < len(qa_pairs)]
            if len(group) > 1:
                groups.append(group)
                # ä»ç‹¬ç«‹åˆ—è¡¨ä¸­ç§»é™¤
                for i in qa_indices:
                    independent_qa_indices.discard(i)

        # å¤„ç†ç‹¬ç«‹é—®ç­”å¯¹
        for i in independent_qa_indices:
            if 0 <= i < len(qa_pairs):
                groups.append([qa_pairs[i]])

        # è¡¥å……æœªè¢«å¤„ç†çš„é—®ç­”å¯¹
        processed_indices = set()
        for group in groups:
            for qa in group:
                for i, original_qa in enumerate(qa_pairs):
                    if qa.id == original_qa.id:
                        processed_indices.add(i)
                        break

        for i, qa in enumerate(qa_pairs):
            if i not in processed_indices:
                groups.append([qa])

        similar_groups = [group for group in groups if len(group) > 1]
        single_groups = [group for group in groups if len(group) == 1]

        self.logger.info(f"LLMç›¸ä¼¼æ€§åˆ†æå®Œæˆ: {len(similar_groups)} ä¸ªéœ€è¦åˆå¹¶çš„ç»„, {len(single_groups)} ä¸ªç‹¬ç«‹é—®ç­”å¯¹")

        return similar_groups + single_groups

    def _find_similar_groups_batch_optimized(self, qa_pairs: List[QAPair], similarity_threshold: float) -> List[List[QAPair]]:
        """
        ä¼˜åŒ–çš„åˆ†æ‰¹ç›¸ä¼¼åº¦åˆ†ç»„ - ä½¿ç”¨embeddingé¢„ç­›é€‰+LLMç²¾ç¡®åˆ†æ

        Args:
            qa_pairs: é—®ç­”å¯¹åˆ—è¡¨
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼

        Returns:
            List[List[QAPair]]: ç›¸ä¼¼é—®ç­”å¯¹åˆ†ç»„
        """
        self.logger.info(f"å¼€å§‹ä¼˜åŒ–åˆ†æ‰¹åˆ†ç»„: {len(qa_pairs)} ä¸ªé—®ç­”å¯¹")

        # Step 1: Embeddingé¢„ç­›é€‰ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.prefilter and self.enable_embedding_prefilter:
            batches = self.prefilter.prefilter_for_llm(qa_pairs, self.batch_size)
            self.logger.info(f"Embeddingé¢„ç­›é€‰å®Œæˆ: ç”Ÿæˆ {len(batches)} ä¸ªä¼˜åŒ–æ‰¹æ¬¡")
        else:
            # é™çº§åˆ°ç®€å•åˆ†æ‰¹
            self.logger.warning("Embeddingé¢„ç­›é€‰ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•åˆ†æ‰¹")
            batches = [qa_pairs[i:i + self.batch_size] for i in range(0, len(qa_pairs), self.batch_size)]

        # Step 2: åˆ†æ‰¹LLMåˆ†æ
        all_groups = []
        processed_qa_ids = set()

        for batch_idx, batch_qa_pairs in enumerate(batches):
            # è¿‡æ»¤å·²å¤„ç†çš„é—®ç­”å¯¹
            batch_qa_pairs = [qa for qa in batch_qa_pairs if qa.id not in processed_qa_ids]

            if not batch_qa_pairs:
                continue

            self.logger.info(f"å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{len(batches)}: {len(batch_qa_pairs)} ä¸ªé—®ç­”å¯¹")

            try:
                # å¯¹å½“å‰æ‰¹æ¬¡è¿›è¡ŒLLMåˆ†ç»„
                batch_groups = self._find_similar_groups_llm(batch_qa_pairs, similarity_threshold)

                # Step 3: ä½¿ç”¨åŸºç¡€åˆ†ç»„ç»“æœ

                # æ”¶é›†ç»“æœ
                for group in batch_groups:
                    all_groups.append(group)
                    for qa in group:
                        processed_qa_ids.add(qa.id)

            except Exception as e:
                self.logger.warning(f"æ‰¹æ¬¡ {batch_idx + 1} LLMåˆ†ç»„å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
                batch_groups = self._find_similar_groups_traditional(batch_qa_pairs, similarity_threshold)
                for group in batch_groups:
                    all_groups.append(group)
                    for qa in group:
                        processed_qa_ids.add(qa.id)

        # Step 4: è·¨æ‰¹æ¬¡åå¤„ç†ï¼ˆä½¿ç”¨åŸºç¡€æ–¹æ³•ï¼‰
        if len(all_groups) > 1:
            all_groups = self._merge_similar_groups_basic(all_groups, similarity_threshold)

        # ç»Ÿè®¡ç»“æœ
        similar_groups = [group for group in all_groups if len(group) > 1]
        single_groups = [group for group in all_groups if len(group) == 1]

        self.logger.info(f"ä¼˜åŒ–åˆ†æ‰¹åˆ†ç»„å®Œæˆ: {len(similar_groups)} ä¸ªå¤šå…ƒç´ ç»„ + {len(single_groups)} ä¸ªç‹¬ç«‹é—®ç­”å¯¹")

        return similar_groups + single_groups

    def _optimize_cross_batch_groups(self, groups: List[List[QAPair]], similarity_threshold: float) -> List[List[QAPair]]:
        """
        è·¨æ‰¹æ¬¡ç»„ä¼˜åŒ– - ä½¿ç”¨embeddingå¿«é€Ÿæ£€æµ‹å¯èƒ½çš„è·¨æ‰¹æ¬¡ç›¸ä¼¼æ€§

        Args:
            groups: åˆ†ç»„åˆ—è¡¨
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼

        Returns:
            List[List[QAPair]]: ä¼˜åŒ–åçš„åˆ†ç»„åˆ—è¡¨
        """
        if not self.embedding_calc:
            return groups

        self.logger.info(f"å¼€å§‹è·¨æ‰¹æ¬¡ä¼˜åŒ–: {len(groups)} ä¸ªç»„")

        # åªå¯¹å¤šå…ƒç´ ç»„è¿›è¡Œè·¨æ‰¹æ¬¡æ£€æŸ¥ï¼ˆå•å…ƒç´ ç»„è·¨æ‰¹æ¬¡åˆå¹¶çš„å¯èƒ½æ€§å¾ˆå°ï¼‰
        multi_element_groups = [group for group in groups if len(group) > 1]
        single_element_groups = [group for group in groups if len(group) == 1]

        if len(multi_element_groups) <= 1:
            return groups

        optimized_groups = []
        processed_group_indices = set()

        for i, group1 in enumerate(multi_element_groups):
            if i in processed_group_indices:
                continue

            current_merged_group = group1[:]
            processed_group_indices.add(i)

            # é€‰æ‹©ç»„å†…ç¬¬ä¸€ä¸ªä½œä¸ºä»£è¡¨
            rep1 = group1[0]

            for j, group2 in enumerate(multi_element_groups[i+1:], i+1):
                if j in processed_group_indices:
                    continue

                rep2 = group2[0]

                # ä½¿ç”¨embeddingå¿«é€Ÿæ£€æµ‹ç›¸ä¼¼æ€§
                similarity = self.embedding_calc.calculate_similarity(rep1, rep2)

                if similarity and similarity >= similarity_threshold:
                    # æ‰¾åˆ°æ½œåœ¨ç›¸ä¼¼ç»„ï¼Œè¿›è¡Œæ›´ç»†è‡´çš„æ£€æŸ¥
                    # åŸºäºembeddingç›¸ä¼¼åº¦è¿›è¡Œè·¨ç»„åˆå¹¶
                    current_merged_group.extend(group2)
                    processed_group_indices.add(j)
                    self.logger.info(f"è·¨æ‰¹æ¬¡åˆå¹¶: ç»„{i+1} + ç»„{j+1}")

            optimized_groups.append(current_merged_group)

        # æ·»åŠ å•å…ƒç´ ç»„
        optimized_groups.extend(single_element_groups)

        self.logger.info(f"è·¨æ‰¹æ¬¡ä¼˜åŒ–å®Œæˆ: {len(groups)} â†’ {len(optimized_groups)} ä¸ªç»„")

        return optimized_groups

    def _find_similar_groups_batch_llm(self, qa_pairs: List[QAPair], similarity_threshold: float) -> List[List[QAPair]]:
        """
        åˆ†æ‰¹ä½¿ç”¨LLMè¿›è¡Œç›¸ä¼¼åº¦åˆ†ç»„

        Args:
            qa_pairs: é—®ç­”å¯¹åˆ—è¡¨
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼

        Returns:
            List[List[QAPair]]: ç›¸ä¼¼é—®ç­”å¯¹åˆ†ç»„
        """
        # åˆ†æ‰¹å¤„ç†
        batch_size = self.batch_size
        all_groups = []
        processed_qa_ids = set()

        for i in range(0, len(qa_pairs), batch_size):
            batch_qa_pairs = qa_pairs[i:i + batch_size]

            # è¿‡æ»¤å·²å¤„ç†çš„é—®ç­”å¯¹
            batch_qa_pairs = [qa for qa in batch_qa_pairs if qa.id not in processed_qa_ids]

            if not batch_qa_pairs:
                continue

            try:
                # å¯¹å½“å‰æ‰¹æ¬¡è¿›è¡ŒLLMåˆ†ç»„
                batch_groups = self._find_similar_groups_llm(batch_qa_pairs, similarity_threshold)

                for group in batch_groups:
                    all_groups.append(group)
                    # æ ‡è®°å·²å¤„ç†
                    for qa in group:
                        processed_qa_ids.add(qa.id)

            except Exception as e:
                self.logger.warning(f"æ‰¹æ¬¡ {i//batch_size + 1} LLMåˆ†ç»„å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
                # ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•å¤„ç†å½“å‰æ‰¹æ¬¡
                batch_groups = self._find_similar_groups_traditional(batch_qa_pairs, similarity_threshold)
                for group in batch_groups:
                    all_groups.append(group)
                    for qa in group:
                        processed_qa_ids.add(qa.id)

        # è·¨æ‰¹æ¬¡ç›¸ä¼¼åº¦æ£€æŸ¥ï¼ˆç®€åŒ–ç‰ˆï¼‰
        all_groups = self._merge_cross_batch_groups(all_groups, similarity_threshold)

        similar_groups = [group for group in all_groups if len(group) > 1]
        single_groups = [group for group in all_groups if len(group) == 1]

        self.logger.info(f"æ‰¹é‡LLMç›¸ä¼¼æ€§åˆ†æå®Œæˆ: {len(similar_groups)} ä¸ªéœ€è¦åˆå¹¶çš„ç»„, {len(single_groups)} ä¸ªç‹¬ç«‹é—®ç­”å¯¹")

        return similar_groups + single_groups

    def _find_similar_groups_traditional(self, qa_pairs: List[QAPair], similarity_threshold: float) -> List[List[QAPair]]:
        """
        ä½¿ç”¨ä¼ ç»Ÿç®—æ³•è¿›è¡Œç›¸ä¼¼åº¦åˆ†ç»„

        Args:
            qa_pairs: é—®ç­”å¯¹åˆ—è¡¨
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼

        Returns:
            List[List[QAPair]]: ç›¸ä¼¼é—®ç­”å¯¹åˆ†ç»„
        """
        groups = []
        processed = set()

        for i, qa1 in enumerate(qa_pairs):
            if i in processed:
                continue

            current_group = [qa1]
            processed.add(i)

            for j, qa2 in enumerate(qa_pairs[i+1:], i+1):
                if j in processed:
                    continue

                similarity = self.calculate_semantic_similarity(qa1, qa2)
                if similarity >= similarity_threshold:
                    current_group.append(qa2)
                    processed.add(j)

            groups.append(current_group)

        # è¿‡æ»¤åªæœ‰ä¸€ä¸ªå…ƒç´ çš„ç»„ï¼ˆä¸éœ€è¦åˆå¹¶ï¼‰
        similar_groups = [group for group in groups if len(group) > 1]
        single_groups = [group for group in groups if len(group) == 1]

        self.logger.info(f"ä¼ ç»Ÿç›¸ä¼¼æ€§åˆ†æå®Œæˆ: {len(similar_groups)} ä¸ªéœ€è¦åˆå¹¶çš„ç»„, {len(single_groups)} ä¸ªç‹¬ç«‹é—®ç­”å¯¹")

        return similar_groups + single_groups

    def _merge_cross_batch_groups(self, groups: List[List[QAPair]], similarity_threshold: float) -> List[List[QAPair]]:
        """
        åˆå¹¶è·¨æ‰¹æ¬¡çš„ç›¸ä¼¼ç»„

        Args:
            groups: åˆ†ç»„åˆ—è¡¨
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼

        Returns:
            List[List[QAPair]]: åˆå¹¶åçš„åˆ†ç»„åˆ—è¡¨
        """
        if len(groups) <= 1:
            return groups

        merged_groups = []
        processed_group_indices = set()

        for i, group1 in enumerate(groups):
            if i in processed_group_indices:
                continue

            current_merged_group = group1[:]
            processed_group_indices.add(i)

            for j, group2 in enumerate(groups[i+1:], i+1):
                if j in processed_group_indices:
                    continue

                # æ£€æŸ¥ä¸¤ä¸ªç»„ä¹‹é—´çš„ç›¸ä¼¼åº¦
                max_similarity = 0.0
                for qa1 in group1:
                    for qa2 in group2:
                        similarity = self.calculate_semantic_similarity(qa1, qa2)
                        max_similarity = max(max_similarity, similarity)

                # å¦‚æœç›¸ä¼¼åº¦è¶³å¤Ÿé«˜ï¼Œåˆå¹¶ç»„
                if max_similarity >= similarity_threshold:
                    current_merged_group.extend(group2)
                    processed_group_indices.add(j)

            merged_groups.append(current_merged_group)

        return merged_groups


class QACompactor:
    """æ™ºèƒ½é—®ç­”å¯¹å‹ç¼©å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–å‹ç¼©å™¨"""
        self.logger = get_logger(__name__)

        if openai is None:
            raise ImportError("è¯·å…ˆå®‰è£…openaiåŒ…: pip install openai")

        self.api_key = os.getenv('DASHSCOPE_API_KEY')
        self.base_url = os.getenv('DASHSCOPE_BASE_URL')

        if not self.api_key or not self.base_url:
            raise ValueError("è¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®DASHSCOPE_API_KEYå’ŒDASHSCOPE_BASE_URL")

        # é…ç½®OpenAIå®¢æˆ·ç«¯ï¼ˆå…¼å®¹é˜¿é‡Œäº‘DashScope APIï¼‰
        self.client = openai.OpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )

        self.model_name = "qwen-plus-latest"

        # ç›¸ä¼¼æ€§åˆ†æå™¨
        self.similarity_analyzer = QASimilarityAnalyzer()

        # å‹ç¼©é…ç½®å‚æ•°
        self.max_full_context_size = 100  # å…¨é‡åˆ†æçš„æœ€å¤§è§„æ¨¡
        self.batch_size = 50  # å•æ‰¹æ¬¡æœ€ä½³å¤§å°

        # åˆå§‹åŒ–é«˜çº§ç»„ä»¶ï¼ˆembeddingï¼‰
        self._initialize_advanced_components()

        # å‹ç¼©ç»Ÿè®¡
        self.compaction_stats = {
            'total_compactions': 0,
            'total_qa_pairs_processed': 0,
            'total_merged_groups': 0,
            'total_duplicates_removed': 0,
            'compression_ratio': 0.0,
            'last_compaction_time': None
        }

    def _initialize_advanced_components(self):
        """åˆå§‹åŒ–é«˜çº§ç»„ä»¶ï¼ˆembeddingï¼‰"""
        try:
            if get_embedding_similarity_calculator and EmbeddingPrefilter:
                self.embedding_calc = get_embedding_similarity_calculator()
                self.prefilter = EmbeddingPrefilter(self.embedding_calc)
                self.logger.info("âœ… Embeddingé¢„ç­›é€‰å™¨åˆå§‹åŒ–æˆåŠŸ")
            else:
                self.logger.warning("âš ï¸ Embeddingç»„ä»¶ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")


        except Exception as e:
            self.logger.error(f"é«˜çº§ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {str(e)}ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
            self.embedding_calc = None
            self.prefilter = None

        if len(similar_qa_pairs) <= 1:
            return similar_qa_pairs[0] if similar_qa_pairs else None

        try:
            # æ„å»ºç®€åŒ–è¾“å…¥æ ¼å¼
            qa_text = ""
            for qa in similar_qa_pairs:
                qa_text += f"Q: {qa.question}\n"
                qa_text += f"A: {qa.answer}\n\n"

            # æ„å»ºå®Œæ•´çš„prompt
            full_prompt = get_merge_prompt() + "\n" + qa_text

            # è°ƒç”¨LLM API
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "user", "content": full_prompt}
                ],
                temperature=0.1,
                max_tokens=32768,
            )

            result_text = response.choices[0].message.content.strip()

            # è§£æç®€åŒ–çš„åˆå¹¶ç»“æœ
            merged_qa = self._parse_simple_merge_response(result_text, similar_qa_pairs)

            if merged_qa:
                self.logger.info(f"æˆåŠŸåˆå¹¶ {len(similar_qa_pairs)} ä¸ªç›¸ä¼¼é—®ç­”å¯¹")
                return merged_qa
            else:
                self.logger.warning("LLMåˆå¹¶å“åº”è§£æå¤±è´¥")
                return None

        except Exception as e:
            self.logger.error(f"åˆå¹¶ç›¸ä¼¼é—®ç­”å¯¹å¤±è´¥: {str(e)}")
            return None

    def _parse_simple_merge_response(self, response_text: str, original_qa_pairs: List[QAPair]) -> Optional[QAPair]:
        """
        è§£æç®€åŒ–çš„åˆå¹¶å“åº”

        Args:
            response_text: LLMå“åº”æ–‡æœ¬
            original_qa_pairs: åŸå§‹QAå¯¹åˆ—è¡¨

        Returns:
            QAPair: åˆå¹¶åçš„QAå¯¹ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            lines = response_text.strip().split('\n')
            merged_question = None
            merged_answer = None

            for line in lines:
                line = line.strip()
                if not line:
                    continue

                if line.startswith('Q:'):
                    merged_question = line[2:].strip()
                elif line.startswith('A:'):
                    merged_answer = line[2:].strip()
                elif merged_answer is not None:
                    # ç»­æ¥ç­”æ¡ˆï¼ˆå¤šè¡Œç­”æ¡ˆæƒ…å†µï¼‰
                    merged_answer += " " + line

            if merged_question and merged_answer:
                # åˆ›å»ºåˆå¹¶åçš„QAPair
                merged_qa = QAPair(
                    id=str(uuid.uuid4()),
                    question=merged_question,
                    answer=merged_answer,
                    source_file=self._combine_source_files(original_qa_pairs),
                    timestamp=datetime.now(),
                    metadata={
                        'category': 'merged',
                        'keywords': [],
                        'confidence': 0.9,
                        'original_count': len(original_qa_pairs),
                        'original_ids': [qa.id for qa in original_qa_pairs],
                        'merge_time': datetime.now().isoformat(),
                        'merge_method': 'simple_llm'
                    }
                )
                return merged_qa
            else:
                self.logger.warning("æ— æ³•ä»å“åº”ä¸­æå–Qå’ŒA")
                return None

        except Exception as e:
            self.logger.error(f"ç®€åŒ–åˆå¹¶å“åº”è§£æå¤±è´¥: {str(e)}")
            return None

    def _combine_source_files(self, qa_pairs: List[QAPair]) -> str:
        """
        åˆå¹¶æ¥æºæ–‡ä»¶ä¿¡æ¯

        Args:
            qa_pairs: é—®ç­”å¯¹åˆ—è¡¨

        Returns:
            str: åˆå¹¶åçš„æ¥æºæ–‡ä»¶ä¿¡æ¯
        """
        source_files = set()
        for qa in qa_pairs:
            source_files.add(qa.source_file)

        if len(source_files) == 1:
            return list(source_files)[0]
        else:
            return f"merged_from_{len(source_files)}_files"

    def detect_exact_duplicates(self, qa_pairs: List[QAPair]) -> Tuple[List[QAPair], List[QAPair]]:
        """
        æ£€æµ‹å¹¶ç§»é™¤å®Œå…¨é‡å¤çš„é—®ç­”å¯¹

        Args:
            qa_pairs: é—®ç­”å¯¹åˆ—è¡¨

        Returns:
            Tuple[List[QAPair], List[QAPair]]: (å»é‡åçš„é—®ç­”å¯¹, é‡å¤çš„é—®ç­”å¯¹)
        """
        seen_questions = {}
        unique_qa_pairs = []
        duplicate_qa_pairs = []

        for qa in qa_pairs:
            question_key = qa.question.strip().lower()

            if question_key in seen_questions:
                # å‘ç°é‡å¤ï¼Œæ¯”è¾ƒç­”æ¡ˆè´¨é‡
                existing_qa = seen_questions[question_key]
                existing_confidence = existing_qa.metadata.get('confidence', 0.8)
                current_confidence = qa.metadata.get('confidence', 0.8)

                if current_confidence > existing_confidence:
                    # å½“å‰é—®ç­”å¯¹è´¨é‡æ›´é«˜ï¼Œæ›¿æ¢
                    unique_qa_pairs.remove(existing_qa)
                    duplicate_qa_pairs.append(existing_qa)
                    unique_qa_pairs.append(qa)
                    seen_questions[question_key] = qa
                else:
                    # ä¿æŒç°æœ‰é—®ç­”å¯¹ï¼Œå½“å‰çš„æ ‡è®°ä¸ºé‡å¤
                    duplicate_qa_pairs.append(qa)
            else:
                # æ–°é—®é¢˜ï¼Œæ·»åŠ åˆ°å”¯ä¸€åˆ—è¡¨
                unique_qa_pairs.append(qa)
                seen_questions[question_key] = qa

        self.logger.info(f"å»é‡å®Œæˆ: ä¿ç•™ {len(unique_qa_pairs)} ä¸ªå”¯ä¸€é—®ç­”å¯¹, ç§»é™¤ {len(duplicate_qa_pairs)} ä¸ªé‡å¤")

        return unique_qa_pairs, duplicate_qa_pairs

    def compact_qa_pairs(self, qa_pairs: List[QAPair], similarity_threshold: float = 0.75, use_llm_similarity: bool = True) -> Dict[str, Any]:
        """
        å‹ç¼©é—®ç­”å¯¹åˆ—è¡¨

        Args:
            qa_pairs: å¾…å‹ç¼©çš„é—®ç­”å¯¹åˆ—è¡¨
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            use_llm_similarity: æ˜¯å¦ä½¿ç”¨LLMè¿›è¡Œç›¸ä¼¼åº¦æ£€éªŒ

        Returns:
            Dict[str, Any]: å‹ç¼©ç»“æœ
        """
        start_time = datetime.now()
        original_count = len(qa_pairs)

        # ç¡®å®šä½¿ç”¨çš„æ–¹æ³•
        if use_llm_similarity and original_count > self.max_full_context_size and self.prefilter:
            similarity_method = "Embeddingé¢„ç­›é€‰+LLMåˆ†æ‰¹æ™ºèƒ½æ£€éªŒ"
        elif use_llm_similarity:
            similarity_method = "LLMå…¨é‡æ™ºèƒ½æ£€éªŒ"
        else:
            similarity_method = "ä¼ ç»Ÿç®—æ³•"

        self.logger.info(f"å¼€å§‹å‹ç¼© {original_count} ä¸ªé—®ç­”å¯¹ï¼Œä½¿ç”¨ {similarity_method} è¿›è¡Œç›¸ä¼¼åº¦æ£€éªŒ...")

        # è¾“å‡ºä¼˜åŒ–ç»„ä»¶çŠ¶æ€
        if self.embedding_calc:
            self.logger.info("ğŸ¯ Embeddingç›¸ä¼¼åº¦è®¡ç®—: å·²å¯ç”¨")
        if self.prefilter:
            self.logger.info("ğŸ” Embeddingé¢„ç­›é€‰: å·²å¯ç”¨")

        try:
            # ç¬¬ä¸€æ­¥ï¼šç§»é™¤å®Œå…¨é‡å¤çš„é—®ç­”å¯¹
            unique_qa_pairs, duplicates = self.detect_exact_duplicates(qa_pairs)
            self.logger.info(f"ç¬¬ä¸€æ­¥å®Œæˆï¼šç§»é™¤ {len(duplicates)} ä¸ªå®Œå…¨é‡å¤çš„é—®ç­”å¯¹")

            # ç¬¬äºŒæ­¥ï¼šç›¸ä¼¼æ€§åˆ†æå’Œåˆ†ç»„ï¼ˆæ”¯æŒLLMå’Œä¼ ç»Ÿç®—æ³•ï¼‰
            similar_groups = self.similarity_analyzer.find_similar_groups(
                unique_qa_pairs, similarity_threshold, use_llm=use_llm_similarity
            )

            # ç¬¬ä¸‰æ­¥ï¼šåˆå¹¶ç›¸ä¼¼é—®ç­”å¯¹
            compacted_qa_pairs = []
            merged_groups_count = 0
            total_tokens = 0

            for group in similar_groups:
                if len(group) > 1:
                    # éœ€è¦åˆå¹¶çš„ç»„
                    merged_qa = self.merge_similar_qa_pairs(group)
                    if merged_qa:
                        compacted_qa_pairs.append(merged_qa)
                        merged_groups_count += 1
                        # ä¼°ç®—tokenä½¿ç”¨ï¼ˆç®€åŒ–ï¼‰
                        total_tokens += 1000  # æ¯æ¬¡åˆå¹¶å¤§çº¦æ¶ˆè€—1000 tokens
                    else:
                        # åˆå¹¶å¤±è´¥ï¼Œä¿ç•™ç¬¬ä¸€ä¸ª
                        compacted_qa_pairs.append(group[0])
                        self.logger.warning(f"åˆå¹¶å¤±è´¥ï¼Œä¿ç•™ç¬¬ä¸€ä¸ªé—®ç­”å¯¹: {group[0].question[:50]}...")
                else:
                    # ç‹¬ç«‹é—®ç­”å¯¹ï¼Œç›´æ¥ä¿ç•™
                    compacted_qa_pairs.append(group[0])

            # ç»Ÿè®¡ç»“æœ
            final_count = len(compacted_qa_pairs)
            compression_ratio = 1.0 - (final_count / original_count) if original_count > 0 else 0.0
            processing_time = (datetime.now() - start_time).total_seconds()

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.compaction_stats.update({
                'total_compactions': self.compaction_stats['total_compactions'] + 1,
                'total_qa_pairs_processed': self.compaction_stats['total_qa_pairs_processed'] + original_count,
                'total_merged_groups': self.compaction_stats['total_merged_groups'] + merged_groups_count,
                'total_duplicates_removed': self.compaction_stats['total_duplicates_removed'] + len(duplicates),
                'compression_ratio': compression_ratio,
                'last_compaction_time': datetime.now().isoformat(),
                'similarity_method': similarity_method,
                'llm_similarity_enabled': use_llm_similarity,
                'embedding_enabled': self.embedding_calc is not None,
                'batch_optimization_enabled': len(qa_pairs) > self.max_full_context_size and self.prefilter is not None
            })

            self.logger.info(f"å‹ç¼©å®Œæˆï¼")
            self.logger.info(f"åŸå§‹æ•°é‡: {original_count}, å‹ç¼©å: {final_count}")
            self.logger.info(f"å‹ç¼©æ¯”ä¾‹: {compression_ratio:.2%}")
            self.logger.info(f"å¤„ç†æ—¶é—´: {processing_time:.1f}ç§’")

            return {
                'success': True,
                'original_count': original_count,
                'final_count': final_count,
                'compression_ratio': compression_ratio,
                'duplicates_removed': len(duplicates),
                'groups_merged': merged_groups_count,
                'processing_time': processing_time,
                'compacted_qa_pairs': compacted_qa_pairs,
                'estimated_tokens': total_tokens,
                'compaction_stats': self.compaction_stats.copy()
            }

        except Exception as e:
            self.logger.error(f"å‹ç¼©è¿‡ç¨‹å¤±è´¥: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'original_count': original_count,
                'processing_time': (datetime.now() - start_time).total_seconds()
            }

    def get_compaction_statistics(self) -> Dict[str, Any]:
        """
        è·å–å‹ç¼©ç»Ÿè®¡ä¿¡æ¯

        Returns:
            Dict[str, Any]: å‹ç¼©ç»Ÿè®¡ä¿¡æ¯
        """
        return self.compaction_stats.copy()


class CompactionScheduler:
    """å‹ç¼©è°ƒåº¦å™¨ - ç®¡ç†å®šæ—¶å‹ç¼©ä»»åŠ¡"""

    def __init__(self, compactor: QACompactor, interval_minutes: int = 0.1):
        """
        åˆå§‹åŒ–å‹ç¼©è°ƒåº¦å™¨

        Args:
            compactor: å‹ç¼©å™¨å®ä¾‹
            interval_minutes: å‹ç¼©é—´éš”ï¼ˆåˆ†é’Ÿï¼‰
        """
        self.logger = get_logger(__name__)
        self.compactor = compactor
        self.interval_minutes = interval_minutes
        self.is_running = False
        self.scheduler_thread: Optional[threading.Thread] = None
        self.stop_event = threading.Event()

        # å‹ç¼©è§¦å‘æ¡ä»¶
        self.min_qa_pairs_threshold = 50  # æœ€å°‘é—®ç­”å¯¹æ•°é‡æ‰è§¦å‘å‹ç¼©
        self.compression_ratio_threshold = 0.1  # æœ€å°å‹ç¼©æ¯”ä¾‹æ‰æœ‰æ„ä¹‰
        self.min_inactive_buffer_size = 20  # éæ´»è·ƒç¼“å†²åŒºæœ€å°å¤§å°æ‰å‹ç¼©
        self.max_time_since_last_compaction = 60  # æœ€å¤§æ—¶é—´é—´éš”ï¼ˆåˆ†é’Ÿï¼‰

        # å‹ç¼©ç»Ÿè®¡
        self.compaction_attempts = 0
        self.successful_compactions = 0
        self.failed_compactions = 0
        self.last_compaction_attempt = None
        self.last_successful_compaction = None

    def start_scheduler(self):
        """å¯åŠ¨å‹ç¼©è°ƒåº¦å™¨"""
        if self.is_running:
            self.logger.warning("å‹ç¼©è°ƒåº¦å™¨å·²åœ¨è¿è¡Œ")
            return

        self.is_running = True
        self.stop_event.clear()
        self.scheduler_thread = threading.Thread(target=self._run_scheduler, daemon=True)
        self.scheduler_thread.start()

        self.logger.info(f"å‹ç¼©è°ƒåº¦å™¨å¯åŠ¨ï¼Œå‹ç¼©é—´éš”: {self.interval_minutes} åˆ†é’Ÿ")

    def stop_scheduler(self):
        """åœæ­¢å‹ç¼©è°ƒåº¦å™¨"""
        if not self.is_running:
            return

        self.is_running = False
        self.stop_event.set()

        if self.scheduler_thread:
            self.scheduler_thread.join(timeout=30)

        self.logger.info("å‹ç¼©è°ƒåº¦å™¨å·²åœæ­¢")

    def _run_scheduler(self):
        """è¿è¡Œè°ƒåº¦å™¨å¾ªç¯"""
        while self.is_running and not self.stop_event.is_set():
            try:
                # ç­‰å¾…é—´éš”æ—¶é—´
                if self.stop_event.wait(timeout=self.interval_minutes * 60):
                    # æ”¶åˆ°åœæ­¢ä¿¡å·
                    break

                # æ‰§è¡Œå‹ç¼©æ£€æŸ¥
                self._check_and_compact()

            except Exception as e:
                self.logger.error(f"è°ƒåº¦å™¨è¿è¡Œé”™è¯¯: {str(e)}")

    def _check_and_compact(self):
        """æ£€æŸ¥å¹¶æ‰§è¡Œå‹ç¼©"""
        try:
            from src.core.knowledge_base import get_knowledge_base
            from datetime import datetime

            self.compaction_attempts += 1
            self.last_compaction_attempt = datetime.now()

            self.logger.info("å®šæ—¶å‹ç¼©æ£€æŸ¥ä¸­...")

            # è·å–çŸ¥è¯†åº“å®ä¾‹
            knowledge_base = get_knowledge_base()

            # è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯
            kb_stats = knowledge_base.get_statistics()
            total_qa_pairs = kb_stats.get('total_qa_pairs', 0)
            active_buffer_size = kb_stats.get('active_buffer_size', 0)
            inactive_buffer_size = kb_stats.get('inactive_buffer_size', 0)

            self.logger.debug(f"çŸ¥è¯†åº“çŠ¶æ€: æ€»è®¡{total_qa_pairs}ä¸ªé—®ç­”å¯¹, æ´»è·ƒç¼“å†²åŒº{active_buffer_size}, éæ´»è·ƒç¼“å†²åŒº{inactive_buffer_size}")

            # æ£€æŸ¥æ˜¯å¦æ»¡è¶³å‹ç¼©æ¡ä»¶
            should_compact = False
            reasons = []

            # æ¡ä»¶1: æ€»é—®ç­”å¯¹æ•°é‡è¶³å¤Ÿ
            if total_qa_pairs < self.min_qa_pairs_threshold:
                self.logger.debug(f"é—®ç­”å¯¹æ•°é‡ä¸è¶³: {total_qa_pairs} < {self.min_qa_pairs_threshold}")
                return

            # æ¡ä»¶2: æ´»è·ƒç¼“å†²åŒºæœ‰è¶³å¤Ÿæ•°æ®
            if active_buffer_size >= self.min_inactive_buffer_size:
                should_compact = True
                reasons.append(f"æ´»è·ƒç¼“å†²åŒºæ•°æ®å……è¶³({active_buffer_size})")

            # æ¡ä»¶3: æ£€æŸ¥è·ç¦»ä¸Šæ¬¡å‹ç¼©çš„æ—¶é—´
            compaction_stats = self.compactor.get_compaction_statistics()
            last_compaction_time_str = compaction_stats.get('last_compaction_time')

            if last_compaction_time_str:
                try:
                    last_compaction_time = datetime.fromisoformat(last_compaction_time_str)
                    time_since_last = (datetime.now() - last_compaction_time).total_seconds() / 60

                    if time_since_last >= self.max_time_since_last_compaction:
                        should_compact = True
                        reasons.append(f"è·ç¦»ä¸Šæ¬¡å‹ç¼©æ—¶é—´è¿‡é•¿({time_since_last:.1f}åˆ†é’Ÿ)")
                except:
                    # å¦‚æœè§£ææ—¶é—´å¤±è´¥ï¼Œå¿½ç•¥æ­¤æ¡ä»¶
                    pass
            else:
                # ä»æœªå‹ç¼©è¿‡ï¼Œå¦‚æœæ•°æ®è¶³å¤Ÿå°±å‹ç¼©
                if active_buffer_size >= self.min_inactive_buffer_size:
                    should_compact = True
                    reasons.append("é¦–æ¬¡å‹ç¼©")

            # æ‰§è¡Œå‹ç¼©
            if should_compact:
                self.logger.info(f"è§¦å‘å‹ç¼©: {', '.join(reasons)}")

                # åˆ›å»ºå¿«ç…§
                snapshot = knowledge_base.create_snapshot()
                if not snapshot:
                    self.logger.error("åˆ›å»ºå¿«ç…§å¤±è´¥ï¼Œè·³è¿‡å‹ç¼©")
                    self.failed_compactions += 1
                    return

                # æ‰§è¡Œå‹ç¼©ï¼ˆé»˜è®¤ä½¿ç”¨LLMæ™ºèƒ½æ£€éªŒï¼‰
                compaction_result = self.compactor.compact_qa_pairs(
                    snapshot.data,
                    similarity_threshold=0.75,
                    use_llm_similarity=True
                )

                if compaction_result["success"]:
                    # åˆ‡æ¢ç¼“å†²åŒºå¹¶åŒæ­¥å°¾éƒ¨æ•°æ®
                    compacted_qa_pairs = compaction_result["compacted_qa_pairs"]
                    switch_success = knowledge_base.switch_buffers_with_tail_sync(compacted_qa_pairs)

                    if switch_success:
                        self.successful_compactions += 1
                        self.last_successful_compaction = datetime.now()

                        compression_ratio = compaction_result["compression_ratio"]
                        original_count = compaction_result["original_count"]
                        final_count = compaction_result["final_count"]

                        self.logger.info(f"âœ… å®šæ—¶å‹ç¼©å®Œæˆ: {original_count} â†’ {final_count} ({compression_ratio:.2%})")

                        # å¦‚æœå‹ç¼©æ•ˆæœä¸æ˜æ˜¾ï¼Œè°ƒæ•´è§¦å‘æ¡ä»¶
                        if compression_ratio < self.compression_ratio_threshold:
                            self.min_inactive_buffer_size = min(self.min_inactive_buffer_size + 10, 100)
                            self.logger.info(f"å‹ç¼©æ•ˆæœä¸æ˜æ˜¾ï¼Œè°ƒæ•´è§¦å‘é˜ˆå€¼è‡³ {self.min_inactive_buffer_size}")

                    else:
                        self.failed_compactions += 1
                        self.logger.error("ç¼“å†²åŒºåˆ‡æ¢å¤±è´¥")
                else:
                    self.failed_compactions += 1
                    self.logger.error(f"å‹ç¼©å¤±è´¥: {compaction_result.get('error', 'æœªçŸ¥é”™è¯¯')}")

            else:
                self.logger.debug("æš‚ä¸éœ€è¦å‹ç¼©")

        except Exception as e:
            self.failed_compactions += 1
            self.logger.error(f"å®šæ—¶å‹ç¼©æ£€æŸ¥å¤±è´¥: {str(e)}")

    def get_scheduler_statistics(self) -> Dict[str, Any]:
        """è·å–è°ƒåº¦å™¨ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'is_running': self.is_running,
            'interval_minutes': self.interval_minutes,
            'compaction_attempts': self.compaction_attempts,
            'successful_compactions': self.successful_compactions,
            'failed_compactions': self.failed_compactions,
            'success_rate': self.successful_compactions / max(self.compaction_attempts, 1),
            'last_compaction_attempt': self.last_compaction_attempt.isoformat() if self.last_compaction_attempt else None,
            'last_successful_compaction': self.last_successful_compaction.isoformat() if self.last_successful_compaction else None,
            'trigger_thresholds': {
                'min_qa_pairs': self.min_qa_pairs_threshold,
                'min_inactive_buffer_size': self.min_inactive_buffer_size,
                'max_time_since_last_compaction_minutes': self.max_time_since_last_compaction,
                'compression_ratio_threshold': self.compression_ratio_threshold
            }
        }


# å…¨å±€å‹ç¼©å™¨å®ä¾‹
_compactor_instance: Optional[QACompactor] = None
_compactor_lock = threading.Lock()


def get_qa_compactor() -> QACompactor:
    """
    è·å–å…¨å±€å‹ç¼©å™¨å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰

    Returns:
        QACompactor: å‹ç¼©å™¨å®ä¾‹
    """
    global _compactor_instance

    if _compactor_instance is None:
        with _compactor_lock:
            if _compactor_instance is None:
                _compactor_instance = QACompactor()

    return _compactor_instance