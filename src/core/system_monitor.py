"""
ç³»ç»Ÿç›‘æ§æ¨¡å—
ç›‘æ§çŸ¥è¯†åº“ç³»ç»Ÿçš„è¿è¡ŒçŠ¶æ€ã€æ€§èƒ½æŒ‡æ ‡å’Œèµ„æºä½¿ç”¨æƒ…å†µ
"""

import os
import threading
import time
import psutil
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
import json

from src.utils.logger import get_logger
from src.utils.concurrency import get_concurrency_monitor
from src.core.knowledge_base import get_knowledge_base
from src.core.qa_compactor import get_qa_compactor


@dataclass
class SystemMetrics:
    """ç³»ç»ŸæŒ‡æ ‡"""
    timestamp: datetime
    cpu_percent: float
    memory_percent: float
    memory_used_mb: float
    memory_available_mb: float
    disk_usage_percent: float
    disk_free_gb: float
    process_count: int
    thread_count: int
    file_descriptors: int


@dataclass
class KnowledgeBaseMetrics:
    """çŸ¥è¯†åº“æŒ‡æ ‡"""
    timestamp: datetime
    total_qa_pairs: int
    active_buffer_size: int
    inactive_buffer_size: int
    clean_finished_files: int
    qa_extracted_files: int
    processing_success_rate: float
    avg_qa_pairs_per_file: float
    last_compaction_time: Optional[str]
    compression_ratio: float


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    timestamp: datetime
    avg_processing_time_per_file: float
    avg_qa_extraction_time: float
    avg_compaction_time: float
    total_files_processed: int
    total_tokens_consumed: int
    processing_throughput: float  # files per hour


class SystemMonitor:
    """ç³»ç»Ÿç›‘æ§å™¨"""

    def __init__(self, monitoring_interval: int = 60, retention_hours: int = 24):
        """
        åˆå§‹åŒ–ç³»ç»Ÿç›‘æ§å™¨

        Args:
            monitoring_interval: ç›‘æ§é—´éš”ï¼ˆç§’ï¼‰
            retention_hours: æ•°æ®ä¿ç•™æ—¶é—´ï¼ˆå°æ—¶ï¼‰
        """
        self.logger = get_logger(__name__)
        self.monitoring_interval = monitoring_interval
        self.retention_hours = retention_hours

        # ç›‘æ§çŠ¶æ€
        self.is_monitoring = False
        self.monitor_thread: Optional[threading.Thread] = None
        self.stop_event = threading.Event()

        # æ•°æ®å­˜å‚¨
        self.system_metrics: List[SystemMetrics] = []
        self.kb_metrics: List[KnowledgeBaseMetrics] = []
        self.performance_metrics: List[PerformanceMetrics] = []
        self.metrics_lock = threading.RLock()

        # è­¦æŠ¥é…ç½®
        self.alert_thresholds = {
            'cpu_percent': 80.0,
            'memory_percent': 85.0,
            'disk_usage_percent': 90.0,
            'processing_error_rate': 0.2,  # 20%
            'qa_extraction_error_rate': 0.15,  # 15%
        }

        # è­¦æŠ¥çŠ¶æ€
        self.active_alerts: Dict[str, Dict[str, Any]] = {}
        self.alert_history: List[Dict[str, Any]] = []

        # ç»„ä»¶å¼•ç”¨
        self.knowledge_base = get_knowledge_base()
        self.concurrency_monitor = get_concurrency_monitor()
        self.qa_compactor = None  # å»¶è¿Ÿåˆå§‹åŒ–

        self.logger.info(f"ç³»ç»Ÿç›‘æ§å™¨åˆå§‹åŒ–å®Œæˆ - ç›‘æ§é—´éš”: {monitoring_interval}s, æ•°æ®ä¿ç•™: {retention_hours}h")

    def start_monitoring(self):
        """å¯åŠ¨ç³»ç»Ÿç›‘æ§"""
        if self.is_monitoring:
            self.logger.warning("ç³»ç»Ÿç›‘æ§å·²åœ¨è¿è¡Œ")
            return

        self.is_monitoring = True
        self.stop_event.clear()
        self.monitor_thread = threading.Thread(target=self._monitoring_loop, daemon=True)
        self.monitor_thread.start()

        self.logger.info("ç³»ç»Ÿç›‘æ§å¯åŠ¨æˆåŠŸ")

    def stop_monitoring(self):
        """åœæ­¢ç³»ç»Ÿç›‘æ§"""
        if not self.is_monitoring:
            return

        self.is_monitoring = False
        self.stop_event.set()

        if self.monitor_thread:
            try:
                from config import get_config
                timeout = get_config('system.endpoints.network.monitor_thread_timeout', 30)
            except Exception:
                timeout = 30
            self.monitor_thread.join(timeout=timeout)

        self.logger.info("ç³»ç»Ÿç›‘æ§å·²åœæ­¢")

    def _monitoring_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.is_monitoring and not self.stop_event.is_set():
            try:
                # æ”¶é›†ç³»ç»ŸæŒ‡æ ‡
                self._collect_system_metrics()

                # æ”¶é›†çŸ¥è¯†åº“æŒ‡æ ‡
                self._collect_knowledge_base_metrics()

                # æ”¶é›†æ€§èƒ½æŒ‡æ ‡
                self._collect_performance_metrics()

                # æ£€æŸ¥è­¦æŠ¥
                self._check_alerts()

                # æ¸…ç†è¿‡æœŸæ•°æ®
                self._cleanup_old_metrics()

                # ç­‰å¾…ä¸‹ä¸ªç›‘æ§å‘¨æœŸ
                if self.stop_event.wait(timeout=self.monitoring_interval):
                    break

            except Exception as e:
                self.logger.error(f"ç›‘æ§å¾ªç¯å¼‚å¸¸: {str(e)}")

    def _collect_system_metrics(self):
        """æ”¶é›†ç³»ç»ŸæŒ‡æ ‡"""
        try:
            # CPUä½¿ç”¨ç‡
            cpu_percent = psutil.cpu_percent(interval=1)

            # å†…å­˜ä½¿ç”¨æƒ…å†µ
            memory = psutil.virtual_memory()
            memory_percent = memory.percent
            memory_used_mb = memory.used / (1024 * 1024)
            memory_available_mb = memory.available / (1024 * 1024)

            # ç£ç›˜ä½¿ç”¨æƒ…å†µ
            disk = psutil.disk_usage('/')
            disk_usage_percent = (disk.used / disk.total) * 100
            disk_free_gb = disk.free / (1024 * 1024 * 1024)

            # è¿›ç¨‹ä¿¡æ¯
            current_process = psutil.Process()
            process_count = len(psutil.pids())
            thread_count = current_process.num_threads()

            # æ–‡ä»¶æè¿°ç¬¦
            try:
                file_descriptors = current_process.num_fds()
            except (AttributeError, psutil.AccessDenied):
                file_descriptors = 0

            # åˆ›å»ºæŒ‡æ ‡
            metrics = SystemMetrics(
                timestamp=datetime.now(),
                cpu_percent=cpu_percent,
                memory_percent=memory_percent,
                memory_used_mb=memory_used_mb,
                memory_available_mb=memory_available_mb,
                disk_usage_percent=disk_usage_percent,
                disk_free_gb=disk_free_gb,
                process_count=process_count,
                thread_count=thread_count,
                file_descriptors=file_descriptors
            )

            # å­˜å‚¨æŒ‡æ ‡
            with self.metrics_lock:
                self.system_metrics.append(metrics)

        except Exception as e:
            self.logger.error(f"æ”¶é›†ç³»ç»ŸæŒ‡æ ‡å¤±è´¥: {str(e)}")

    def _collect_knowledge_base_metrics(self):
        """æ”¶é›†çŸ¥è¯†åº“æŒ‡æ ‡"""
        try:
            # è·å–çŸ¥è¯†åº“ç»Ÿè®¡
            kb_stats = self.knowledge_base.get_statistics()

            # è·å–æ–‡ä»¶çŠ¶æ€ç»Ÿè®¡
            clean_finished_count = len(self.knowledge_base.get_clean_finished_files())
            total_files = len(self.knowledge_base.file_status_map)

            # è®¡ç®—æˆåŠŸç‡
            processing_success_rate = 0.0
            avg_qa_pairs_per_file = 0.0

            if total_files > 0:
                success_files = 0
                total_qa_pairs = 0

                for file_status in self.knowledge_base.file_status_map.values():
                    if hasattr(file_status, 'metadata') and file_status.metadata:
                        qa_count = file_status.metadata.get('qa_count', 0)
                        if qa_count > 0:
                            success_files += 1
                            total_qa_pairs += qa_count

                processing_success_rate = success_files / total_files if total_files > 0 else 0.0
                avg_qa_pairs_per_file = total_qa_pairs / success_files if success_files > 0 else 0.0

            # è·å–å‹ç¼©ä¿¡æ¯
            compression_ratio = 0.0
            last_compaction_time = None

            if not self.qa_compactor:
                try:
                    self.qa_compactor = get_qa_compactor()
                except:
                    pass

            if self.qa_compactor:
                compaction_stats = self.qa_compactor.get_compaction_statistics()
                compression_ratio = compaction_stats.get('compression_ratio', 0.0)
                last_compaction_time = compaction_stats.get('last_compaction_time')

            # åˆ›å»ºæŒ‡æ ‡
            metrics = KnowledgeBaseMetrics(
                timestamp=datetime.now(),
                total_qa_pairs=kb_stats.get('total_qa_pairs', 0),
                active_buffer_size=kb_stats.get('active_buffer_size', 0),
                inactive_buffer_size=kb_stats.get('inactive_buffer_size', 0),
                clean_finished_files=clean_finished_count,
                qa_extracted_files=kb_stats.get('clean_finished_files', 0),
                processing_success_rate=processing_success_rate,
                avg_qa_pairs_per_file=avg_qa_pairs_per_file,
                last_compaction_time=last_compaction_time,
                compression_ratio=compression_ratio
            )

            # å­˜å‚¨æŒ‡æ ‡
            with self.metrics_lock:
                self.kb_metrics.append(metrics)

        except Exception as e:
            self.logger.error(f"æ”¶é›†çŸ¥è¯†åº“æŒ‡æ ‡å¤±è´¥: {str(e)}")

    def _collect_performance_metrics(self):
        """æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
        try:
            # è·å–å¹¶å‘ç»Ÿè®¡
            concurrency_stats = self.concurrency_monitor.get_statistics()

            # è®¡ç®—å¹³å‡å¤„ç†æ—¶é—´ï¼ˆç®€åŒ–å®ç°ï¼‰
            avg_processing_time_per_file = 0.0
            avg_qa_extraction_time = 0.0
            avg_compaction_time = 0.0
            processing_throughput = 0.0

            # è¿™é‡Œå¯ä»¥åŸºäºå†å²æ•°æ®è®¡ç®—æ›´å‡†ç¡®çš„æ€§èƒ½æŒ‡æ ‡
            # ç®€åŒ–å®ç°ï¼Œä½¿ç”¨å›ºå®šå€¼
            total_files_processed = concurrency_stats.get('total_operations', 0)
            total_tokens_consumed = 0  # éœ€è¦ä»å®é™…ç»Ÿè®¡ä¸­è·å–

            # è®¡ç®—ååé‡ï¼ˆæ–‡ä»¶/å°æ—¶ï¼‰
            if len(self.performance_metrics) > 0:
                time_diff = (datetime.now() - self.performance_metrics[0].timestamp).total_seconds() / 3600
                if time_diff > 0:
                    processing_throughput = total_files_processed / time_diff

            # åˆ›å»ºæŒ‡æ ‡
            metrics = PerformanceMetrics(
                timestamp=datetime.now(),
                avg_processing_time_per_file=avg_processing_time_per_file,
                avg_qa_extraction_time=avg_qa_extraction_time,
                avg_compaction_time=avg_compaction_time,
                total_files_processed=total_files_processed,
                total_tokens_consumed=total_tokens_consumed,
                processing_throughput=processing_throughput
            )

            # å­˜å‚¨æŒ‡æ ‡
            with self.metrics_lock:
                self.performance_metrics.append(metrics)

        except Exception as e:
            self.logger.error(f"æ”¶é›†æ€§èƒ½æŒ‡æ ‡å¤±è´¥: {str(e)}")

    def _check_alerts(self):
        """æ£€æŸ¥è­¦æŠ¥æ¡ä»¶"""
        try:
            current_time = datetime.now()

            # è·å–æœ€æ–°çš„ç³»ç»ŸæŒ‡æ ‡
            if self.system_metrics:
                latest_system = self.system_metrics[-1]

                # CPUä½¿ç”¨ç‡è­¦æŠ¥
                self._check_threshold_alert(
                    'cpu_high',
                    latest_system.cpu_percent,
                    self.alert_thresholds['cpu_percent'],
                    f"CPUä½¿ç”¨ç‡è¿‡é«˜: {latest_system.cpu_percent:.1f}%"
                )

                # å†…å­˜ä½¿ç”¨ç‡è­¦æŠ¥
                self._check_threshold_alert(
                    'memory_high',
                    latest_system.memory_percent,
                    self.alert_thresholds['memory_percent'],
                    f"å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {latest_system.memory_percent:.1f}%"
                )

                # ç£ç›˜ä½¿ç”¨ç‡è­¦æŠ¥
                self._check_threshold_alert(
                    'disk_high',
                    latest_system.disk_usage_percent,
                    self.alert_thresholds['disk_usage_percent'],
                    f"ç£ç›˜ä½¿ç”¨ç‡è¿‡é«˜: {latest_system.disk_usage_percent:.1f}%"
                )

            # è·å–æœ€æ–°çš„çŸ¥è¯†åº“æŒ‡æ ‡
            if self.kb_metrics:
                latest_kb = self.kb_metrics[-1]

                # å¤„ç†é”™è¯¯ç‡è­¦æŠ¥
                error_rate = 1.0 - latest_kb.processing_success_rate
                self._check_threshold_alert(
                    'processing_error_high',
                    error_rate,
                    self.alert_thresholds['processing_error_rate'],
                    f"å¤„ç†é”™è¯¯ç‡è¿‡é«˜: {error_rate:.1%}"
                )

        except Exception as e:
            self.logger.error(f"æ£€æŸ¥è­¦æŠ¥å¤±è´¥: {str(e)}")

    def _check_threshold_alert(self, alert_id: str, current_value: float, threshold: float, message: str):
        """æ£€æŸ¥é˜ˆå€¼è­¦æŠ¥"""
        if current_value >= threshold:
            if alert_id not in self.active_alerts:
                # æ–°è­¦æŠ¥
                alert = {
                    'id': alert_id,
                    'message': message,
                    'start_time': datetime.now(),
                    'current_value': current_value,
                    'threshold': threshold,
                    'severity': 'high' if current_value >= threshold * 1.1 else 'medium'
                }
                self.active_alerts[alert_id] = alert
                self.alert_history.append(alert.copy())
                self.logger.warning(f"ğŸš¨ è­¦æŠ¥è§¦å‘: {message}")
            else:
                # æ›´æ–°ç°æœ‰è­¦æŠ¥
                self.active_alerts[alert_id]['current_value'] = current_value
        else:
            if alert_id in self.active_alerts:
                # è­¦æŠ¥è§£é™¤
                alert = self.active_alerts.pop(alert_id)
                duration = (datetime.now() - alert['start_time']).total_seconds()
                self.logger.info(f"âœ… è­¦æŠ¥è§£é™¤: {alert['message']} (æŒç»­: {duration:.0f}ç§’)")

    def _cleanup_old_metrics(self):
        """æ¸…ç†è¿‡æœŸçš„æŒ‡æ ‡æ•°æ®"""
        try:
            cutoff_time = datetime.now() - timedelta(hours=self.retention_hours)

            with self.metrics_lock:
                # æ¸…ç†ç³»ç»ŸæŒ‡æ ‡
                self.system_metrics = [m for m in self.system_metrics if m.timestamp > cutoff_time]

                # æ¸…ç†çŸ¥è¯†åº“æŒ‡æ ‡
                self.kb_metrics = [m for m in self.kb_metrics if m.timestamp > cutoff_time]

                # æ¸…ç†æ€§èƒ½æŒ‡æ ‡
                self.performance_metrics = [m for m in self.performance_metrics if m.timestamp > cutoff_time]

            # æ¸…ç†è­¦æŠ¥å†å²
            self.alert_history = [a for a in self.alert_history if a['start_time'] > cutoff_time]

        except Exception as e:
            self.logger.error(f"æ¸…ç†è¿‡æœŸæŒ‡æ ‡å¤±è´¥: {str(e)}")

    def get_system_status(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸçŠ¶æ€æ‘˜è¦"""
        try:
            with self.metrics_lock:
                latest_system = self.system_metrics[-1] if self.system_metrics else None
                latest_kb = self.kb_metrics[-1] if self.kb_metrics else None
                latest_perf = self.performance_metrics[-1] if self.performance_metrics else None

            status = {
                'monitoring': {
                    'is_active': self.is_monitoring,
                    'interval_seconds': self.monitoring_interval,
                    'data_retention_hours': self.retention_hours,
                    'last_update': datetime.now().isoformat()
                },
                'system': asdict(latest_system) if latest_system else None,
                'knowledge_base': asdict(latest_kb) if latest_kb else None,
                'performance': asdict(latest_perf) if latest_perf else None,
                'alerts': {
                    'active_count': len(self.active_alerts),
                    'active_alerts': list(self.active_alerts.values()),
                    'recent_alerts': self.alert_history[-10:]  # æœ€è¿‘10ä¸ªè­¦æŠ¥
                },
                'metrics_count': {
                    'system': len(self.system_metrics),
                    'knowledge_base': len(self.kb_metrics),
                    'performance': len(self.performance_metrics)
                }
            }

            return status

        except Exception as e:
            self.logger.error(f"è·å–ç³»ç»ŸçŠ¶æ€å¤±è´¥: {str(e)}")
            return {
                'error': str(e),
                'monitoring': {'is_active': False}
            }

    def get_historical_metrics(self, metric_type: str, hours: int = 1) -> List[Dict[str, Any]]:
        """
        è·å–å†å²æŒ‡æ ‡æ•°æ®

        Args:
            metric_type: æŒ‡æ ‡ç±»å‹ ('system', 'knowledge_base', 'performance')
            hours: æ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼‰

        Returns:
            List[Dict[str, Any]]: å†å²æŒ‡æ ‡æ•°æ®
        """
        try:
            cutoff_time = datetime.now() - timedelta(hours=hours)

            with self.metrics_lock:
                if metric_type == 'system':
                    metrics = [asdict(m) for m in self.system_metrics if m.timestamp > cutoff_time]
                elif metric_type == 'knowledge_base':
                    metrics = [asdict(m) for m in self.kb_metrics if m.timestamp > cutoff_time]
                elif metric_type == 'performance':
                    metrics = [asdict(m) for m in self.performance_metrics if m.timestamp > cutoff_time]
                else:
                    return []

            # è½¬æ¢datetimeä¸ºå­—ç¬¦ä¸²
            for metric in metrics:
                if 'timestamp' in metric and isinstance(metric['timestamp'], datetime):
                    metric['timestamp'] = metric['timestamp'].isoformat()

            return metrics

        except Exception as e:
            self.logger.error(f"è·å–å†å²æŒ‡æ ‡å¤±è´¥: {str(e)}")
            return []

    def export_metrics(self, file_path: str, hours: int = 24) -> bool:
        """
        å¯¼å‡ºæŒ‡æ ‡æ•°æ®åˆ°æ–‡ä»¶

        Args:
            file_path: å¯¼å‡ºæ–‡ä»¶è·¯å¾„
            hours: æ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼‰

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            export_data = {
                'export_time': datetime.now().isoformat(),
                'time_range_hours': hours,
                'system_metrics': self.get_historical_metrics('system', hours),
                'knowledge_base_metrics': self.get_historical_metrics('knowledge_base', hours),
                'performance_metrics': self.get_historical_metrics('performance', hours),
                'alert_history': [
                    {**alert, 'start_time': alert['start_time'].isoformat() if isinstance(alert['start_time'], datetime) else alert['start_time']}
                    for alert in self.alert_history
                ]
            }

            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2)

            self.logger.info(f"æŒ‡æ ‡æ•°æ®å¯¼å‡ºæˆåŠŸ: {file_path}")
            return True

        except Exception as e:
            self.logger.error(f"å¯¼å‡ºæŒ‡æ ‡æ•°æ®å¤±è´¥: {str(e)}")
            return False


# å…¨å±€ç›‘æ§å™¨å®ä¾‹
_system_monitor: Optional[SystemMonitor] = None
_monitor_lock = threading.Lock()


def get_system_monitor() -> SystemMonitor:
    """è·å–å…¨å±€ç³»ç»Ÿç›‘æ§å™¨å®ä¾‹"""
    global _system_monitor
    if _system_monitor is None:
        with _monitor_lock:
            if _system_monitor is None:
                _system_monitor = SystemMonitor()
    return _system_monitor


def cleanup_system_monitor():
    """æ¸…ç†å…¨å±€ç³»ç»Ÿç›‘æ§å™¨å®ä¾‹"""
    global _system_monitor
    if _system_monitor is not None:
        _system_monitor.stop_monitoring()
        _system_monitor = None