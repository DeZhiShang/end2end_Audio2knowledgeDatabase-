"""
LLMæ•°æ®æ¸…æ´—æ¨¡å—
è´Ÿè´£ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹æ¸…æ´—ASRè¯†åˆ«ç»“æœï¼Œè¿˜åŸçœŸå®å¯¹è¯
"""

import os
from typing import Dict, Any
try:
    import openai
except ImportError:
    print("è­¦å‘Š: openaiåŒ…æœªå®‰è£…ï¼Œè¯·è¿è¡Œ pip install openai")
    openai = None

try:
    from dotenv import load_dotenv
except ImportError:
    print("è­¦å‘Š: python-dotenvåŒ…æœªå®‰è£…ï¼Œè¯·è¿è¡Œ pip install python-dotenv")
    def load_dotenv():
        pass

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

from src.utils.logger import get_logger


class LLMDataCleaner:
    """LLMæ•°æ®æ¸…æ´—å™¨ï¼šä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹æ¸…æ´—ASRè¯†åˆ«ç»“æœ"""

    def __init__(self):
        """åˆå§‹åŒ–LLMæ¸…æ´—å™¨"""
        self.logger = get_logger(__name__)

        if openai is None:
            raise ImportError("è¯·å…ˆå®‰è£…openaiåŒ…: pip install openai")

        self.api_key = os.getenv('DASHSCOPE_API_KEY')
        self.base_url = os.getenv('DASHSCOPE_BASE_URL')

        if not self.api_key or not self.base_url:
            raise ValueError("è¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®DASHSCOPE_API_KEYå’ŒDASHSCOPE_BASE_URL")

        # é…ç½®OpenAIå®¢æˆ·ç«¯ï¼ˆå…¼å®¹é˜¿é‡Œäº‘DashScope APIï¼‰
        self.client = openai.OpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )

        self.model_name = "qwen-plus-latest"

        # Gleaningæœºåˆ¶é…ç½®
        self.max_gleaning_rounds = 3  # æœ€å¤§æ¸…æ´—è½®æ•°
        self.quality_threshold = 0.90  # è´¨é‡é˜ˆå€¼(0-1)
        # ç§»é™¤æœ€å°æ”¹è¿›é˜ˆå€¼é™åˆ¶ï¼Œä»¥è´¨é‡è¾¾æ ‡ä¸ºå‡†

    def get_cleaning_prompt(self) -> str:
        """
        è·å–æ•°æ®æ¸…æ´—çš„promptæ¨¡æ¿

        Returns:
            str: å®Œæ•´çš„promptæ¨¡æ¿
        """
        prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¯­éŸ³è¯†åˆ«æ•°æ®æ¸…æ´—ä¸“å®¶ï¼Œè´Ÿè´£æ¸…æ´—åšé‚¦æ–¹èˆŸæ— åˆ›è¡€ç³–ä»ªå®¢æœå¯¹è¯è®°å½•ã€‚

## èƒŒæ™¯ä¿¡æ¯
- å¯¹è¯åœºæ™¯ï¼šåšé‚¦æ–¹èˆŸæ— åˆ›è¡€ç³–ä»ªå®¢æœä¸ç”¨æˆ·çš„ç”µè¯å’¨è¯¢
- è¯­è¨€ï¼šå…¨éƒ¨ä¸ºä¸­æ–‡å¯¹è¯
- äº§å“ï¼šåšé‚¦æ–¹èˆŸæ— åˆ›è¡€ç³–ä»ªï¼ˆéä¾µå…¥å¼è¡€ç³–æ£€æµ‹è®¾å¤‡ï¼‰
- æ•°æ®æ¥æºï¼šé€šè¿‡pyannoteè¯´è¯äººåˆ†ç¦» + SenseVoice ASRè¯­éŸ³è¯†åˆ«å¾—åˆ°çš„æ–‡æœ¬

## æ•°æ®è´¨é‡é—®é¢˜
ç”±äºæŠ€æœ¯é™åˆ¶ï¼ŒåŸå§‹æ•°æ®å­˜åœ¨ä»¥ä¸‹é—®é¢˜éœ€è¦æ¸…æ´—ï¼š

1. **è¯†åˆ«é”™è¯¯**ï¼š
   - "æ— å·è¡€ç³–ä»ª"ã€"é“¶è¡Œäº”åˆ›æ£€æŸ¥ä»ª" â†’ åº”ä¸º"æ— åˆ›è¡€ç³–ä»ª"
   - "åšé‚¦æ–¹å·"ã€"åšå¸®æ–¹èˆŸ" â†’ åº”ä¸º"åšé‚¦æ–¹èˆŸ"
   - æ—¥æ–‡è¯¯è¯†åˆ«ï¼šè¯­æ°”è¯"é¢"ã€"å—¯"è¢«è¯†åˆ«ä¸ºæ—¥æ–‡"ãã‚Œ"ç­‰

2. **èƒŒæ™¯å™ªéŸ³å¹²æ‰°**ï¼š
   - ç”µè§†å£°éŸ³ã€çŸ­è§†é¢‘å£°éŸ³
   - å…¶ä»–äººè¯´è¯å£°
   - ä¸è¡€ç³–ä»ªå®¢æœå®Œå…¨æ— å…³çš„å†…å®¹

3. **è¯­éŸ³è´¨é‡é—®é¢˜**ï¼š
   - æ¨¡ç³Šä¸æ¸…çš„è¯­éŸ³
   - é‡å¤ã€æ–­ç»­çš„è¡¨è¾¾
   - å£å¤´ç¦…å’Œå¡«å……è¯è¿‡å¤š

## æ¸…æ´—è¦æ±‚

### å¿…é¡»éµå¾ªçš„åŸåˆ™ï¼š
1. **çœŸå®æ€§åŸåˆ™**ï¼šä¸å¾—ç¼–é€ ä»»ä½•äº‹å®ï¼Œå®ç¼ºæ¯‹æ»¥
2. **ä¸“ä¸šæ€§åŸåˆ™**ï¼šåŸºäºæ— åˆ›è¡€ç³–ä»ªå®¢æœåœºæ™¯è¿›è¡Œåˆ¤æ–­
3. **ä¿å®ˆæ€§åŸåˆ™**ï¼šä¸ç¡®å®šçš„å†…å®¹æ ‡æ³¨ä¸º"[ä¸æ¸…æ¥š]"ï¼Œä¸è¦çŒœæµ‹

### å…·ä½“æ¸…æ´—æ ‡å‡†ï¼š
1. **ä¿ç•™å†…å®¹**ï¼š
   - æ˜ç¡®ä¸è¡€ç³–ä»ªç›¸å…³çš„å¯¹è¯
   - å®¢æœä¸“ä¸šå›ç­”
   - ç”¨æˆ·çš„çœŸå®é—®é¢˜å’Œåé¦ˆ

2. **ä¿®æ­£å†…å®¹**ï¼š
   - æ˜æ˜¾çš„äº§å“åç§°è¯†åˆ«é”™è¯¯
   - å¸¸è§çš„ä¸­æ–‡è¯­éŸ³è¯†åˆ«é”™è¯¯
   - æ—¥æ–‡è¯¯è¯†åˆ«çš„ä¸­æ–‡è¯­æ°”è¯

3. **åˆ é™¤å†…å®¹**ï¼š
   - æ˜æ˜¾çš„èƒŒæ™¯å™ªéŸ³ï¼ˆç”µè§†ã€éŸ³ä¹ç­‰ï¼‰
   - ä¸è¡€ç³–ä»ªæ— å…³çš„å¯¹è¯å†…å®¹
   - é‡å¤çš„æ— æ„ä¹‰è¯­å¥

4. **æ ‡æ³¨å†…å®¹**ï¼š
   - ä¸æ¸…æ¥šçš„å†…å®¹ç”¨"[ä¸æ¸…æ¥š]"æ ‡æ³¨
   - æ¨æµ‹æ€§ä¿®æ­£ç”¨"[ç–‘ä¼¼ï¼šåŸæ–‡]"æ ‡æ³¨

## è¾“å‡ºæ ¼å¼è¦æ±‚
è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºæ¸…æ´—åçš„å¯¹è¯ï¼š

```
# æ¸…æ´—åçš„å®¢æœå¯¹è¯

**SPEAKER_00**: [æ¸…æ´—åçš„æ–‡æœ¬]

**SPEAKER_01**: [æ¸…æ´—åçš„æ–‡æœ¬]

**SPEAKER_00**: [æ¸…æ´—åçš„æ–‡æœ¬]

...

## æ¸…æ´—è¯´æ˜
- åˆ é™¤å†…å®¹ï¼š[ç®€è¦è¯´æ˜åˆ é™¤äº†ä»€ä¹ˆå†…å®¹]
- ä¿®æ­£å†…å®¹ï¼š[è¯´æ˜ä¸»è¦ä¿®æ­£äº†å“ªäº›è¯†åˆ«é”™è¯¯]
- ä¸ç¡®å®šå†…å®¹ï¼š[è¯´æ˜å“ªäº›å†…å®¹æ ‡æ³¨ä¸ºä¸æ¸…æ¥š]
```

## ç¤ºä¾‹
åŸå§‹ASRç»“æœï¼š
```
**SPEAKER_00**: å–‚ä½ å¥½ï¼Œè¿™é‡Œæ˜¯åšå¸®æ–¹èˆŸå®¢æœ
**SPEAKER_01**: ä½ å¥½æˆ‘æƒ³å’¨è¯¢ä¸€ä¸‹é“¶è¡Œäº”åˆ›æ£€æŸ¥ä»ªçš„ä½¿ç”¨æ–¹æ³•
**SPEAKER_00**: ãã‚Œå¥½çš„ï¼Œæˆ‘æ¥ä¸ºæ‚¨ä»‹ç»æ— å·è¡€ç³–ä»ªçš„ä½¿ç”¨
**SPEAKER_01**: ä»Šå¤©å¤©æ°”çœŸå¥½å•Šå¤–é¢é˜³å…‰æ˜åªš
```

æ¸…æ´—åç»“æœï¼š
```
# æ¸…æ´—åçš„å®¢æœå¯¹è¯

**SPEAKER_00**: å–‚ä½ å¥½ï¼Œè¿™é‡Œæ˜¯åšé‚¦æ–¹èˆŸå®¢æœ

**SPEAKER_01**: ä½ å¥½æˆ‘æƒ³å’¨è¯¢ä¸€ä¸‹æ— åˆ›è¡€ç³–ä»ªçš„ä½¿ç”¨æ–¹æ³•

**SPEAKER_00**: å¥½çš„ï¼Œæˆ‘æ¥ä¸ºæ‚¨ä»‹ç»æ— åˆ›è¡€ç³–ä»ªçš„ä½¿ç”¨

## æ¸…æ´—è¯´æ˜
- åˆ é™¤å†…å®¹ï¼šåˆ é™¤äº†ä¸è¡€ç³–ä»ªæ— å…³çš„å¤©æ°”å¯¹è¯
- ä¿®æ­£å†…å®¹ï¼šä¿®æ­£äº†"åšå¸®æ–¹èˆŸ"â†’"åšé‚¦æ–¹èˆŸ"ï¼Œ"é“¶è¡Œäº”åˆ›æ£€æŸ¥ä»ª"â†’"æ— åˆ›è¡€ç³–ä»ª"ï¼Œ"æ— å·è¡€ç³–ä»ª"â†’"æ— åˆ›è¡€ç³–ä»ª"ï¼Œæ—¥æ–‡"ãã‚Œ"â†’åˆ é™¤
- ä¸ç¡®å®šå†…å®¹ï¼šæ— 
```

ç°åœ¨è¯·æ¸…æ´—ä»¥ä¸‹ASRè¯†åˆ«ç»“æœï¼š

"""
        return prompt

    def get_gleaning_prompt(self, round_number: int) -> str:
        """
        è·å–gleaningæ¸…æ´—çš„promptæ¨¡æ¿ï¼ˆç”¨äºç¬¬äºŒè½®åŠåç»­æ¸…æ´—ï¼‰

        Args:
            round_number: å½“å‰æ¸…æ´—è½®æ•°

        Returns:
            str: gleaning promptæ¨¡æ¿
        """
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¯­éŸ³è¯†åˆ«æ•°æ®æ¸…æ´—ä¸“å®¶ï¼Œæ­£åœ¨è¿›è¡Œç¬¬{round_number}è½®ç²¾ç»†åŒ–æ¸…æ´—ã€‚

## ä»»åŠ¡èƒŒæ™¯
ä½ æ­£åœ¨å¯¹åšé‚¦æ–¹èˆŸæ— åˆ›è¡€ç³–ä»ªå®¢æœå¯¹è¯è¿›è¡Œå¤šè½®è¿­ä»£æ¸…æ´—ã€‚å‰ä¸€è½®æ¸…æ´—å·²ç»å®ŒæˆåŸºç¡€çš„é”™è¯¯ä¿®æ­£å’Œå™ªéŸ³è¿‡æ»¤ï¼Œç°åœ¨éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚

## å½“å‰è½®æ¬¡ç›®æ ‡
ç¬¬{round_number}è½®æ¸…æ´—é‡ç‚¹å…³æ³¨ï¼š

### 1. å¯¹è¯æµç•…æ€§ä¼˜åŒ–
- æ£€æŸ¥å¯¹è¯æ˜¯å¦è‡ªç„¶æµç•…
- ä¿®æ­£è¯­è¨€è¡¨è¾¾çš„ç”Ÿç¡¬ä¹‹å¤„
- ç¡®ä¿å®¢æœå›ç­”ä¸“ä¸šè§„èŒƒ

### 2. ä¸“ä¸šæœ¯è¯­ç²¾å‡†æ€§
- è¿›ä¸€æ­¥æ£€æŸ¥è¡€ç³–ä»ªç›¸å…³æœ¯è¯­çš„å‡†ç¡®æ€§
- ç»Ÿä¸€äº§å“åç§°çš„è¡¨è¿°æ–¹å¼
- ç¡®ä¿æŠ€æœ¯æè¿°çš„ä¸“ä¸šæ€§

### 3. é€»è¾‘ä¸€è‡´æ€§éªŒè¯
- æ£€æŸ¥å¯¹è¯é€»è¾‘æ˜¯å¦åˆç†
- éªŒè¯é—®ç­”åŒ¹é…åº¦
- ç¡®ä¿æ—¶é—´é¡ºåºæ­£ç¡®

### 4. ç»†èŠ‚å®Œå–„
- è¡¥å……å¯èƒ½é—æ¼çš„é‡è¦ä¿¡æ¯
- åˆ é™¤ä»ç„¶å­˜åœ¨çš„å†—ä½™å†…å®¹
- ä¼˜åŒ–è¯­è¨€è¡¨è¾¾çš„å‡†ç¡®æ€§

## ä¼˜åŒ–åŸåˆ™

### æ ¸å¿ƒè¦æ±‚ï¼š
1. **è´¨é‡å¯¼å‘**ï¼šä»¥å¯¹è¯è´¨é‡è¾¾æ ‡ä¸ºå‡†ï¼Œä¸è®¾ç½®æœ€å°æ”¹åŠ¨é™åˆ¶
2. **ä¿æŒçœŸå®**ï¼šç»ä¸ç¼–é€ æˆ–æ·»åŠ ä¸å­˜åœ¨çš„å†…å®¹
3. **ä¸“ä¸šæ ‡å‡†**ï¼šç¡®ä¿å®¢æœå¯¹è¯è¾¾åˆ°ä¸“ä¸šæœåŠ¡æ ‡å‡†
4. **æ¸è¿›æ”¹è¿›**ï¼šåœ¨å‰ä¸€è½®åŸºç¡€ä¸Šç»§ç»­ä¼˜åŒ–

### å…·ä½“æ ‡å‡†ï¼š
- **è¯­è¨€æµç•…åº¦**: è‡ªç„¶ã€ä¸“ä¸šã€æ˜“æ‡‚
- **ä¿¡æ¯å®Œæ•´åº¦**: é‡è¦ä¿¡æ¯ä¸ç¼ºå¤±
- **é€»è¾‘æ¸…æ™°åº¦**: é—®ç­”é€»è¾‘æ¸…æ¥š
- **ä¸“ä¸šå‡†ç¡®åº¦**: æœ¯è¯­ä½¿ç”¨å‡†ç¡®

## è¾“å‡ºè¦æ±‚

**é‡è¦**ï¼šæ— è®ºå†…å®¹è´¨é‡å¦‚ä½•ï¼Œéƒ½å¿…é¡»è¾“å‡ºå®Œæ•´çš„å¯¹è¯å†…å®¹ï¼ŒæŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š

```
# ç¬¬{round_number}è½®å¤„ç†åçš„å®¢æœå¯¹è¯

**SPEAKER_00**: [å¤„ç†åçš„å¯¹è¯å†…å®¹ï¼Œå¦‚æ— éœ€ä¿®æ”¹åˆ™ä¿æŒåŸå†…å®¹]

**SPEAKER_01**: [å¤„ç†åçš„å¯¹è¯å†…å®¹ï¼Œå¦‚æ— éœ€ä¿®æ”¹åˆ™ä¿æŒåŸå†…å®¹]

## ç¬¬{round_number}è½®å¤„ç†è¯´æ˜
- ä¸»è¦æ”¹è¿›ï¼š[è¯´æ˜æœ¬è½®çš„ä¸»è¦æ”¹è¿›ï¼Œå¦‚æ— æ”¹è¿›åˆ™è¯´æ˜"å†…å®¹è´¨é‡å·²è¾¾æ ‡ï¼Œä¿æŒåŸçŠ¶"]
- æµç•…æ€§ï¼š[è¯´æ˜è¯­è¨€è¡¨è¾¾æ–¹é¢çš„å¤„ç†]
- ä¸“ä¸šæ€§ï¼š[è¯´æ˜æœ¯è¯­å’Œè¡¨è¿°æ–¹é¢çš„å¤„ç†]
- é€»è¾‘æ€§ï¼š[è¯´æ˜é€»è¾‘ç»“æ„æ–¹é¢çš„å¤„ç†]

## è´¨é‡è¯„ä¼°
- æµç•…åº¦ï¼š[1-10åˆ†è¯„åˆ†]
- ä¸“ä¸šåº¦ï¼š[1-10åˆ†è¯„åˆ†]
- å®Œæ•´åº¦ï¼š[1-10åˆ†è¯„åˆ†]
- å‡†ç¡®åº¦ï¼š[1-10åˆ†è¯„åˆ†]
- ç»¼åˆè¯„åˆ†ï¼š[1-10åˆ†ç»¼åˆè¯„åˆ†]
```

**æ³¨æ„**ï¼šå³ä½¿å†…å®¹å·²ç»å¾ˆé«˜è´¨é‡ï¼Œä¹Ÿå¿…é¡»å®Œæ•´è¾“å‡ºå¯¹è¯å†…å®¹ï¼Œä¸èƒ½åªè¾“å‡ºè¯„ä¼°ä¿¡æ¯ã€‚

ç°åœ¨è¯·å¯¹ä»¥ä¸‹å¯¹è¯è¿›è¡Œç¬¬{round_number}è½®ç²¾ç»†åŒ–æ¸…æ´—ï¼š

"""
        return prompt

    def evaluate_content_quality(self, content: str) -> Dict[str, Any]:
        """
        è¯„ä¼°å†…å®¹è´¨é‡ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦è¿›ä¸€æ­¥æ¸…æ´—

        Args:
            content: å¾…è¯„ä¼°çš„å†…å®¹

        Returns:
            Dict[str, Any]: è´¨é‡è¯„ä¼°ç»“æœ
        """
        try:
            evaluation_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¯¹è¯è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹å®¢æœå¯¹è¯å†…å®¹è¿›è¡Œè´¨é‡è¯„ä¼°ã€‚

## è¯„ä¼°ç»´åº¦
1. **æµç•…åº¦** (1-10åˆ†): è¯­è¨€è¡¨è¾¾æ˜¯å¦è‡ªç„¶æµç•…
2. **ä¸“ä¸šåº¦** (1-10åˆ†): å®¢æœå›ç­”æ˜¯å¦ä¸“ä¸šè§„èŒƒ
3. **å®Œæ•´åº¦** (1-10åˆ†): é‡è¦ä¿¡æ¯æ˜¯å¦å®Œæ•´
4. **å‡†ç¡®åº¦** (1-10åˆ†): æœ¯è¯­ä½¿ç”¨æ˜¯å¦å‡†ç¡®
5. **é€»è¾‘æ€§** (1-10åˆ†): å¯¹è¯é€»è¾‘æ˜¯å¦æ¸…æ™°

## è¾“å‡ºæ ¼å¼
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºè¯„ä¼°ç»“æœï¼š
```json
{
    "fluency_score": 8,
    "professionalism_score": 7,
    "completeness_score": 9,
    "accuracy_score": 8,
    "logic_score": 8,
    "overall_score": 8.0,
    "needs_improvement": true,
    "improvement_suggestions": ["å…·ä½“æ”¹è¿›å»ºè®®1", "å…·ä½“æ”¹è¿›å»ºè®®2"]
}
```

å¾…è¯„ä¼°å†…å®¹ï¼š
""" + content

            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": evaluation_prompt}],
                temperature=0.1,
                max_tokens=1000,
            )

            result_text = response.choices[0].message.content.strip()

            # å°è¯•è§£æJSONç»“æœ
            import json
            import re

            # æå–JSONéƒ¨åˆ†
            json_match = re.search(r'```json\n(.*?)\n```', result_text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
                evaluation_result = json.loads(json_str)
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONæ ¼å¼ï¼Œå°è¯•ç›´æ¥è§£æ
                try:
                    evaluation_result = json.loads(result_text)
                except:
                    # è§£æå¤±è´¥ï¼Œè¿”å›é»˜è®¤è¯„ä¼°
                    evaluation_result = {
                        "fluency_score": 7,
                        "professionalism_score": 7,
                        "completeness_score": 7,
                        "accuracy_score": 7,
                        "logic_score": 7,
                        "overall_score": 7.0,
                        "needs_improvement": True,
                        "improvement_suggestions": ["éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–"]
                    }

            return {
                "success": True,
                "evaluation": evaluation_result,
                "raw_response": result_text
            }

        except Exception as e:
            self.logger.warning(f"è´¨é‡è¯„ä¼°å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "evaluation": {
                    "overall_score": 5.0,
                    "needs_improvement": True,
                    "improvement_suggestions": ["è¯„ä¼°å¤±è´¥ï¼Œå»ºè®®äººå·¥æ£€æŸ¥"]
                }
            }

    def clean_with_gleaning(self, asr_content: str, max_rounds: int = None, quality_threshold: float = None) -> Dict[str, Any]:
        """
        ä½¿ç”¨gleaningæœºåˆ¶è¿›è¡Œå¤šè½®è¿­ä»£æ¸…æ´—

        Args:
            asr_content: ASRè¯†åˆ«çš„åŸå§‹å†…å®¹
            max_rounds: æœ€å¤§æ¸…æ´—è½®æ•°ï¼ˆNoneä½¿ç”¨é»˜è®¤å€¼ï¼‰
            quality_threshold: è´¨é‡é˜ˆå€¼ï¼ˆNoneä½¿ç”¨é»˜è®¤å€¼ï¼‰

        Returns:
            Dict[str, Any]: åŒ…å«å¤šè½®æ¸…æ´—ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        # ä½¿ç”¨é»˜è®¤é…ç½®
        max_rounds = max_rounds or self.max_gleaning_rounds
        quality_threshold = quality_threshold or self.quality_threshold

        pass  # é™é»˜å¼€å§‹å¤šè½®æ¸…æ´—

        # å­˜å‚¨æ‰€æœ‰è½®æ¬¡çš„ç»“æœ
        rounds_results = []
        current_content = asr_content
        total_tokens = 0

        try:
            # ç¬¬ä¸€è½®ï¼šåŸºç¡€æ¸…æ´—
            first_round_result = self.clean_asr_result(current_content)

            if not first_round_result["success"]:
                return {
                    "success": False,
                    "error": f"ç¬¬1è½®æ¸…æ´—å¤±è´¥: {first_round_result.get('error', 'æœªçŸ¥é”™è¯¯')}",
                    "rounds": 1,
                    "total_tokens": 0
                }

            current_content = first_round_result["cleaned_content"]
            total_tokens += first_round_result["token_usage"]["total_tokens"]

            # è¯„ä¼°ç¬¬ä¸€è½®è´¨é‡
            quality_eval = self.evaluate_content_quality(current_content)
            overall_score = quality_eval["evaluation"]["overall_score"] / 10.0 if quality_eval["success"] else 0.5

            rounds_results.append({
                "round": 1,
                "method": "basic_cleaning",
                "content": current_content,
                "quality_score": overall_score,
                "tokens": first_round_result["token_usage"]["total_tokens"],
                "evaluation": quality_eval
            })

            # æ£€æŸ¥æ˜¯å¦å·²è¾¾åˆ°è´¨é‡è¦æ±‚
            if overall_score >= quality_threshold:
                return {
                    "success": True,
                    "rounds": 1,
                    "final_content": current_content,
                    "final_quality_score": overall_score,
                    "total_tokens": total_tokens,
                    "rounds_details": rounds_results,
                    "improvement_achieved": True,
                    "early_stop_reason": "quality_threshold_met"
                }

            # åç»­è½®æ¬¡ï¼šgleaningä¼˜åŒ–
            previous_score = overall_score

            for round_num in range(2, max_rounds + 1):
                pass  # é™é»˜Gleaningä¼˜åŒ–

                # ä½¿ç”¨gleaning prompt
                gleaning_prompt = self.get_gleaning_prompt(round_num) + "\n" + current_content

                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[{"role": "user", "content": gleaning_prompt}],
                    temperature=0.1,
                    max_tokens=4000,
                )

                round_result = response.choices[0].message.content.strip()
                round_tokens = response.usage.total_tokens
                total_tokens += round_tokens

                # è¯„ä¼°æœ¬è½®è´¨é‡
                quality_eval = self.evaluate_content_quality(round_result)
                current_score = quality_eval["evaluation"]["overall_score"] / 10.0 if quality_eval["success"] else 0.5

                # è®¡ç®—æ”¹è¿›å¹…åº¦
                improvement = current_score - previous_score

                rounds_results.append({
                    "round": round_num,
                    "method": "gleaning",
                    "content": round_result,
                    "quality_score": current_score,
                    "tokens": round_tokens,
                    "improvement": improvement,
                    "evaluation": quality_eval
                })

                pass  # é™é»˜è´¨é‡è¯„åˆ†

                # æ£€æŸ¥åœæ­¢æ¡ä»¶
                if current_score >= quality_threshold:
                    pass  # é™é»˜è¾¾åˆ°è´¨é‡é˜ˆå€¼
                    return {
                        "success": True,
                        "rounds": round_num,
                        "final_content": round_result,
                        "final_quality_score": current_score,
                        "total_tokens": total_tokens,
                        "rounds_details": rounds_results,
                        "improvement_achieved": True,
                        "early_stop_reason": "quality_threshold_met"
                    }

                # ç§»é™¤æœ€å°æ”¹è¿›é˜ˆå€¼æ£€æŸ¥ï¼Œç»§ç»­ä¸‹ä¸€è½®ä¼˜åŒ–

                # æ›´æ–°å½“å‰å†…å®¹å’Œåˆ†æ•°
                current_content = round_result
                previous_score = current_score

            # è¾¾åˆ°æœ€å¤§è½®æ•°ï¼Œé€‰æ‹©è´¨é‡æœ€é«˜çš„è½®æ¬¡
            best_round = max(rounds_results, key=lambda r: r["quality_score"])
            self.logger.info(f"  ğŸ è¾¾åˆ°æœ€å¤§è½®æ•° ({max_rounds})ï¼Œé€‰æ‹©æœ€ä½³ç»“æœ (ç¬¬{best_round['round']}è½®)")

            return {
                "success": True,
                "rounds": max_rounds,
                "final_content": best_round["content"],
                "final_quality_score": best_round["quality_score"],
                "total_tokens": total_tokens,
                "rounds_details": rounds_results,
                "improvement_achieved": best_round["quality_score"] > rounds_results[0]["quality_score"],
                "early_stop_reason": "max_rounds_reached"
            }

        except Exception as e:
            self.logger.error(f"Gleaningæ¸…æ´—å¤±è´¥: {str(e)}")
            # å¦‚æœæœ‰éƒ¨åˆ†ç»“æœï¼Œè¿”å›æœ€ä½³ç»“æœ
            if rounds_results:
                best_round = max(rounds_results, key=lambda r: r["quality_score"])
                return {
                    "success": False,
                    "error": str(e),
                    "partial_success": True,
                    "rounds": len(rounds_results),
                    "final_content": best_round["content"],
                    "final_quality_score": best_round["quality_score"],
                    "total_tokens": total_tokens,
                    "rounds_details": rounds_results
                }
            else:
                return {
                    "success": False,
                    "error": str(e),
                    "rounds": 0,
                    "total_tokens": total_tokens
                }

    def clean_asr_result(self, asr_content: str) -> Dict[str, Any]:
        """
        ä½¿ç”¨LLMæ¸…æ´—ASRè¯†åˆ«ç»“æœ

        Args:
            asr_content: ASRè¯†åˆ«çš„åŸå§‹å†…å®¹

        Returns:
            Dict[str, Any]: åŒ…å«æ¸…æ´—ç»“æœå’Œå…ƒä¿¡æ¯çš„å­—å…¸
        """
        try:
            # æ„å»ºå®Œæ•´çš„prompt
            full_prompt = self.get_cleaning_prompt() + "\n" + asr_content

            # è°ƒç”¨LLM API
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "user", "content": full_prompt}
                ],
                temperature=0.1,  # è¾ƒä½çš„æ¸©åº¦ç¡®ä¿ç¨³å®šè¾“å‡º
                max_tokens=4000,  # è¶³å¤Ÿçš„tokenæ•°é‡
            )

            cleaned_content = response.choices[0].message.content.strip()

            return {
                "success": True,
                "original_content": asr_content,
                "cleaned_content": cleaned_content,
                "model": self.model_name,
                "token_usage": {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                }
            }

        except Exception as e:
            self.logger.error(f"LLMæ¸…æ´—å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "original_content": asr_content,
                "cleaned_content": asr_content,  # å¤±è´¥æ—¶è¿”å›åŸå†…å®¹
                "error": str(e),
                "model": self.model_name
            }

    def clean_markdown_file(self, input_file: str, output_file: str = None, enable_gleaning: bool = True, max_rounds: int = None, quality_threshold: float = None) -> Dict[str, Any]:
        """
        ä½¿ç”¨gleaningæœºåˆ¶æ¸…æ´—markdownæ ¼å¼çš„ASRç»“æœæ–‡ä»¶

        Args:
            input_file: è¾“å…¥çš„markdownæ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºçš„æ¸…æ´—åæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ä¸ºåŸæ–‡ä»¶å_gleaned.mdï¼‰
            enable_gleaning: æ˜¯å¦å¯ç”¨gleaningå¤šè½®æ¸…æ´—
            max_rounds: æœ€å¤§æ¸…æ´—è½®æ•°ï¼ˆNoneä½¿ç”¨é»˜è®¤å€¼ï¼‰
            quality_threshold: è´¨é‡é˜ˆå€¼ï¼ˆNoneä½¿ç”¨é»˜è®¤å€¼ï¼‰

        Returns:
            Dict[str, Any]: æ¸…æ´—ç»“æœç»Ÿè®¡
        """
        if not os.path.exists(input_file):
            return {
                "success": False,
                "error": f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}"
            }

        # é»˜è®¤è¾“å‡ºæ–‡ä»¶å
        if output_file is None:
            base_name = os.path.splitext(input_file)[0]
            suffix = "_gleaned" if enable_gleaning else "_cleaned"
            output_file = f"{base_name}{suffix}.md"

        try:
            # è¯»å–åŸå§‹ASRç»“æœ
            with open(input_file, 'r', encoding='utf-8') as f:
                original_content = f.read()

            self.logger.info(f"ğŸ“– è¯»å–ASRç»“æœæ–‡ä»¶: {input_file}")
            self.logger.info(f"ğŸ“„ åŸå§‹å†…å®¹é•¿åº¦: {len(original_content)} å­—ç¬¦")

            # é€‰æ‹©æ¸…æ´—æ–¹æ³•
            if enable_gleaning:
                self.logger.info("ğŸ”„ ä½¿ç”¨Gleaningå¤šè½®æ¸…æ´—...")
                clean_result = self.clean_with_gleaning(original_content, max_rounds, quality_threshold)

                if clean_result["success"] or clean_result.get("partial_success"):
                    # ä¿å­˜æ¸…æ´—åçš„ç»“æœ
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)
                    with open(output_file, 'w', encoding='utf-8') as f:
                        f.write(clean_result["final_content"])

                    self.logger.info(f"âœ… Gleaningæ¸…æ´—å®Œæˆï¼Œæœ€ç»ˆç»“æœå·²ä¿å­˜è‡³: {output_file}")
                    self.logger.info(f"ğŸ“Š å¤„ç†ç»Ÿè®¡: {clean_result['rounds']}è½®, {clean_result['total_tokens']} tokens")
                    self.logger.info(f"ğŸ’¯ æœ€ç»ˆè´¨é‡è¯„åˆ†: {clean_result['final_quality_score']:.2f}")

                    return {
                        "success": True,
                        "input_file": input_file,
                        "output_file": output_file,
                        "original_length": len(original_content),
                        "cleaned_length": len(clean_result["final_content"]),
                        "gleaning_enabled": True,
                        "rounds": clean_result["rounds"],
                        "final_quality_score": clean_result["final_quality_score"],
                        "total_tokens": clean_result["total_tokens"],
                        "improvement_achieved": clean_result.get("improvement_achieved", False),
                        "early_stop_reason": clean_result.get("early_stop_reason", "unknown"),
                        "rounds_details": clean_result.get("rounds_details", [])
                    }
                else:
                    self.logger.error(f"Gleaningæ¸…æ´—å¤±è´¥: {clean_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                    return {
                        "success": False,
                        "input_file": input_file,
                        "error": clean_result.get("error", "Gleaningæ¸…æ´—å¤±è´¥"),
                        "gleaning_enabled": True
                    }
            else:
                self.logger.info("ğŸ¤– ä½¿ç”¨æ ‡å‡†å•è½®æ¸…æ´—...")
                clean_result = self.clean_asr_result(original_content)

                if clean_result["success"]:
                    # ä¿å­˜æ¸…æ´—åçš„ç»“æœ
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)
                    with open(output_file, 'w', encoding='utf-8') as f:
                        f.write(clean_result["cleaned_content"])

                    self.logger.info(f"âœ… æ ‡å‡†æ¸…æ´—å®Œæˆï¼Œç»“æœä¿å­˜è‡³: {output_file}")
                    self.logger.info(f"ğŸ“Š Tokenä½¿ç”¨æƒ…å†µ: {clean_result['token_usage']['total_tokens']} tokens")

                    return {
                        "success": True,
                        "input_file": input_file,
                        "output_file": output_file,
                        "original_length": len(original_content),
                        "cleaned_length": len(clean_result["cleaned_content"]),
                        "gleaning_enabled": False,
                        "token_usage": clean_result["token_usage"]
                    }
                else:
                    self.logger.error(f"æ ‡å‡†æ¸…æ´—å¤±è´¥: {clean_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                    return {
                        "success": False,
                        "input_file": input_file,
                        "error": clean_result.get("error", "æ ‡å‡†æ¸…æ´—å¤±è´¥"),
                        "gleaning_enabled": False
                    }

        except Exception as e:
            self.logger.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            return {
                "success": False,
                "input_file": input_file,
                "error": str(e)
            }


    def batch_clean_directory(self, input_dir: str = "docs", output_dir: str = "docs", enable_gleaning: bool = True, max_rounds: int = None, quality_threshold: float = None) -> Dict[str, Any]:
        """
        æ‰¹é‡æ¸…æ´—ç›®å½•ä¸‹çš„æ‰€æœ‰ASRç»“æœæ–‡ä»¶ï¼ˆæ”¯æŒgleaningï¼‰

        Args:
            input_dir: è¾“å…¥ç›®å½•è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
            enable_gleaning: æ˜¯å¦å¯ç”¨gleaningå¤šè½®æ¸…æ´—
            max_rounds: æœ€å¤§æ¸…æ´—è½®æ•°ï¼ˆNoneä½¿ç”¨é»˜è®¤å€¼ï¼‰
            quality_threshold: è´¨é‡é˜ˆå€¼ï¼ˆNoneä½¿ç”¨é»˜è®¤å€¼ï¼‰

        Returns:
            Dict[str, Any]: æ‰¹é‡å¤„ç†ç»“æœç»Ÿè®¡
        """
        if not os.path.exists(input_dir):
            return {
                "success": False,
                "error": f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}"
            }

        # è·å–æ‰€æœ‰markdownæ–‡ä»¶
        md_files = [f for f in os.listdir(input_dir) if f.endswith('.md')]

        if not md_files:
            return {
                "success": False,
                "error": f"ç›®å½• {input_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°markdownæ–‡ä»¶"
            }

        method_name = "Gleaningå¤šè½®æ¸…æ´—" if enable_gleaning else "æ ‡å‡†æ¸…æ´—"
        pass  # é™é»˜å¼€å§‹æ‰¹é‡å¤„ç†

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)

        results = []
        success_count = 0
        error_count = 0
        total_tokens = 0
        total_rounds = 0
        quality_scores = []

        for md_file in md_files:
            input_file = os.path.join(input_dir, md_file)
            output_file = os.path.join(output_dir, md_file)

            self.logger.info(f"\nğŸ“„ å¤„ç†æ–‡ä»¶: {md_file}")
            result = self.clean_markdown_file(
                input_file, output_file, enable_gleaning, max_rounds, quality_threshold
            )
            results.append(result)

            if result["success"]:
                success_count += 1
                if enable_gleaning:
                    total_tokens += result["total_tokens"]
                    total_rounds += result["rounds"]
                    quality_scores.append(result["final_quality_score"])
                else:
                    total_tokens += result["token_usage"]["total_tokens"]
            else:
                error_count += 1

        # ç»Ÿè®¡æ€»ç»“
        avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0
        avg_rounds = total_rounds / success_count if success_count > 0 else 0

        self.logger.info(f"\nğŸ‰ æ‰¹é‡{method_name}å®Œæˆï¼")
        self.logger.info(f"âœ… æˆåŠŸ: {success_count} ä¸ªæ–‡ä»¶")
        self.logger.info(f"å¤±è´¥: {error_count} ä¸ªæ–‡ä»¶")
        self.logger.info(f"ğŸ“Š æ€»è®¡ä½¿ç”¨: {total_tokens} tokens")

        if enable_gleaning and quality_scores:
            self.logger.info(f"ğŸ’¯ å¹³å‡è´¨é‡è¯„åˆ†: {avg_quality:.2f}")
            self.logger.info(f"ğŸ”„ å¹³å‡æ¸…æ´—è½®æ•°: {avg_rounds:.1f}")

        return {
            "success": True,
            "total_files": len(md_files),
            "success_count": success_count,
            "error_count": error_count,
            "total_tokens": total_tokens,
            "gleaning_enabled": enable_gleaning,
            "average_quality_score": avg_quality if enable_gleaning else None,
            "average_rounds": avg_rounds if enable_gleaning else None,
            "results": results,
            "output_directory": output_dir
        }



