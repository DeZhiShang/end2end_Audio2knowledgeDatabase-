"""
LLMæ•°æ®æ¸…æ´—æ¨¡å—
è´Ÿè´£ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹æ¸…æ´—ASRè¯†åˆ«ç»“æœï¼Œè¿˜åŸçœŸå®å¯¹è¯
"""

import os
from typing import Dict, Any
from src.core.prompt import get_cleaning_prompt, get_gleaning_prompt
try:
    import openai
except ImportError:
    print("è­¦å‘Š: openaiåŒ…æœªå®‰è£…ï¼Œè¯·è¿è¡Œ pip install openai")
    openai = None

try:
    from dotenv import load_dotenv
except ImportError:
    print("è­¦å‘Š: python-dotenvåŒ…æœªå®‰è£…ï¼Œè¯·è¿è¡Œ pip install python-dotenv")
    def load_dotenv():
        pass

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

from src.utils.logger import get_logger
# å¯¼å…¥é…ç½®ç³»ç»Ÿ
from config import get_config, get_api_config


class LLMDataCleaner:
    """LLMæ•°æ®æ¸…æ´—å™¨ï¼šä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹æ¸…æ´—ASRè¯†åˆ«ç»“æœ"""

    def __init__(self):
        """åˆå§‹åŒ–LLMæ¸…æ´—å™¨"""
        self.logger = get_logger(__name__)

        if openai is None:
            raise ImportError("è¯·å…ˆå®‰è£…openaiåŒ…: pip install openai")

        # è·å–APIé…ç½®
        api_config = get_api_config()

        self.api_key = api_config.get('api_key') or os.getenv('DASHSCOPE_API_KEY')
        self.base_url = api_config.get('api_base') or os.getenv('DASHSCOPE_BASE_URL')

        if not self.api_key or not self.base_url:
            raise ValueError("è¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®DASHSCOPE_API_KEYå’ŒDASHSCOPE_BASE_URL")

        # é…ç½®OpenAIå®¢æˆ·ç«¯ï¼ˆå…¼å®¹é˜¿é‡Œäº‘DashScope APIï¼‰
        self.client = openai.OpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )

        # ä»é…ç½®ç³»ç»Ÿè·å–æ¨¡å‹é…ç½®
        self.model_name = get_config('models.llm.model_name', 'qwen-plus-latest')

        # Gleaningæœºåˆ¶é…ç½® (ä»é…ç½®ç³»ç»Ÿè·å–)
        self.max_gleaning_rounds = get_config('processing.gleaning.max_gleaning_rounds', 3)
        self.quality_threshold = get_config('models.llm.quality_threshold', 0.90)
        # ç§»é™¤æœ€å°æ”¹è¿›é˜ˆå€¼é™åˆ¶ï¼Œä»¥è´¨é‡è¾¾æ ‡ä¸ºå‡†



    def evaluate_content_quality(self, content: str) -> Dict[str, Any]:
        """
        è¯„ä¼°å†…å®¹è´¨é‡ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦è¿›ä¸€æ­¥æ¸…æ´—

        Args:
            content: å¾…è¯„ä¼°çš„å†…å®¹

        Returns:
            Dict[str, Any]: è´¨é‡è¯„ä¼°ç»“æœ
        """
        try:
            evaluation_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¯¹è¯è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹å®¢æœå¯¹è¯å†…å®¹è¿›è¡Œè´¨é‡è¯„ä¼°ã€‚

## è¯„ä¼°ç»´åº¦
1. **æµç•…åº¦** (1-10åˆ†): è¯­è¨€è¡¨è¾¾æ˜¯å¦è‡ªç„¶æµç•…
2. **ä¸“ä¸šåº¦** (1-10åˆ†): å®¢æœå›ç­”æ˜¯å¦ä¸“ä¸šè§„èŒƒ
3. **å®Œæ•´åº¦** (1-10åˆ†): é‡è¦ä¿¡æ¯æ˜¯å¦å®Œæ•´
4. **å‡†ç¡®åº¦** (1-10åˆ†): æœ¯è¯­ä½¿ç”¨æ˜¯å¦å‡†ç¡®
5. **é€»è¾‘æ€§** (1-10åˆ†): å¯¹è¯é€»è¾‘æ˜¯å¦æ¸…æ™°

## è¾“å‡ºæ ¼å¼
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºè¯„ä¼°ç»“æœï¼š
```json
{
    "fluency_score": 8,
    "professionalism_score": 7,
    "completeness_score": 9,
    "accuracy_score": 8,
    "logic_score": 8,
    "overall_score": 8.0,
    "needs_improvement": true,
    "improvement_suggestions": ["å…·ä½“æ”¹è¿›å»ºè®®1", "å…·ä½“æ”¹è¿›å»ºè®®2"]
}
```

å¾…è¯„ä¼°å†…å®¹ï¼š
""" + content

            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": evaluation_prompt}],
                temperature=get_config('models.llm.temperature', 0.1),
                max_tokens=get_config('models.llm.max_tokens_evaluation', 1000),
            )

            result_text = response.choices[0].message.content.strip()

            # å°è¯•è§£æJSONç»“æœ
            import json
            import re

            # æå–JSONéƒ¨åˆ†
            json_match = re.search(r'```json\n(.*?)\n```', result_text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
                evaluation_result = json.loads(json_str)
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONæ ¼å¼ï¼Œå°è¯•ç›´æ¥è§£æ
                try:
                    evaluation_result = json.loads(result_text)
                except:
                    # è§£æå¤±è´¥ï¼Œè¿”å›é»˜è®¤è¯„ä¼°
                    evaluation_result = {
                        "fluency_score": 7,
                        "professionalism_score": 7,
                        "completeness_score": 7,
                        "accuracy_score": 7,
                        "logic_score": 7,
                        "overall_score": 7.0,
                        "needs_improvement": True,
                        "improvement_suggestions": ["éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–"]
                    }

            return {
                "success": True,
                "evaluation": evaluation_result,
                "raw_response": result_text
            }

        except Exception as e:
            self.logger.warning(f"è´¨é‡è¯„ä¼°å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "evaluation": {
                    "overall_score": 5.0,
                    "needs_improvement": True,
                    "improvement_suggestions": ["è¯„ä¼°å¤±è´¥ï¼Œå»ºè®®äººå·¥æ£€æŸ¥"]
                }
            }

    def clean_with_gleaning(self, asr_content: str, max_rounds: int = None, quality_threshold: float = None) -> Dict[str, Any]:
        """
        ä½¿ç”¨gleaningæœºåˆ¶è¿›è¡Œå¤šè½®è¿­ä»£æ¸…æ´—

        Args:
            asr_content: ASRè¯†åˆ«çš„åŸå§‹å†…å®¹
            max_rounds: æœ€å¤§æ¸…æ´—è½®æ•°ï¼ˆNoneä½¿ç”¨é»˜è®¤å€¼ï¼‰
            quality_threshold: è´¨é‡é˜ˆå€¼ï¼ˆNoneä½¿ç”¨é»˜è®¤å€¼ï¼‰

        Returns:
            Dict[str, Any]: åŒ…å«å¤šè½®æ¸…æ´—ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        # ä½¿ç”¨é»˜è®¤é…ç½®
        max_rounds = max_rounds or self.max_gleaning_rounds
        quality_threshold = quality_threshold or self.quality_threshold

        pass  # é™é»˜å¼€å§‹å¤šè½®æ¸…æ´—

        # å­˜å‚¨æ‰€æœ‰è½®æ¬¡çš„ç»“æœ
        rounds_results = []
        current_content = asr_content
        total_tokens = 0

        try:
            # ç¬¬ä¸€è½®ï¼šåŸºç¡€æ¸…æ´—
            first_round_result = self.clean_asr_result(current_content)

            if not first_round_result["success"]:
                return {
                    "success": False,
                    "error": f"ç¬¬1è½®æ¸…æ´—å¤±è´¥: {first_round_result.get('error', 'æœªçŸ¥é”™è¯¯')}",
                    "rounds": 1,
                    "total_tokens": 0
                }

            current_content = first_round_result["cleaned_content"]
            total_tokens += first_round_result["token_usage"]["total_tokens"]

            # è¯„ä¼°ç¬¬ä¸€è½®è´¨é‡
            quality_eval = self.evaluate_content_quality(current_content)
            overall_score = quality_eval["evaluation"]["overall_score"] / 10.0 if quality_eval["success"] else 0.5

            rounds_results.append({
                "round": 1,
                "method": "basic_cleaning",
                "content": current_content,
                "quality_score": overall_score,
                "tokens": first_round_result["token_usage"]["total_tokens"],
                "evaluation": quality_eval
            })

            # æ£€æŸ¥æ˜¯å¦å·²è¾¾åˆ°è´¨é‡è¦æ±‚
            if overall_score >= quality_threshold:
                return {
                    "success": True,
                    "rounds": 1,
                    "final_content": current_content,
                    "final_quality_score": overall_score,
                    "total_tokens": total_tokens,
                    "rounds_details": rounds_results,
                    "improvement_achieved": True,
                    "early_stop_reason": "quality_threshold_met"
                }

            # åç»­è½®æ¬¡ï¼šgleaningä¼˜åŒ–
            previous_score = overall_score

            for round_num in range(2, max_rounds + 1):
                pass  # é™é»˜Gleaningä¼˜åŒ–

                # ä½¿ç”¨gleaning prompt
                gleaning_prompt = get_gleaning_prompt(round_num) + "\n" + current_content

                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[{"role": "user", "content": gleaning_prompt}],
                    temperature=get_config('models.llm.temperature', 0.1),
                    max_tokens=get_config('models.llm.max_tokens_gleaning', 4000),
                )

                round_result = response.choices[0].message.content.strip()
                round_tokens = response.usage.total_tokens
                total_tokens += round_tokens

                # è¯„ä¼°æœ¬è½®è´¨é‡
                quality_eval = self.evaluate_content_quality(round_result)
                current_score = quality_eval["evaluation"]["overall_score"] / 10.0 if quality_eval["success"] else 0.5

                # è®¡ç®—æ”¹è¿›å¹…åº¦
                improvement = current_score - previous_score

                rounds_results.append({
                    "round": round_num,
                    "method": "gleaning",
                    "content": round_result,
                    "quality_score": current_score,
                    "tokens": round_tokens,
                    "improvement": improvement,
                    "evaluation": quality_eval
                })

                pass  # é™é»˜è´¨é‡è¯„åˆ†

                # æ£€æŸ¥åœæ­¢æ¡ä»¶
                if current_score >= quality_threshold:
                    pass  # é™é»˜è¾¾åˆ°è´¨é‡é˜ˆå€¼
                    return {
                        "success": True,
                        "rounds": round_num,
                        "final_content": round_result,
                        "final_quality_score": current_score,
                        "total_tokens": total_tokens,
                        "rounds_details": rounds_results,
                        "improvement_achieved": True,
                        "early_stop_reason": "quality_threshold_met"
                    }

                # ç§»é™¤æœ€å°æ”¹è¿›é˜ˆå€¼æ£€æŸ¥ï¼Œç»§ç»­ä¸‹ä¸€è½®ä¼˜åŒ–

                # æ›´æ–°å½“å‰å†…å®¹å’Œåˆ†æ•°
                current_content = round_result
                previous_score = current_score

            # è¾¾åˆ°æœ€å¤§è½®æ•°ï¼Œé€‰æ‹©è´¨é‡æœ€é«˜çš„è½®æ¬¡
            best_round = max(rounds_results, key=lambda r: r["quality_score"])
            self.logger.info(f"  ğŸ è¾¾åˆ°æœ€å¤§è½®æ•° ({max_rounds})ï¼Œé€‰æ‹©æœ€ä½³ç»“æœ (ç¬¬{best_round['round']}è½®)")

            return {
                "success": True,
                "rounds": max_rounds,
                "final_content": best_round["content"],
                "final_quality_score": best_round["quality_score"],
                "total_tokens": total_tokens,
                "rounds_details": rounds_results,
                "improvement_achieved": best_round["quality_score"] > rounds_results[0]["quality_score"],
                "early_stop_reason": "max_rounds_reached"
            }

        except Exception as e:
            self.logger.error(f"Gleaningæ¸…æ´—å¤±è´¥: {str(e)}")
            # å¦‚æœæœ‰éƒ¨åˆ†ç»“æœï¼Œè¿”å›æœ€ä½³ç»“æœ
            if rounds_results:
                best_round = max(rounds_results, key=lambda r: r["quality_score"])
                return {
                    "success": False,
                    "error": str(e),
                    "partial_success": True,
                    "rounds": len(rounds_results),
                    "final_content": best_round["content"],
                    "final_quality_score": best_round["quality_score"],
                    "total_tokens": total_tokens,
                    "rounds_details": rounds_results
                }
            else:
                return {
                    "success": False,
                    "error": str(e),
                    "rounds": 0,
                    "total_tokens": total_tokens
                }

    def clean_asr_result(self, asr_content: str) -> Dict[str, Any]:
        """
        ä½¿ç”¨LLMæ¸…æ´—ASRè¯†åˆ«ç»“æœ

        Args:
            asr_content: ASRè¯†åˆ«çš„åŸå§‹å†…å®¹

        Returns:
            Dict[str, Any]: åŒ…å«æ¸…æ´—ç»“æœå’Œå…ƒä¿¡æ¯çš„å­—å…¸
        """
        try:
            # æ„å»ºå®Œæ•´çš„prompt
            full_prompt = get_cleaning_prompt() + "\n" + asr_content

            # è°ƒç”¨LLM API
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "user", "content": full_prompt}
                ],
                temperature=get_config('models.llm.temperature', 0.1),  # è¾ƒä½çš„æ¸©åº¦ç¡®ä¿ç¨³å®šè¾“å‡º
                max_tokens=get_config('models.llm.max_tokens_gleaning', 4000),  # è¶³å¤Ÿçš„tokenæ•°é‡
            )

            cleaned_content = response.choices[0].message.content.strip()

            return {
                "success": True,
                "original_content": asr_content,
                "cleaned_content": cleaned_content,
                "model": self.model_name,
                "token_usage": {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                }
            }

        except Exception as e:
            self.logger.error(f"LLMæ¸…æ´—å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "original_content": asr_content,
                "cleaned_content": asr_content,  # å¤±è´¥æ—¶è¿”å›åŸå†…å®¹
                "error": str(e),
                "model": self.model_name
            }

    def clean_markdown_file(self, input_file: str, output_file: str = None, enable_gleaning: bool = True, max_rounds: int = None, quality_threshold: float = None) -> Dict[str, Any]:
        """
        ä½¿ç”¨gleaningæœºåˆ¶æ¸…æ´—markdownæ ¼å¼çš„ASRç»“æœæ–‡ä»¶

        Args:
            input_file: è¾“å…¥çš„markdownæ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºçš„æ¸…æ´—åæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ä¸ºåŸæ–‡ä»¶å_gleaned.mdï¼‰
            enable_gleaning: æ˜¯å¦å¯ç”¨gleaningå¤šè½®æ¸…æ´—
            max_rounds: æœ€å¤§æ¸…æ´—è½®æ•°ï¼ˆNoneä½¿ç”¨é»˜è®¤å€¼ï¼‰
            quality_threshold: è´¨é‡é˜ˆå€¼ï¼ˆNoneä½¿ç”¨é»˜è®¤å€¼ï¼‰

        Returns:
            Dict[str, Any]: æ¸…æ´—ç»“æœç»Ÿè®¡
        """
        if not os.path.exists(input_file):
            return {
                "success": False,
                "error": f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}"
            }

        # é»˜è®¤è¾“å‡ºæ–‡ä»¶å
        if output_file is None:
            base_name = os.path.splitext(input_file)[0]
            suffix = "_gleaned" if enable_gleaning else "_cleaned"
            output_file = f"{base_name}{suffix}.md"

        try:
            # è¯»å–åŸå§‹ASRç»“æœ
            with open(input_file, 'r', encoding='utf-8') as f:
                original_content = f.read()

            self.logger.info(f"ğŸ“– è¯»å–ASRç»“æœæ–‡ä»¶: {input_file}")
            self.logger.info(f"ğŸ“„ åŸå§‹å†…å®¹é•¿åº¦: {len(original_content)} å­—ç¬¦")

            # é€‰æ‹©æ¸…æ´—æ–¹æ³•
            if enable_gleaning:
                self.logger.info("ğŸ”„ ä½¿ç”¨Gleaningå¤šè½®æ¸…æ´—...")
                clean_result = self.clean_with_gleaning(original_content, max_rounds, quality_threshold)

                if clean_result["success"] or clean_result.get("partial_success"):
                    # ä¿å­˜æ¸…æ´—åçš„ç»“æœ
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)
                    with open(output_file, 'w', encoding='utf-8') as f:
                        f.write(clean_result["final_content"])

                    self.logger.info(f"âœ… Gleaningæ¸…æ´—å®Œæˆï¼Œæœ€ç»ˆç»“æœå·²ä¿å­˜è‡³: {output_file}")
                    self.logger.info(f"ğŸ“Š å¤„ç†ç»Ÿè®¡: {clean_result['rounds']}è½®, {clean_result['total_tokens']} tokens")
                    self.logger.info(f"ğŸ’¯ æœ€ç»ˆè´¨é‡è¯„åˆ†: {clean_result['final_quality_score']:.2f}")

                    return {
                        "success": True,
                        "input_file": input_file,
                        "output_file": output_file,
                        "original_length": len(original_content),
                        "cleaned_length": len(clean_result["final_content"]),
                        "gleaning_enabled": True,
                        "rounds": clean_result["rounds"],
                        "final_quality_score": clean_result["final_quality_score"],
                        "total_tokens": clean_result["total_tokens"],
                        "improvement_achieved": clean_result.get("improvement_achieved", False),
                        "early_stop_reason": clean_result.get("early_stop_reason", "unknown"),
                        "rounds_details": clean_result.get("rounds_details", [])
                    }
                else:
                    self.logger.error(f"Gleaningæ¸…æ´—å¤±è´¥: {clean_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                    return {
                        "success": False,
                        "input_file": input_file,
                        "error": clean_result.get("error", "Gleaningæ¸…æ´—å¤±è´¥"),
                        "gleaning_enabled": True
                    }
            else:
                self.logger.info("ğŸ¤– ä½¿ç”¨æ ‡å‡†å•è½®æ¸…æ´—...")
                clean_result = self.clean_asr_result(original_content)

                if clean_result["success"]:
                    # ä¿å­˜æ¸…æ´—åçš„ç»“æœ
                    os.makedirs(os.path.dirname(output_file), exist_ok=True)
                    with open(output_file, 'w', encoding='utf-8') as f:
                        f.write(clean_result["cleaned_content"])

                    self.logger.info(f"âœ… æ ‡å‡†æ¸…æ´—å®Œæˆï¼Œç»“æœä¿å­˜è‡³: {output_file}")
                    self.logger.info(f"ğŸ“Š Tokenä½¿ç”¨æƒ…å†µ: {clean_result['token_usage']['total_tokens']} tokens")

                    return {
                        "success": True,
                        "input_file": input_file,
                        "output_file": output_file,
                        "original_length": len(original_content),
                        "cleaned_length": len(clean_result["cleaned_content"]),
                        "gleaning_enabled": False,
                        "token_usage": clean_result["token_usage"]
                    }
                else:
                    self.logger.error(f"æ ‡å‡†æ¸…æ´—å¤±è´¥: {clean_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                    return {
                        "success": False,
                        "input_file": input_file,
                        "error": clean_result.get("error", "æ ‡å‡†æ¸…æ´—å¤±è´¥"),
                        "gleaning_enabled": False
                    }

        except Exception as e:
            self.logger.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            return {
                "success": False,
                "input_file": input_file,
                "error": str(e)
            }


    def batch_clean_directory(self, input_dir: str = None, output_dir: str = None, enable_gleaning: bool = True, max_rounds: int = None, quality_threshold: float = None) -> Dict[str, Any]:
        """
        æ‰¹é‡æ¸…æ´—ç›®å½•ä¸‹çš„æ‰€æœ‰ASRç»“æœæ–‡ä»¶ï¼ˆæ”¯æŒgleaningï¼‰

        Args:
            input_dir: è¾“å…¥ç›®å½•è·¯å¾„ï¼Œä¸ºNoneæ—¶ä»é…ç½®è·å–
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„ï¼Œä¸ºNoneæ—¶ä»é…ç½®è·å–
            enable_gleaning: æ˜¯å¦å¯ç”¨gleaningå¤šè½®æ¸…æ´—
            max_rounds: æœ€å¤§æ¸…æ´—è½®æ•°ï¼ˆNoneä½¿ç”¨é»˜è®¤å€¼ï¼‰
            quality_threshold: è´¨é‡é˜ˆå€¼ï¼ˆNoneä½¿ç”¨é»˜è®¤å€¼ï¼‰

        Returns:
            Dict[str, Any]: æ‰¹é‡å¤„ç†ç»“æœç»Ÿè®¡
        """
        # ä»é…ç½®ç³»ç»Ÿè·å–é»˜è®¤è·¯å¾„
        if input_dir is None or output_dir is None:
            try:
                from config import get_output_paths
                output_paths = get_output_paths()
                if input_dir is None:
                    input_dir = output_paths['docs_dir']
                if output_dir is None:
                    output_dir = output_paths['docs_dir']
            except Exception:
                # å›é€€åˆ°ç¡¬ç¼–ç é»˜è®¤å€¼
                if input_dir is None:
                    input_dir = "data/output/docs"
                if output_dir is None:
                    output_dir = "data/output/docs"

        if not os.path.exists(input_dir):
            return {
                "success": False,
                "error": f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}"
            }

        # è·å–æ‰€æœ‰markdownæ–‡ä»¶
        md_files = [f for f in os.listdir(input_dir) if f.endswith('.md')]

        if not md_files:
            return {
                "success": False,
                "error": f"ç›®å½• {input_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°markdownæ–‡ä»¶"
            }

        method_name = "Gleaningå¤šè½®æ¸…æ´—" if enable_gleaning else "æ ‡å‡†æ¸…æ´—"
        pass  # é™é»˜å¼€å§‹æ‰¹é‡å¤„ç†

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)

        results = []
        success_count = 0
        error_count = 0
        total_tokens = 0
        total_rounds = 0
        quality_scores = []

        for md_file in md_files:
            input_file = os.path.join(input_dir, md_file)
            output_file = os.path.join(output_dir, md_file)

            self.logger.info(f"\nğŸ“„ å¤„ç†æ–‡ä»¶: {md_file}")
            result = self.clean_markdown_file(
                input_file, output_file, enable_gleaning, max_rounds, quality_threshold
            )
            results.append(result)

            if result["success"]:
                success_count += 1
                if enable_gleaning:
                    total_tokens += result["total_tokens"]
                    total_rounds += result["rounds"]
                    quality_scores.append(result["final_quality_score"])
                else:
                    total_tokens += result["token_usage"]["total_tokens"]
            else:
                error_count += 1

        # ç»Ÿè®¡æ€»ç»“
        avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0
        avg_rounds = total_rounds / success_count if success_count > 0 else 0

        self.logger.info(f"\nğŸ‰ æ‰¹é‡{method_name}å®Œæˆï¼")
        self.logger.info(f"âœ… æˆåŠŸ: {success_count} ä¸ªæ–‡ä»¶")
        self.logger.info(f"å¤±è´¥: {error_count} ä¸ªæ–‡ä»¶")
        self.logger.info(f"ğŸ“Š æ€»è®¡ä½¿ç”¨: {total_tokens} tokens")

        if enable_gleaning and quality_scores:
            self.logger.info(f"ğŸ’¯ å¹³å‡è´¨é‡è¯„åˆ†: {avg_quality:.2f}")
            self.logger.info(f"ğŸ”„ å¹³å‡æ¸…æ´—è½®æ•°: {avg_rounds:.1f}")

        return {
            "success": True,
            "total_files": len(md_files),
            "success_count": success_count,
            "error_count": error_count,
            "total_tokens": total_tokens,
            "gleaning_enabled": enable_gleaning,
            "average_quality_score": avg_quality if enable_gleaning else None,
            "average_rounds": avg_rounds if enable_gleaning else None,
            "results": results,
            "output_directory": output_dir
        }



