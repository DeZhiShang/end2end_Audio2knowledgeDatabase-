"""
é—®ç­”å¯¹æŠ½å–æ¨¡å—
ä»æ¸…æ´—åçš„å®¢æœå¯¹è¯ä¸­æå–é«˜è´¨é‡é—®ç­”å¯¹
"""

import os
import json
import uuid
from typing import Dict, List, Any, Optional
from datetime import datetime
import re
from src.core.prompt import get_qa_extraction_prompt

try:
    import openai
except ImportError:
    print("è­¦å‘Š: openaiåŒ…æœªå®‰è£…ï¼Œè¯·è¿è¡Œ pip install openai")
    openai = None

try:
    from dotenv import load_dotenv
except ImportError:
    print("è­¦å‘Š: python-dotenvåŒ…æœªå®‰è£…ï¼Œè¯·è¿è¡Œ pip install python-dotenv")
    def load_dotenv():
        pass

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

from src.utils.logger import get_logger
from src.core.knowledge_base import QAPair, get_knowledge_base
from src.utils.file_cleaner import get_file_cleaner


class QAExtractor:
    """é—®ç­”å¯¹æŠ½å–å™¨ï¼šä»æ¸…æ´—åçš„å¯¹è¯ä¸­æå–é—®ç­”å¯¹"""

    def __init__(self, enable_auto_cleanup: bool = True, cleanup_dry_run: bool = False):
        """
        åˆå§‹åŒ–é—®ç­”å¯¹æŠ½å–å™¨

        Args:
            enable_auto_cleanup: æ˜¯å¦å¯ç”¨è‡ªåŠ¨æ¸…ç†ä¸­é—´æ–‡ä»¶
            cleanup_dry_run: æ˜¯å¦ä¸ºæ¸…ç†å¹²è¿è¡Œæ¨¡å¼
        """
        self.logger = get_logger(__name__)

        if openai is None:
            raise ImportError("è¯·å…ˆå®‰è£…openaiåŒ…: pip install openai")

        self.api_key = os.getenv('DASHSCOPE_API_KEY')
        self.base_url = os.getenv('DASHSCOPE_BASE_URL')

        if not self.api_key or not self.base_url:
            raise ValueError("è¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®DASHSCOPE_API_KEYå’ŒDASHSCOPE_BASE_URL")

        # é…ç½®OpenAIå®¢æˆ·ç«¯ï¼ˆå…¼å®¹é˜¿é‡Œäº‘DashScope APIï¼‰
        self.client = openai.OpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )

        self.model_name = "qwen-plus-latest"

        # è·å–çŸ¥è¯†åº“å®ä¾‹
        self.knowledge_base = get_knowledge_base()

        # æ–‡ä»¶æ¸…ç†å™¨é…ç½®
        self.enable_auto_cleanup = enable_auto_cleanup
        self.cleanup_dry_run = cleanup_dry_run
        self.file_cleaner = None  # å»¶è¿Ÿåˆå§‹åŒ–

    def _initialize_file_cleaner(self):
        """å»¶è¿Ÿåˆå§‹åŒ–æ–‡ä»¶æ¸…ç†å™¨"""
        if self.file_cleaner is None and self.enable_auto_cleanup:
            try:
                self.file_cleaner = get_file_cleaner(
                    enable_cleanup=self.enable_auto_cleanup,
                    dry_run=self.cleanup_dry_run
                )
                mode_desc = "å¹²è¿è¡Œæ¨¡å¼" if self.cleanup_dry_run else "å®é™…åˆ é™¤æ¨¡å¼"
                self.logger.info(f"æ–‡ä»¶æ¸…ç†å™¨åˆå§‹åŒ–æˆåŠŸ ({mode_desc})")
            except Exception as e:
                self.logger.warning(f"æ–‡ä»¶æ¸…ç†å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
                self.file_cleaner = False  # æ ‡è®°ä¸ºå¤±è´¥

    def _trigger_file_cleanup(self, file_path: str) -> Dict[str, Any]:
        """
        è§¦å‘æ–‡ä»¶æ¸…ç†

        Args:
            file_path: è§¦å‘æ¸…ç†çš„æ–‡ä»¶è·¯å¾„

        Returns:
            Dict[str, Any]: æ¸…ç†ç»“æœ
        """
        if not self.enable_auto_cleanup:
            return {
                "success": False,
                "message": "æ–‡ä»¶æ¸…ç†å·²ç¦ç”¨"
            }

        # åˆå§‹åŒ–æ¸…ç†å™¨
        self._initialize_file_cleaner()
        if not self.file_cleaner or self.file_cleaner is False:
            return {
                "success": False,
                "error": "æ–‡ä»¶æ¸…ç†å™¨ä¸å¯ç”¨"
            }

        # æ‰§è¡Œæ¸…ç†
        try:
            cleanup_result = self.file_cleaner.cleanup_intermediate_files(file_path)

            if cleanup_result["success"]:
                action_desc = "DRY-RUNæ¸…ç†" if self.cleanup_dry_run else "æ¸…ç†"
                self.logger.info(f"ğŸ§¹ {action_desc}ä¸­é—´æ–‡ä»¶æˆåŠŸ: {cleanup_result.get('file_number', 'unknown')}, "
                               f"é‡Šæ”¾ç©ºé—´: {cleanup_result.get('disk_space_freed', 0):.2f}MB")
            else:
                self.logger.warning(f"æ¸…ç†ä¸­é—´æ–‡ä»¶å¤±è´¥: {cleanup_result.get('error', 'æœªçŸ¥é”™è¯¯')}")

            return cleanup_result

        except Exception as e:
            self.logger.error(f"æ–‡ä»¶æ¸…ç†å¼‚å¸¸: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }

    def extract_qa_pairs_from_text(self, dialogue_content: str, source_file: str = "unknown") -> Dict[str, Any]:
        """
        ä»å¯¹è¯æ–‡æœ¬ä¸­æŠ½å–é—®ç­”å¯¹

        Args:
            dialogue_content: æ¸…æ´—åçš„å¯¹è¯å†…å®¹
            source_file: æ¥æºæ–‡ä»¶å

        Returns:
            Dict[str, Any]: æŠ½å–ç»“æœ
        """
        try:
            # æ„å»ºå®Œæ•´çš„prompt
            full_prompt = get_qa_extraction_prompt() + "\n" + dialogue_content

            # è°ƒç”¨LLM API
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "user", "content": full_prompt}
                ],
                temperature=0.1,  # è¾ƒä½çš„æ¸©åº¦ç¡®ä¿ç¨³å®šè¾“å‡º
                max_tokens=32768,  # è¶³å¤Ÿçš„tokenæ•°é‡
            )

            result_text = response.choices[0].message.content.strip()

            # è§£æç®€åŒ–çš„QAå¯¹ç»“æœ
            qa_pairs = self._parse_simple_qa_response(result_text, source_file)

            if qa_pairs:
                return {
                    "success": True,
                    "qa_pairs": qa_pairs,
                    "summary": {"total_pairs": len(qa_pairs)},
                    "original_content": dialogue_content,
                    "source_file": source_file,
                    "token_usage": {
                        "prompt_tokens": response.usage.prompt_tokens,
                        "completion_tokens": response.usage.completion_tokens,
                        "total_tokens": response.usage.total_tokens
                    },
                    "raw_response": result_text
                }
            else:
                return {
                    "success": False,
                    "error": "æ— æ³•è§£æLLMå“åº”",
                    "raw_response": result_text,
                    "source_file": source_file
                }

        except Exception as e:
            self.logger.error(f"é—®ç­”å¯¹æŠ½å–å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "source_file": source_file
            }

    def _parse_simple_qa_response(self, response_text: str, source_file: str = "unknown") -> List[QAPair]:
        """
        è§£æç®€åŒ–çš„QAå¯¹å“åº”

        Args:
            response_text: LLMå“åº”æ–‡æœ¬

        Returns:
            List[QAPair]: è§£æåçš„QAå¯¹åˆ—è¡¨
        """
        qa_pairs = []

        try:
            # æŒ‰Q:å’ŒA:æ¨¡å¼è§£æ
            lines = response_text.strip().split('\n')
            current_q = None
            current_a = None

            for line in lines:
                line = line.strip()
                if not line:
                    continue

                if line.startswith('Q:'):
                    # å¦‚æœæœ‰æœªå®Œæˆçš„QAå¯¹ï¼Œå…ˆä¿å­˜
                    if current_q and current_a:
                        qa_pair = QAPair(
                            id=str(uuid.uuid4()),
                            question=current_q.strip(),
                            answer=current_a.strip(),
                            source_file=source_file,
                            timestamp=datetime.now(),
                            metadata={
                                'category': 'unknown',
                                'keywords': [],
                                'confidence': 0.9,
                                'extraction_method': 'simple_llm'
                            }
                        )
                        qa_pairs.append(qa_pair)

                    # å¼€å§‹æ–°çš„é—®é¢˜
                    current_q = line[2:].strip()
                    current_a = None

                elif line.startswith('A:'):
                    # è®°å½•ç­”æ¡ˆ
                    current_a = line[2:].strip()

                elif current_a is not None:
                    # ç»­æ¥ç­”æ¡ˆï¼ˆå¤šè¡Œç­”æ¡ˆæƒ…å†µï¼‰
                    current_a += " " + line
                elif current_q is not None:
                    # ç»­æ¥é—®é¢˜ï¼ˆå¤šè¡Œé—®é¢˜æƒ…å†µï¼‰
                    current_q += " " + line

            # å¤„ç†æœ€åä¸€ä¸ªQAå¯¹
            if current_q and current_a:
                qa_pair = QAPair(
                    id=str(uuid.uuid4()),
                    question=current_q.strip(),
                    answer=current_a.strip(),
                    source_file=source_file,
                    timestamp=datetime.now(),
                    metadata={
                        'category': 'unknown',
                        'keywords': [],
                        'confidence': 0.9,
                        'extraction_method': 'simple_llm'
                    }
                )
                qa_pairs.append(qa_pair)

            pass  # é™é»˜è§£æQAå¯¹
            return qa_pairs

        except Exception as e:
            self.logger.error(f"ç®€åŒ–QAå“åº”è§£æå¤±è´¥: {str(e)}")
            return []

    def extract_and_save_qa_pairs(self, input_file: str) -> Dict[str, Any]:
        """
        ä»æ–‡ä»¶ä¸­æŠ½å–é—®ç­”å¯¹å¹¶ä¿å­˜åˆ°çŸ¥è¯†åº“

        Args:
            input_file: è¾“å…¥çš„markdownæ–‡ä»¶è·¯å¾„

        Returns:
            Dict[str, Any]: å¤„ç†ç»“æœ
        """
        if not os.path.exists(input_file):
            return {
                "success": False,
                "error": f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}"
            }

        try:
            # è¯»å–å¯¹è¯å†…å®¹
            with open(input_file, 'r', encoding='utf-8') as f:
                dialogue_content = f.read()

            filename = os.path.basename(input_file)
            self.logger.info(f"å¼€å§‹ä» {filename} æŠ½å–é—®ç­”å¯¹...")

            # æŠ½å–é—®ç­”å¯¹
            extraction_result = self.extract_qa_pairs_from_text(dialogue_content, filename)

            if extraction_result["success"]:
                qa_pairs = extraction_result["qa_pairs"]

                if qa_pairs:
                    # ä¿å­˜åˆ°çŸ¥è¯†åº“
                    success = self.knowledge_base.append_qa_pairs(qa_pairs)

                    if success:
                        # æ›´æ–°æ–‡ä»¶çŠ¶æ€
                        from src.core.knowledge_base import ProcessingStatus
                        self.knowledge_base.update_file_status(
                            input_file,
                            ProcessingStatus.QA_EXTRACTED,
                            {
                                'qa_count': len(qa_pairs),
                                'extraction_time': datetime.now().isoformat(),
                                'token_usage': extraction_result.get('token_usage', {})
                            }
                        )

                        self.logger.info(f"âœ… æˆåŠŸæŠ½å–å¹¶ä¿å­˜ {len(qa_pairs)} ä¸ªé—®ç­”å¯¹")

                        # è§¦å‘ä¸­é—´æ–‡ä»¶æ¸…ç†
                        if self.enable_auto_cleanup:
                            try:
                                cleanup_result = self._trigger_file_cleanup(input_file)
                                # æ¸…ç†ç»“æœè®°å½•åœ¨_trigger_file_cleanupä¸­ï¼Œè¿™é‡Œä¸éœ€è¦é¢å¤–æ—¥å¿—
                            except Exception as e:
                                self.logger.warning(f"æ–‡ä»¶æ¸…ç†è§¦å‘å¤±è´¥: {str(e)}")

                        return {
                            "success": True,
                            "input_file": input_file,
                            "qa_count": len(qa_pairs),
                            "summary": extraction_result.get('summary', {}),
                            "token_usage": extraction_result.get('token_usage', {}),
                            "qa_pairs": qa_pairs
                        }
                    else:
                        return {
                            "success": False,
                            "error": "ä¿å­˜é—®ç­”å¯¹åˆ°çŸ¥è¯†åº“å¤±è´¥",
                            "input_file": input_file
                        }
                else:
                    self.logger.warning(f"ä» {filename} ä¸­æœªæŠ½å–åˆ°ä»»ä½•é—®ç­”å¯¹")
                    return {
                        "success": True,
                        "input_file": input_file,
                        "qa_count": 0,
                        "warning": "æœªæŠ½å–åˆ°é—®ç­”å¯¹"
                    }
            else:
                return {
                    "success": False,
                    "error": extraction_result.get("error", "æŠ½å–å¤±è´¥"),
                    "input_file": input_file
                }

        except Exception as e:
            self.logger.error(f"å¤„ç†æ–‡ä»¶ {input_file} æ—¶å‡ºé”™: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "input_file": input_file
            }

    def batch_extract_qa_pairs(self, input_dir: str = "data/output/docs") -> Dict[str, Any]:
        """
        æ‰¹é‡æŠ½å–ç›®å½•ä¸‹æ‰€æœ‰æ¸…æ´—å®Œæˆæ–‡ä»¶çš„é—®ç­”å¯¹

        Args:
            input_dir: è¾“å…¥ç›®å½•è·¯å¾„

        Returns:
            Dict[str, Any]: æ‰¹é‡å¤„ç†ç»“æœç»Ÿè®¡
        """
        if not os.path.exists(input_dir):
            return {
                "success": False,
                "error": f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}"
            }

        # è·å–æ‰€æœ‰æ¸…æ´—å®Œæˆçš„æ–‡ä»¶
        clean_finished_files = self.knowledge_base.get_clean_finished_files()

        if not clean_finished_files:
            self.logger.warning("æ²¡æœ‰æ‰¾åˆ°çŠ¶æ€ä¸ºclean_finishedçš„æ–‡ä»¶")
            return {
                "success": False,
                "error": "æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„æ–‡ä»¶"
            }

        self.logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡é—®ç­”å¯¹æŠ½å–ï¼Œå‘ç° {len(clean_finished_files)} ä¸ªå·²æ¸…æ´—æ–‡ä»¶")

        results = []
        success_count = 0
        error_count = 0
        total_qa_pairs = 0
        total_tokens = 0

        for file_path in clean_finished_files:
            self.logger.info(f"\nğŸ“„ å¤„ç†æ–‡ä»¶: {os.path.basename(file_path)}")

            result = self.extract_and_save_qa_pairs(file_path)
            results.append(result)

            if result["success"]:
                success_count += 1
                qa_count = result.get("qa_count", 0)
                total_qa_pairs += qa_count

                token_usage = result.get("token_usage", {})
                total_tokens += token_usage.get("total_tokens", 0)

                self.logger.info(f"âœ… æˆåŠŸæŠ½å– {qa_count} ä¸ªé—®ç­”å¯¹")
            else:
                error_count += 1
                self.logger.error(f"âŒ å¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

        # ç»Ÿè®¡æ€»ç»“
        self.logger.info(f"\nğŸ‰ æ‰¹é‡é—®ç­”å¯¹æŠ½å–å®Œæˆï¼")
        self.logger.info(f"âœ… æˆåŠŸ: {success_count} ä¸ªæ–‡ä»¶")
        self.logger.info(f"âŒ å¤±è´¥: {error_count} ä¸ªæ–‡ä»¶")
        self.logger.info(f"ğŸ“Š æ€»è®¡æŠ½å–: {total_qa_pairs} ä¸ªé—®ç­”å¯¹")
        self.logger.info(f"ğŸ”¢ æ€»è®¡æ¶ˆè€—: {total_tokens} tokens")

        # è·å–çŸ¥è¯†åº“ç»Ÿè®¡
        kb_stats = self.knowledge_base.get_statistics()

        return {
            "success": True,
            "total_files": len(clean_finished_files),
            "success_count": success_count,
            "error_count": error_count,
            "total_qa_pairs": total_qa_pairs,
            "total_tokens": total_tokens,
            "results": results,
            "knowledge_base_stats": kb_stats
        }

    def get_extraction_statistics(self) -> Dict[str, Any]:
        """
        è·å–é—®ç­”å¯¹æŠ½å–ç»Ÿè®¡ä¿¡æ¯

        Returns:
            Dict[str, Any]: ç»Ÿè®¡ä¿¡æ¯
        """
        # è·å–çŸ¥è¯†åº“ç»Ÿè®¡
        kb_stats = self.knowledge_base.get_statistics()

        # ç»Ÿè®¡å„ç§çŠ¶æ€çš„æ–‡ä»¶æ•°é‡
        status_counts = {}
        for file_path, file_status in self.knowledge_base.file_status_map.items():
            status = file_status.status.value
            status_counts[status] = status_counts.get(status, 0) + 1

        return {
            "knowledge_base_stats": kb_stats,
            "file_status_counts": status_counts,
            "ready_for_extraction": len(self.knowledge_base.get_clean_finished_files()),
            "total_tracked_files": len(self.knowledge_base.file_status_map)
        }