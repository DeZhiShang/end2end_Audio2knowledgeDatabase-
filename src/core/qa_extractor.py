"""
é—®ç­”å¯¹æŠ½å–æ¨¡å—
ä»æ¸…æ´—åçš„å®¢æœå¯¹è¯ä¸­æå–é«˜è´¨é‡é—®ç­”å¯¹
"""

import os
import json
import uuid
from typing import Dict, List, Any, Optional
from datetime import datetime
import re

try:
    import openai
except ImportError:
    print("è­¦å‘Š: openaiåŒ…æœªå®‰è£…ï¼Œè¯·è¿è¡Œ pip install openai")
    openai = None

try:
    from dotenv import load_dotenv
except ImportError:
    print("è­¦å‘Š: python-dotenvåŒ…æœªå®‰è£…ï¼Œè¯·è¿è¡Œ pip install python-dotenv")
    def load_dotenv():
        pass

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

from src.utils.logger import get_logger
from src.core.knowledge_base import QAPair, get_knowledge_base


class QAExtractor:
    """é—®ç­”å¯¹æŠ½å–å™¨ï¼šä»æ¸…æ´—åçš„å¯¹è¯ä¸­æå–é—®ç­”å¯¹"""

    def __init__(self):
        """åˆå§‹åŒ–é—®ç­”å¯¹æŠ½å–å™¨"""
        self.logger = get_logger(__name__)

        if openai is None:
            raise ImportError("è¯·å…ˆå®‰è£…openaiåŒ…: pip install openai")

        self.api_key = os.getenv('DASHSCOPE_API_KEY')
        self.base_url = os.getenv('DASHSCOPE_BASE_URL')

        if not self.api_key or not self.base_url:
            raise ValueError("è¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®DASHSCOPE_API_KEYå’ŒDASHSCOPE_BASE_URL")

        # é…ç½®OpenAIå®¢æˆ·ç«¯ï¼ˆå…¼å®¹é˜¿é‡Œäº‘DashScope APIï¼‰
        self.client = openai.OpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )

        self.model_name = "qwen-plus-latest"

        # è·å–çŸ¥è¯†åº“å®ä¾‹
        self.knowledge_base = get_knowledge_base()

    def get_qa_extraction_prompt(self) -> str:
        """
        è·å–é—®ç­”å¯¹æŠ½å–çš„promptæ¨¡æ¿

        Returns:
            str: å®Œæ•´çš„promptæ¨¡æ¿
        """
        prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†åº“æ„å»ºä¸“å®¶ï¼Œä¸“é—¨ä»åšé‚¦æ–¹èˆŸæ— åˆ›è¡€ç³–ä»ªå®¢æœå¯¹è¯ä¸­æŠ½å–é«˜è´¨é‡é—®ç­”å¯¹ã€‚

## ä»»åŠ¡èƒŒæ™¯
- ç›®æ ‡ï¼šæ„å»ºåšé‚¦æ–¹èˆŸæ— åˆ›è¡€ç³–ä»ªçš„ä¸“ä¸šçŸ¥è¯†åº“
- è¾“å…¥ï¼šå·²ç»LLMæ¸…æ´—è¿‡çš„é«˜è´¨é‡å®¢æœå¯¹è¯
- è¾“å‡ºï¼šç›´æ¥æŠ½å‡ºQAå¯¹ï¼Œåªéœ€è¦é—®é¢˜å’Œç­”æ¡ˆ

## è¾“å‡ºæ ¼å¼
ç›´æ¥è¾“å‡ºé—®ç­”å¯¹ï¼Œæ¯å¯¹ä¹‹é—´ç”¨ç©ºè¡Œåˆ†éš”ï¼Œä¸€å®šä¸èƒ½å¸¦æœ‰ä»»ä½•markdownå­—ç¬¦ï¼Œåƒ"*,#è¿™æ ·çš„"ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š

Q: é—®é¢˜å†…å®¹
A: ç­”æ¡ˆå†…å®¹

Q: é—®é¢˜å†…å®¹
A: ç­”æ¡ˆå†…å®¹

## æŠ½å–åŸåˆ™

### æ ¸å¿ƒè¦æ±‚
1. **å…¨é¢æ€§**ï¼šç¡®ä¿ä¸é—æ¼ä»»ä½•æœ‰ä»·å€¼çš„é—®ç­”ä¿¡æ¯
2. **å‡†ç¡®æ€§**ï¼šåŸºäºå¯¹è¯äº‹å®ï¼Œä¸å¾—ç¼–é€ æˆ–æ·»åŠ ä¸å­˜åœ¨çš„å†…å®¹
3. **ä¸“ä¸šæ€§**ï¼šé—®ç­”å¯¹åº”ä½“ç°ä¸“ä¸šå®¢æœæ°´å‡†
4. **å®ç”¨æ€§**ï¼šé—®ç­”å¯¹åº”å¯¹å®é™…ç”¨æˆ·æœ‰å¸®åŠ©

### é—®ç­”å¯¹è´¨é‡æ ‡å‡†
1. **é—®é¢˜æ˜ç¡®**ï¼šé—®é¢˜è¡¨è¿°æ¸…æ™°ã€å…·ä½“ã€æ˜“ç†è§£
2. **ç­”æ¡ˆå®Œæ•´**ï¼šç­”æ¡ˆå‡†ç¡®ã€è¯¦ç»†ã€æœ‰å¸®åŠ©
3. **é€»è¾‘æ¸…æ™°**ï¼šé—®ç­”é€»è¾‘å¯¹åº”ï¼Œé¿å…ç­”éæ‰€é—®
4. **è¯­è¨€è§„èŒƒ**ï¼šä½¿ç”¨æ ‡å‡†ã€ä¸“ä¸šçš„è¡¨è¾¾æ–¹å¼

## æŠ½å–ç­–ç•¥

### 1. ç›´æ¥é—®ç­”å¯¹
- ç”¨æˆ·æ˜ç¡®æé—®ï¼Œå®¢æœç›´æ¥å›ç­”çš„å†…å®¹
- ä¿æŒé—®é¢˜çš„åŸå§‹æ„å›¾ï¼Œä¼˜åŒ–è¯­è¨€è¡¨è¾¾

### 2. éšå«é—®ç­”å¯¹
- ä»å¯¹è¯ä¸­æ¨å¯¼å‡ºçš„å¸¸è§é—®é¢˜å’Œç­”æ¡ˆ
- åŸºäºç”¨æˆ·éœ€æ±‚å’Œå®¢æœè§£é‡Šç”Ÿæˆé—®ç­”å¯¹

### 3. çŸ¥è¯†ç‚¹é—®ç­”å¯¹
- å®¢æœä¸»åŠ¨ä»‹ç»çš„äº§å“çŸ¥è¯†ç‚¹
- è½¬æ¢ä¸ºé—®ç­”å½¢å¼ï¼Œä¾¿äºçŸ¥è¯†åº“æŸ¥è¯¢

### 4. é—®é¢˜ç»†åˆ†
- å¤æ‚é—®é¢˜æ‹†åˆ†ä¸ºå¤šä¸ªç®€å•é—®ç­”å¯¹
- æ¯ä¸ªé—®ç­”å¯¹èšç„¦ä¸€ä¸ªå…·ä½“çŸ¥è¯†ç‚¹

## é¢†åŸŸèŒƒå›´
éœ€è¦æ¶µç›–ä½†ä¸é™äºä»¥ä¸‹é¢†åŸŸï¼š

### äº§å“åŸºç¡€ä¿¡æ¯
- äº§å“ä»‹ç»ã€åŠŸèƒ½ç‰¹ç‚¹ã€æŠ€æœ¯åŸç†
- äº§å“è§„æ ¼ã€å‹å·ã€é…ç½®ä¿¡æ¯
- é€‚ç”¨äººç¾¤ã€ä½¿ç”¨åœºæ™¯

### ä½¿ç”¨æ“ä½œ
- å¼€æœºè®¾ç½®ã€åŸºæœ¬æ“ä½œæµç¨‹
- æµ‹é‡æ–¹æ³•ã€æ³¨æ„äº‹é¡¹
- æ•°æ®æŸ¥çœ‹ã€è®°å½•ç®¡ç†

### æ•…éšœè§£å†³
- å¸¸è§é—®é¢˜ã€æ•…éšœç°è±¡
- è§£å†³æ–¹æ³•ã€æ“ä½œæ­¥éª¤
- é¢„é˜²æªæ–½ã€ç»´æŠ¤å»ºè®®

### è´­ä¹°å’¨è¯¢
- ä»·æ ¼ä¿¡æ¯ã€è´­ä¹°æ¸ é“
- ä¼˜æƒ æ´»åŠ¨ã€ä¿ƒé”€æ”¿ç­–
- å”®åæœåŠ¡ã€è´¨ä¿ä¿¡æ¯

### æŠ€æœ¯æ”¯æŒ
- äº§å“åŸç†ã€æŠ€æœ¯ç»†èŠ‚
- ä¸å…¶ä»–è®¾å¤‡å¯¹æ¯”
- ä¸“ä¸šæœ¯è¯­è§£é‡Š


## æ³¨æ„äº‹é¡¹

### è´¨é‡æ§åˆ¶
1. **ç½®ä¿¡åº¦è¦æ±‚**ï¼šåªè¾“å‡ºç½®ä¿¡åº¦â‰¥0.8çš„é—®ç­”å¯¹ï¼Œç¡®ä¿é—®ç­”å¯¹èƒ½ä½œä¸ºçŸ¥è¯†åº“çš„é«˜è´¨é‡è¯­æ–™
2. **å»é‡å¤„ç†**ï¼šé¿å…é‡å¤æˆ–é«˜åº¦ç›¸ä¼¼çš„é—®ç­”å¯¹
3. **å®Œæ•´æ€§æ£€æŸ¥**ï¼šç¡®ä¿ç­”æ¡ˆå®Œæ•´ï¼Œé¿å…æˆªæ–­
4. **ä¸“ä¸šæ€§éªŒè¯**ï¼šç­”æ¡ˆåº”ä½“ç°ä¸“ä¸šå®¢æœæ°´å‡†

### è¯­è¨€è§„èŒƒ
1. **æ ‡å‡†è¡¨è¾¾**ï¼šä½¿ç”¨è§„èŒƒçš„äº§å“åç§°å’Œæœ¯è¯­
2. **ç”¨æˆ·è§†è§’**ï¼šé—®é¢˜ä»ç”¨æˆ·è§’åº¦è¡¨è¿°
3. **æœåŠ¡è¯­è°ƒ**ï¼šç­”æ¡ˆä¿æŒä¸“ä¸šå‹å¥½çš„æœåŠ¡è¯­è°ƒ
4. **é¿å…å£è¯­åŒ–**ï¼šå‡å°‘"å—¯"ã€"å“¦"ç­‰å£è¯­åŒ–è¡¨è¾¾

## ç¤ºä¾‹

è¾“å…¥å¯¹è¯ï¼š
```
**SPEAKER_00**: ä½ å¥½ï¼Œæˆ‘æƒ³äº†è§£ä¸€ä¸‹åšé‚¦æ–¹èˆŸæ— åˆ›è¡€ç³–ä»ªçš„å·¥ä½œåŸç†  
**SPEAKER_01**: æ‚¨å¥½ï¼åšé‚¦æ–¹èˆŸæ— åˆ›è¡€ç³–ä»ªé‡‡ç”¨ä»£è°¢çƒ­æ•´åˆæ³•è¿›è¡Œæ£€æµ‹ã€‚äººä½“ä»£è°¢äº§ç”Ÿçš„çƒ­é‡ä¸è¡€ç³–æµ“åº¦ã€è€—æ°§é‡å¯†åˆ‡ç›¸å…³ã€‚å½“äººä½“å’Œç¯å¢ƒå¤„äºçƒ­å¹³è¡¡çŠ¶æ€æ—¶ï¼Œä»£è°¢äº§ç”Ÿçš„çƒ­é‡ä¸æ•£å‘åˆ°ç¯å¢ƒçš„çƒ­é‡åŸºæœ¬ç›¸ç­‰ã€‚è®¾å¤‡é€šè¿‡å¤šä¼ æ„Ÿå™¨å¤¹å­å¼æ¢å¤´ï¼Œç›‘æµ‹ç¯å¢ƒæ¸©åº¦ã€æ¹¿åº¦ã€äººä½“æ¸©åº¦ã€è¡€æµé€Ÿã€è¾å°„æ•£çƒ­é‡ã€è’¸å‘æ•£çƒ­é‡ã€è¡€æ°§é¥±å’Œåº¦ç­‰å‚æ•°ï¼Œæ®æ­¤è®¡ç®—å‡ºè¡€ç³–æ°´å¹³ã€‚  
**SPEAKER_00**: é‚£å‡†ç¡®åº¦æ€ä¹ˆæ ·ï¼Ÿ  
**SPEAKER_01**: äº§å“åœ¨2018å¹´å®Œæˆäº†æ³¨å†Œä¸´åºŠè¯•éªŒï¼ˆåŒ—äº¬å¤§å­¦ç¬¬ä¸€åŒ»é™¢ã€åŒ—äº¬åå’ŒåŒ»é™¢ã€åº”æ€¥æ€»åŒ»é™¢ï¼‰ï¼Œæ£€æµ‹ç»“æœä¸é™è„‰è¡€æ£€æµ‹å¯¹æ¯”ä¸€è‡´æ€§è¾¾93.9%ï¼Œä¸æŒ‡å°–è¡€æ£€æµ‹å¯¹æ¯”ä¸€è‡´æ€§ä¸º94.4%ï¼Œç¬¦åˆä¸´åºŠåº”ç”¨æ ‡å‡†ã€‚ 
```

è¾“å‡ºç¤ºä¾‹ï¼š
Q: åšé‚¦æ–¹èˆŸæ— åˆ›è¡€ç³–ä»ªçš„å·¥ä½œåŸç†æ˜¯ä»€ä¹ˆï¼Ÿ
A: åšé‚¦æ–¹èˆŸæ— åˆ›è¡€ç³–ä»ªåŸºäºä»£è°¢çƒ­æ•´åˆæ³•åŸç†è¿›è¡Œè¡€ç³–æ£€æµ‹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œäººä½“ä»£è°¢äº§ç”Ÿçš„çƒ­é‡ä¸è¡€ç³–æµ“åº¦ã€è€—æ°§é‡æ­£ç›¸å…³ã€‚å½“äººä½“ä¸ç¯å¢ƒå¤„äºçƒ­å¹³è¡¡çŠ¶æ€æ—¶ï¼Œä»£è°¢çƒ­é‡ä¸æ•£å‘çš„çƒ­é‡åŸºæœ¬ç›¸ç­‰ã€‚è®¾å¤‡é€šè¿‡å¤¹å­å¼æ¢å¤´é›†æˆå¤šç§ä¼ æ„Ÿå™¨ï¼Œç›‘æµ‹ç¯å¢ƒæ¸©åº¦ã€æ¹¿åº¦ã€äººä½“æ¸©åº¦ã€è¡€æµé€Ÿã€è¾å°„æ•£çƒ­é‡ã€è’¸å‘æ•£çƒ­é‡å’Œè¡€æ°§é¥±å’Œåº¦ç­‰æŒ‡æ ‡ï¼Œå¹¶ç»“åˆä»£è°¢çƒ­é‡å’Œè€—æ°§é‡è®¡ç®—å‡ºè¡€ç³–æµ“åº¦ã€‚å¤¹å­å¼ç»“æ„è®¾è®¡èƒ½ä¿è¯æ£€æµ‹è¿‡ç¨‹çš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§ã€‚

Q: åšé‚¦æ–¹èˆŸæ— åˆ›è¡€ç³–ä»ªçš„æµ‹é‡å‡†ç¡®æ€§ä¾æ®æ˜¯ä»€ä¹ˆï¼Ÿ
A: æœ¬äº§å“äº2018å¹´åœ¨åŒ—äº¬å¤§å­¦ç¬¬ä¸€åŒ»é™¢ã€åŒ—äº¬åå’ŒåŒ»é™¢å’Œåº”æ€¥æ€»åŒ»é™¢å®Œæˆæ³¨å†Œä¸´åºŠè¯•éªŒã€‚æ£€æµ‹ç»“æœæ˜¾ç¤ºï¼Œä¸é™è„‰è¡€æ£€æµ‹ç›¸æ¯”ä¸€è‡´æ€§ä¸º93.9%ï¼›ä¸æŒ‡å°–è¡€æ£€æµ‹ç›¸æ¯”ä¸€è‡´æ€§ä¸º94.4%ã€‚è¿™äº›ç»“æœè¡¨æ˜è®¾å¤‡çš„æ£€æµ‹å‡†ç¡®åº¦ç¬¦åˆä¸´åºŠåº”ç”¨æ ‡å‡†ï¼Œèƒ½å¤Ÿæ»¡è¶³æ—¥å¸¸è¡€ç³–ç›‘æµ‹éœ€æ±‚ã€‚


ç°åœ¨è¯·å¯¹ä»¥ä¸‹æ¸…æ´—åçš„å®¢æœå¯¹è¯è¿›è¡Œé—®ç­”å¯¹æŠ½å–ï¼š

"""
        return prompt

    def extract_qa_pairs_from_text(self, dialogue_content: str, source_file: str = "unknown") -> Dict[str, Any]:
        """
        ä»å¯¹è¯æ–‡æœ¬ä¸­æŠ½å–é—®ç­”å¯¹

        Args:
            dialogue_content: æ¸…æ´—åçš„å¯¹è¯å†…å®¹
            source_file: æ¥æºæ–‡ä»¶å

        Returns:
            Dict[str, Any]: æŠ½å–ç»“æœ
        """
        try:
            # æ„å»ºå®Œæ•´çš„prompt
            full_prompt = self.get_qa_extraction_prompt() + "\n" + dialogue_content

            # è°ƒç”¨LLM API
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "user", "content": full_prompt}
                ],
                temperature=0.1,  # è¾ƒä½çš„æ¸©åº¦ç¡®ä¿ç¨³å®šè¾“å‡º
                max_tokens=32768,  # è¶³å¤Ÿçš„tokenæ•°é‡
            )

            result_text = response.choices[0].message.content.strip()

            # è§£æç®€åŒ–çš„QAå¯¹ç»“æœ
            qa_pairs = self._parse_simple_qa_response(result_text, source_file)

            if qa_pairs:
                return {
                    "success": True,
                    "qa_pairs": qa_pairs,
                    "summary": {"total_pairs": len(qa_pairs)},
                    "original_content": dialogue_content,
                    "source_file": source_file,
                    "token_usage": {
                        "prompt_tokens": response.usage.prompt_tokens,
                        "completion_tokens": response.usage.completion_tokens,
                        "total_tokens": response.usage.total_tokens
                    },
                    "raw_response": result_text
                }
            else:
                return {
                    "success": False,
                    "error": "æ— æ³•è§£æLLMå“åº”",
                    "raw_response": result_text,
                    "source_file": source_file
                }

        except Exception as e:
            self.logger.error(f"é—®ç­”å¯¹æŠ½å–å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "source_file": source_file
            }

    def _parse_simple_qa_response(self, response_text: str, source_file: str = "unknown") -> List[QAPair]:
        """
        è§£æç®€åŒ–çš„QAå¯¹å“åº”

        Args:
            response_text: LLMå“åº”æ–‡æœ¬

        Returns:
            List[QAPair]: è§£æåçš„QAå¯¹åˆ—è¡¨
        """
        qa_pairs = []

        try:
            # æŒ‰Q:å’ŒA:æ¨¡å¼è§£æ
            lines = response_text.strip().split('\n')
            current_q = None
            current_a = None

            for line in lines:
                line = line.strip()
                if not line:
                    continue

                if line.startswith('Q:'):
                    # å¦‚æœæœ‰æœªå®Œæˆçš„QAå¯¹ï¼Œå…ˆä¿å­˜
                    if current_q and current_a:
                        qa_pair = QAPair(
                            id=str(uuid.uuid4()),
                            question=current_q.strip(),
                            answer=current_a.strip(),
                            source_file=source_file,
                            timestamp=datetime.now(),
                            metadata={
                                'category': 'unknown',
                                'keywords': [],
                                'confidence': 0.9,
                                'extraction_method': 'simple_llm'
                            }
                        )
                        qa_pairs.append(qa_pair)

                    # å¼€å§‹æ–°çš„é—®é¢˜
                    current_q = line[2:].strip()
                    current_a = None

                elif line.startswith('A:'):
                    # è®°å½•ç­”æ¡ˆ
                    current_a = line[2:].strip()

                elif current_a is not None:
                    # ç»­æ¥ç­”æ¡ˆï¼ˆå¤šè¡Œç­”æ¡ˆæƒ…å†µï¼‰
                    current_a += " " + line
                elif current_q is not None:
                    # ç»­æ¥é—®é¢˜ï¼ˆå¤šè¡Œé—®é¢˜æƒ…å†µï¼‰
                    current_q += " " + line

            # å¤„ç†æœ€åä¸€ä¸ªQAå¯¹
            if current_q and current_a:
                qa_pair = QAPair(
                    id=str(uuid.uuid4()),
                    question=current_q.strip(),
                    answer=current_a.strip(),
                    source_file=source_file,
                    timestamp=datetime.now(),
                    metadata={
                        'category': 'unknown',
                        'keywords': [],
                        'confidence': 0.9,
                        'extraction_method': 'simple_llm'
                    }
                )
                qa_pairs.append(qa_pair)

            pass  # é™é»˜è§£æQAå¯¹
            return qa_pairs

        except Exception as e:
            self.logger.error(f"ç®€åŒ–QAå“åº”è§£æå¤±è´¥: {str(e)}")
            return []

    def extract_and_save_qa_pairs(self, input_file: str) -> Dict[str, Any]:
        """
        ä»æ–‡ä»¶ä¸­æŠ½å–é—®ç­”å¯¹å¹¶ä¿å­˜åˆ°çŸ¥è¯†åº“

        Args:
            input_file: è¾“å…¥çš„markdownæ–‡ä»¶è·¯å¾„

        Returns:
            Dict[str, Any]: å¤„ç†ç»“æœ
        """
        if not os.path.exists(input_file):
            return {
                "success": False,
                "error": f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}"
            }

        try:
            # è¯»å–å¯¹è¯å†…å®¹
            with open(input_file, 'r', encoding='utf-8') as f:
                dialogue_content = f.read()

            filename = os.path.basename(input_file)
            self.logger.info(f"å¼€å§‹ä» {filename} æŠ½å–é—®ç­”å¯¹...")

            # æŠ½å–é—®ç­”å¯¹
            extraction_result = self.extract_qa_pairs_from_text(dialogue_content, filename)

            if extraction_result["success"]:
                qa_pairs = extraction_result["qa_pairs"]

                if qa_pairs:
                    # ä¿å­˜åˆ°çŸ¥è¯†åº“
                    success = self.knowledge_base.append_qa_pairs(qa_pairs)

                    if success:
                        # æ›´æ–°æ–‡ä»¶çŠ¶æ€
                        from src.core.knowledge_base import ProcessingStatus
                        self.knowledge_base.update_file_status(
                            input_file,
                            ProcessingStatus.QA_EXTRACTED,
                            {
                                'qa_count': len(qa_pairs),
                                'extraction_time': datetime.now().isoformat(),
                                'token_usage': extraction_result.get('token_usage', {})
                            }
                        )

                        self.logger.info(f"âœ… æˆåŠŸæŠ½å–å¹¶ä¿å­˜ {len(qa_pairs)} ä¸ªé—®ç­”å¯¹")

                        return {
                            "success": True,
                            "input_file": input_file,
                            "qa_count": len(qa_pairs),
                            "summary": extraction_result.get('summary', {}),
                            "token_usage": extraction_result.get('token_usage', {}),
                            "qa_pairs": qa_pairs
                        }
                    else:
                        return {
                            "success": False,
                            "error": "ä¿å­˜é—®ç­”å¯¹åˆ°çŸ¥è¯†åº“å¤±è´¥",
                            "input_file": input_file
                        }
                else:
                    self.logger.warning(f"ä» {filename} ä¸­æœªæŠ½å–åˆ°ä»»ä½•é—®ç­”å¯¹")
                    return {
                        "success": True,
                        "input_file": input_file,
                        "qa_count": 0,
                        "warning": "æœªæŠ½å–åˆ°é—®ç­”å¯¹"
                    }
            else:
                return {
                    "success": False,
                    "error": extraction_result.get("error", "æŠ½å–å¤±è´¥"),
                    "input_file": input_file
                }

        except Exception as e:
            self.logger.error(f"å¤„ç†æ–‡ä»¶ {input_file} æ—¶å‡ºé”™: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "input_file": input_file
            }

    def batch_extract_qa_pairs(self, input_dir: str = "data/output/docs") -> Dict[str, Any]:
        """
        æ‰¹é‡æŠ½å–ç›®å½•ä¸‹æ‰€æœ‰æ¸…æ´—å®Œæˆæ–‡ä»¶çš„é—®ç­”å¯¹

        Args:
            input_dir: è¾“å…¥ç›®å½•è·¯å¾„

        Returns:
            Dict[str, Any]: æ‰¹é‡å¤„ç†ç»“æœç»Ÿè®¡
        """
        if not os.path.exists(input_dir):
            return {
                "success": False,
                "error": f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}"
            }

        # è·å–æ‰€æœ‰æ¸…æ´—å®Œæˆçš„æ–‡ä»¶
        clean_finished_files = self.knowledge_base.get_clean_finished_files()

        if not clean_finished_files:
            self.logger.warning("æ²¡æœ‰æ‰¾åˆ°çŠ¶æ€ä¸ºclean_finishedçš„æ–‡ä»¶")
            return {
                "success": False,
                "error": "æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„æ–‡ä»¶"
            }

        self.logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡é—®ç­”å¯¹æŠ½å–ï¼Œå‘ç° {len(clean_finished_files)} ä¸ªå·²æ¸…æ´—æ–‡ä»¶")

        results = []
        success_count = 0
        error_count = 0
        total_qa_pairs = 0
        total_tokens = 0

        for file_path in clean_finished_files:
            self.logger.info(f"\nğŸ“„ å¤„ç†æ–‡ä»¶: {os.path.basename(file_path)}")

            result = self.extract_and_save_qa_pairs(file_path)
            results.append(result)

            if result["success"]:
                success_count += 1
                qa_count = result.get("qa_count", 0)
                total_qa_pairs += qa_count

                token_usage = result.get("token_usage", {})
                total_tokens += token_usage.get("total_tokens", 0)

                self.logger.info(f"âœ… æˆåŠŸæŠ½å– {qa_count} ä¸ªé—®ç­”å¯¹")
            else:
                error_count += 1
                self.logger.error(f"âŒ å¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

        # ç»Ÿè®¡æ€»ç»“
        self.logger.info(f"\nğŸ‰ æ‰¹é‡é—®ç­”å¯¹æŠ½å–å®Œæˆï¼")
        self.logger.info(f"âœ… æˆåŠŸ: {success_count} ä¸ªæ–‡ä»¶")
        self.logger.info(f"âŒ å¤±è´¥: {error_count} ä¸ªæ–‡ä»¶")
        self.logger.info(f"ğŸ“Š æ€»è®¡æŠ½å–: {total_qa_pairs} ä¸ªé—®ç­”å¯¹")
        self.logger.info(f"ğŸ”¢ æ€»è®¡æ¶ˆè€—: {total_tokens} tokens")

        # è·å–çŸ¥è¯†åº“ç»Ÿè®¡
        kb_stats = self.knowledge_base.get_statistics()

        return {
            "success": True,
            "total_files": len(clean_finished_files),
            "success_count": success_count,
            "error_count": error_count,
            "total_qa_pairs": total_qa_pairs,
            "total_tokens": total_tokens,
            "results": results,
            "knowledge_base_stats": kb_stats
        }

    def get_extraction_statistics(self) -> Dict[str, Any]:
        """
        è·å–é—®ç­”å¯¹æŠ½å–ç»Ÿè®¡ä¿¡æ¯

        Returns:
            Dict[str, Any]: ç»Ÿè®¡ä¿¡æ¯
        """
        # è·å–çŸ¥è¯†åº“ç»Ÿè®¡
        kb_stats = self.knowledge_base.get_statistics()

        # ç»Ÿè®¡å„ç§çŠ¶æ€çš„æ–‡ä»¶æ•°é‡
        status_counts = {}
        for file_path, file_status in self.knowledge_base.file_status_map.items():
            status = file_status.status.value
            status_counts[status] = status_counts.get(status, 0) + 1

        return {
            "knowledge_base_stats": kb_stats,
            "file_status_counts": status_counts,
            "ready_for_extraction": len(self.knowledge_base.get_clean_finished_files()),
            "total_tracked_files": len(self.knowledge_base.file_status_map)
        }