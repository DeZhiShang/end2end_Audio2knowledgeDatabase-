"""
é—®ç­”å¯¹æŠ½å–æ¨¡å—
ä»æ¸…æ´—åçš„å®¢æœå¯¹è¯ä¸­æå–é«˜è´¨é‡é—®ç­”å¯¹
"""

import os
import json
import uuid
from typing import Dict, List, Any, Optional
from datetime import datetime
import re

try:
    import openai
except ImportError:
    print("è­¦å‘Š: openaiåŒ…æœªå®‰è£…ï¼Œè¯·è¿è¡Œ pip install openai")
    openai = None

try:
    from dotenv import load_dotenv
except ImportError:
    print("è­¦å‘Š: python-dotenvåŒ…æœªå®‰è£…ï¼Œè¯·è¿è¡Œ pip install python-dotenv")
    def load_dotenv():
        pass

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

from src.utils.logger import get_logger
from src.core.knowledge_base import QAPair, get_knowledge_base


class QAExtractor:
    """é—®ç­”å¯¹æŠ½å–å™¨ï¼šä»æ¸…æ´—åçš„å¯¹è¯ä¸­æå–é—®ç­”å¯¹"""

    def __init__(self):
        """åˆå§‹åŒ–é—®ç­”å¯¹æŠ½å–å™¨"""
        self.logger = get_logger(__name__)

        if openai is None:
            raise ImportError("è¯·å…ˆå®‰è£…openaiåŒ…: pip install openai")

        self.api_key = os.getenv('DASHSCOPE_API_KEY')
        self.base_url = os.getenv('DASHSCOPE_BASE_URL')

        if not self.api_key or not self.base_url:
            raise ValueError("è¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®DASHSCOPE_API_KEYå’ŒDASHSCOPE_BASE_URL")

        # é…ç½®OpenAIå®¢æˆ·ç«¯ï¼ˆå…¼å®¹é˜¿é‡Œäº‘DashScope APIï¼‰
        self.client = openai.OpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )

        self.model_name = "qwen-plus-latest"

        # è·å–çŸ¥è¯†åº“å®ä¾‹
        self.knowledge_base = get_knowledge_base()

    def get_qa_extraction_prompt(self) -> str:
        """
        è·å–é—®ç­”å¯¹æŠ½å–çš„promptæ¨¡æ¿

        Returns:
            str: å®Œæ•´çš„promptæ¨¡æ¿
        """
        prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†åº“æ„å»ºä¸“å®¶ï¼Œä¸“é—¨ä»åšé‚¦æ–¹èˆŸæ— åˆ›è¡€ç³–ä»ªå®¢æœå¯¹è¯ä¸­æŠ½å–é«˜è´¨é‡é—®ç­”å¯¹ã€‚

## ä»»åŠ¡èƒŒæ™¯
- ç›®æ ‡ï¼šæ„å»ºåšé‚¦æ–¹èˆŸæ— åˆ›è¡€ç³–ä»ªçš„ä¸“ä¸šçŸ¥è¯†åº“
- è¾“å…¥ï¼šå·²ç»LLMæ¸…æ´—è¿‡çš„é«˜è´¨é‡å®¢æœå¯¹è¯
- è¾“å‡ºï¼šç»“æ„åŒ–çš„é—®ç­”å¯¹ï¼Œç”¨äºçŸ¥è¯†åº“å»ºè®¾

## æŠ½å–åŸåˆ™

### æ ¸å¿ƒè¦æ±‚
1. **å…¨é¢æ€§**ï¼šç¡®ä¿ä¸é—æ¼ä»»ä½•æœ‰ä»·å€¼çš„é—®ç­”ä¿¡æ¯
2. **å‡†ç¡®æ€§**ï¼šåŸºäºå¯¹è¯äº‹å®ï¼Œä¸å¾—ç¼–é€ æˆ–æ·»åŠ ä¸å­˜åœ¨çš„å†…å®¹
3. **ä¸“ä¸šæ€§**ï¼šé—®ç­”å¯¹åº”ä½“ç°ä¸“ä¸šå®¢æœæ°´å‡†
4. **å®ç”¨æ€§**ï¼šé—®ç­”å¯¹åº”å¯¹å®é™…ç”¨æˆ·æœ‰å¸®åŠ©

### é—®ç­”å¯¹è´¨é‡æ ‡å‡†
1. **é—®é¢˜æ˜ç¡®**ï¼šé—®é¢˜è¡¨è¿°æ¸…æ™°ã€å…·ä½“ã€æ˜“ç†è§£
2. **ç­”æ¡ˆå®Œæ•´**ï¼šç­”æ¡ˆå‡†ç¡®ã€è¯¦ç»†ã€æœ‰å¸®åŠ©
3. **é€»è¾‘æ¸…æ™°**ï¼šé—®ç­”é€»è¾‘å¯¹åº”ï¼Œé¿å…ç­”éæ‰€é—®
4. **è¯­è¨€è§„èŒƒ**ï¼šä½¿ç”¨æ ‡å‡†ã€ä¸“ä¸šçš„è¡¨è¾¾æ–¹å¼

## æŠ½å–ç­–ç•¥

### 1. ç›´æ¥é—®ç­”å¯¹
- ç”¨æˆ·æ˜ç¡®æé—®ï¼Œå®¢æœç›´æ¥å›ç­”çš„å†…å®¹
- ä¿æŒé—®é¢˜çš„åŸå§‹æ„å›¾ï¼Œä¼˜åŒ–è¯­è¨€è¡¨è¾¾

### 2. éšå«é—®ç­”å¯¹
- ä»å¯¹è¯ä¸­æ¨å¯¼å‡ºçš„å¸¸è§é—®é¢˜å’Œç­”æ¡ˆ
- åŸºäºç”¨æˆ·éœ€æ±‚å’Œå®¢æœè§£é‡Šç”Ÿæˆé—®ç­”å¯¹

### 3. çŸ¥è¯†ç‚¹é—®ç­”å¯¹
- å®¢æœä¸»åŠ¨ä»‹ç»çš„äº§å“çŸ¥è¯†ç‚¹
- è½¬æ¢ä¸ºé—®ç­”å½¢å¼ï¼Œä¾¿äºçŸ¥è¯†åº“æŸ¥è¯¢

### 4. é—®é¢˜ç»†åˆ†
- å¤æ‚é—®é¢˜æ‹†åˆ†ä¸ºå¤šä¸ªç®€å•é—®ç­”å¯¹
- æ¯ä¸ªé—®ç­”å¯¹èšç„¦ä¸€ä¸ªå…·ä½“çŸ¥è¯†ç‚¹

## é¢†åŸŸèŒƒå›´
éœ€è¦æ¶µç›–ä½†ä¸é™äºä»¥ä¸‹é¢†åŸŸï¼š

### äº§å“åŸºç¡€ä¿¡æ¯
- äº§å“ä»‹ç»ã€åŠŸèƒ½ç‰¹ç‚¹ã€æŠ€æœ¯åŸç†
- äº§å“è§„æ ¼ã€å‹å·ã€é…ç½®ä¿¡æ¯
- é€‚ç”¨äººç¾¤ã€ä½¿ç”¨åœºæ™¯

### ä½¿ç”¨æ“ä½œ
- å¼€æœºè®¾ç½®ã€åŸºæœ¬æ“ä½œæµç¨‹
- æµ‹é‡æ–¹æ³•ã€æ³¨æ„äº‹é¡¹
- æ•°æ®æŸ¥çœ‹ã€è®°å½•ç®¡ç†

### æ•…éšœè§£å†³
- å¸¸è§é—®é¢˜ã€æ•…éšœç°è±¡
- è§£å†³æ–¹æ³•ã€æ“ä½œæ­¥éª¤
- é¢„é˜²æªæ–½ã€ç»´æŠ¤å»ºè®®

### è´­ä¹°å’¨è¯¢
- ä»·æ ¼ä¿¡æ¯ã€è´­ä¹°æ¸ é“
- ä¼˜æƒ æ´»åŠ¨ã€ä¿ƒé”€æ”¿ç­–
- å”®åæœåŠ¡ã€è´¨ä¿ä¿¡æ¯

### æŠ€æœ¯æ”¯æŒ
- äº§å“åŸç†ã€æŠ€æœ¯ç»†èŠ‚
- ä¸å…¶ä»–è®¾å¤‡å¯¹æ¯”
- ä¸“ä¸šæœ¯è¯­è§£é‡Š

## è¾“å‡ºæ ¼å¼

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºæŠ½å–ç»“æœï¼š

```json
{
    "qa_pairs": [
        {
            "question": "å…·ä½“é—®é¢˜å†…å®¹",
            "answer": "è¯¦ç»†ç­”æ¡ˆå†…å®¹",
            "category": "é—®é¢˜åˆ†ç±»",
            "keywords": ["å…³é”®è¯1", "å…³é”®è¯2"],
            "confidence": 0.95,
            "source_context": "ç›¸å…³å¯¹è¯ç‰‡æ®µ"
        }
    ],
    "summary": {
        "total_pairs": 5,
        "categories": ["äº§å“ä»‹ç»", "ä½¿ç”¨æ“ä½œ"],
        "extraction_notes": "æŠ½å–è¿‡ç¨‹è¯´æ˜"
    }
}
```

### å­—æ®µè¯´æ˜
- **question**: æ ‡å‡†åŒ–åçš„é—®é¢˜ï¼Œæ¸…æ™°æ˜“æ‡‚
- **answer**: å®Œæ•´å‡†ç¡®çš„ç­”æ¡ˆï¼ŒåŒ…å«å¿…è¦ç»†èŠ‚
- **category**: é—®é¢˜åˆ†ç±»ï¼ˆäº§å“ä»‹ç»/ä½¿ç”¨æ“ä½œ/æ•…éšœè§£å†³/è´­ä¹°å’¨è¯¢/æŠ€æœ¯æ”¯æŒï¼‰
- **keywords**: 3-5ä¸ªå…³é”®è¯ï¼Œç”¨äºæœç´¢å’Œåˆ†ç±»
- **confidence**: æŠ½å–ç½®ä¿¡åº¦ï¼ˆ0.0-1.0ï¼‰ï¼Œå»ºè®®>0.8çš„é—®ç­”å¯¹
- **source_context**: åŸå§‹å¯¹è¯ä¸­çš„ç›¸å…³ç‰‡æ®µï¼Œç”¨äºæº¯æº

## æ³¨æ„äº‹é¡¹

### è´¨é‡æ§åˆ¶
1. **ç½®ä¿¡åº¦è¦æ±‚**ï¼šåªè¾“å‡ºç½®ä¿¡åº¦â‰¥0.8çš„é—®ç­”å¯¹
2. **å»é‡å¤„ç†**ï¼šé¿å…é‡å¤æˆ–é«˜åº¦ç›¸ä¼¼çš„é—®ç­”å¯¹
3. **å®Œæ•´æ€§æ£€æŸ¥**ï¼šç¡®ä¿ç­”æ¡ˆå®Œæ•´ï¼Œé¿å…æˆªæ–­
4. **ä¸“ä¸šæ€§éªŒè¯**ï¼šç­”æ¡ˆåº”ä½“ç°ä¸“ä¸šå®¢æœæ°´å‡†

### è¯­è¨€è§„èŒƒ
1. **æ ‡å‡†è¡¨è¾¾**ï¼šä½¿ç”¨è§„èŒƒçš„äº§å“åç§°å’Œæœ¯è¯­
2. **ç”¨æˆ·è§†è§’**ï¼šé—®é¢˜ä»ç”¨æˆ·è§’åº¦è¡¨è¿°
3. **æœåŠ¡è¯­è°ƒ**ï¼šç­”æ¡ˆä¿æŒä¸“ä¸šå‹å¥½çš„æœåŠ¡è¯­è°ƒ
4. **é¿å…å£è¯­åŒ–**ï¼šå‡å°‘"å—¯"ã€"å“¦"ç­‰å£è¯­åŒ–è¡¨è¾¾

## ç¤ºä¾‹

è¾“å…¥å¯¹è¯ï¼š
```
**SPEAKER_00**: ä½ å¥½ï¼Œæˆ‘æƒ³äº†è§£ä¸€ä¸‹åšé‚¦æ–¹èˆŸæ— åˆ›è¡€ç³–ä»ªçš„å·¥ä½œåŸç†
**SPEAKER_01**: æ‚¨å¥½ï¼åšé‚¦æ–¹èˆŸæ— åˆ›è¡€ç³–ä»ªé‡‡ç”¨å…ˆè¿›çš„å…‰è°±æ£€æµ‹æŠ€æœ¯ï¼Œé€šè¿‡ç…§å°„æ‰‹æŒ‡è·å–è¡€ç³–æ•°æ®ï¼Œæ— éœ€é‡‡è¡€ï¼Œä½¿ç”¨éå¸¸æ–¹ä¾¿
**SPEAKER_00**: é‚£å‡†ç¡®åº¦æ€ä¹ˆæ ·ï¼Ÿ
**SPEAKER_01**: æˆ‘ä»¬çš„è®¾å¤‡å‡†ç¡®åº¦å¾ˆé«˜ï¼Œä¸ä¼ ç»Ÿè¡€ç³–ä»ªç›¸æ¯”ï¼Œè¯¯å·®æ§åˆ¶åœ¨10%ä»¥å†…ï¼Œå®Œå…¨æ»¡è¶³æ—¥å¸¸ç›‘æµ‹éœ€æ±‚
```

è¾“å‡ºç¤ºä¾‹ï¼š
```json
{
    "qa_pairs": [
        {
            "question": "åšé‚¦æ–¹èˆŸæ— åˆ›è¡€ç³–ä»ªçš„å·¥ä½œåŸç†æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "åšé‚¦æ–¹èˆŸæ— åˆ›è¡€ç³–ä»ªé‡‡ç”¨å…ˆè¿›çš„å…‰è°±æ£€æµ‹æŠ€æœ¯ï¼Œé€šè¿‡ç…§å°„æ‰‹æŒ‡è·å–è¡€ç³–æ•°æ®ï¼Œæ— éœ€é‡‡è¡€ï¼Œä½¿ç”¨éå¸¸æ–¹ä¾¿ã€‚",
            "category": "æŠ€æœ¯æ”¯æŒ",
            "keywords": ["å·¥ä½œåŸç†", "å…‰è°±æ£€æµ‹", "æ— åˆ›æµ‹é‡"],
            "confidence": 0.95,
            "source_context": "SPEAKER_01: æ‚¨å¥½ï¼åšé‚¦æ–¹èˆŸæ— åˆ›è¡€ç³–ä»ªé‡‡ç”¨å…ˆè¿›çš„å…‰è°±æ£€æµ‹æŠ€æœ¯..."
        },
        {
            "question": "åšé‚¦æ–¹èˆŸæ— åˆ›è¡€ç³–ä»ªçš„æµ‹é‡å‡†ç¡®åº¦å¦‚ä½•ï¼Ÿ",
            "answer": "åšé‚¦æ–¹èˆŸæ— åˆ›è¡€ç³–ä»ªå‡†ç¡®åº¦å¾ˆé«˜ï¼Œä¸ä¼ ç»Ÿè¡€ç³–ä»ªç›¸æ¯”ï¼Œè¯¯å·®æ§åˆ¶åœ¨10%ä»¥å†…ï¼Œå®Œå…¨æ»¡è¶³æ—¥å¸¸ç›‘æµ‹éœ€æ±‚ã€‚",
            "category": "äº§å“ä»‹ç»",
            "keywords": ["å‡†ç¡®åº¦", "è¯¯å·®èŒƒå›´", "æ—¥å¸¸ç›‘æµ‹"],
            "confidence": 0.92,
            "source_context": "SPEAKER_01: æˆ‘ä»¬çš„è®¾å¤‡å‡†ç¡®åº¦å¾ˆé«˜ï¼Œä¸ä¼ ç»Ÿè¡€ç³–ä»ªç›¸æ¯”..."
        }
    ],
    "summary": {
        "total_pairs": 2,
        "categories": ["æŠ€æœ¯æ”¯æŒ", "äº§å“ä»‹ç»"],
        "extraction_notes": "æˆåŠŸæŠ½å–äº§å“åŸç†å’Œå‡†ç¡®åº¦ç›¸å…³é—®ç­”å¯¹"
    }
}
```

ç°åœ¨è¯·å¯¹ä»¥ä¸‹æ¸…æ´—åçš„å®¢æœå¯¹è¯è¿›è¡Œé—®ç­”å¯¹æŠ½å–ï¼š

"""
        return prompt

    def extract_qa_pairs_from_text(self, dialogue_content: str, source_file: str = "unknown") -> Dict[str, Any]:
        """
        ä»å¯¹è¯æ–‡æœ¬ä¸­æŠ½å–é—®ç­”å¯¹

        Args:
            dialogue_content: æ¸…æ´—åçš„å¯¹è¯å†…å®¹
            source_file: æ¥æºæ–‡ä»¶å

        Returns:
            Dict[str, Any]: æŠ½å–ç»“æœ
        """
        try:
            # æ„å»ºå®Œæ•´çš„prompt
            full_prompt = self.get_qa_extraction_prompt() + "\n" + dialogue_content

            # è°ƒç”¨LLM API
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "user", "content": full_prompt}
                ],
                temperature=0.1,  # è¾ƒä½çš„æ¸©åº¦ç¡®ä¿ç¨³å®šè¾“å‡º
                max_tokens=4000,  # è¶³å¤Ÿçš„tokenæ•°é‡
            )

            result_text = response.choices[0].message.content.strip()

            # è§£æJSONç»“æœ
            qa_data = self._parse_qa_response(result_text)

            if qa_data:
                # åˆ›å»ºQAPairå¯¹è±¡
                qa_pairs = []
                for qa_info in qa_data.get('qa_pairs', []):
                    qa_pair = QAPair(
                        id=str(uuid.uuid4()),
                        question=qa_info['question'],
                        answer=qa_info['answer'],
                        source_file=source_file,
                        timestamp=datetime.now(),
                        metadata={
                            'category': qa_info.get('category', 'unknown'),
                            'keywords': qa_info.get('keywords', []),
                            'confidence': qa_info.get('confidence', 0.8),
                            'source_context': qa_info.get('source_context', ''),
                            'extraction_method': 'llm_auto'
                        }
                    )
                    qa_pairs.append(qa_pair)

                return {
                    "success": True,
                    "qa_pairs": qa_pairs,
                    "summary": qa_data.get('summary', {}),
                    "original_content": dialogue_content,
                    "source_file": source_file,
                    "token_usage": {
                        "prompt_tokens": response.usage.prompt_tokens,
                        "completion_tokens": response.usage.completion_tokens,
                        "total_tokens": response.usage.total_tokens
                    },
                    "raw_response": result_text
                }
            else:
                return {
                    "success": False,
                    "error": "æ— æ³•è§£æLLMå“åº”",
                    "raw_response": result_text,
                    "source_file": source_file
                }

        except Exception as e:
            self.logger.error(f"é—®ç­”å¯¹æŠ½å–å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "source_file": source_file
            }

    def _parse_qa_response(self, response_text: str) -> Optional[Dict[str, Any]]:
        """
        è§£æLLMè¿”å›çš„é—®ç­”å¯¹å“åº”

        Args:
            response_text: LLMå“åº”æ–‡æœ¬

        Returns:
            Dict[str, Any]: è§£æåçš„æ•°æ®ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            # å°è¯•æå–JSONéƒ¨åˆ†
            json_pattern = r'```json\n(.*?)\n```'
            json_match = re.search(json_pattern, response_text, re.DOTALL)

            if json_match:
                json_str = json_match.group(1)
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»£ç å—ï¼Œå°è¯•æŸ¥æ‰¾JSONå¯¹è±¡
                json_pattern = r'\{.*\}'
                json_match = re.search(json_pattern, response_text, re.DOTALL)
                if json_match:
                    json_str = json_match.group(0)
                else:
                    self.logger.warning("æ— æ³•åœ¨å“åº”ä¸­æ‰¾åˆ°JSONæ ¼å¼æ•°æ®")
                    return None

            # è§£æJSON
            qa_data = json.loads(json_str)

            # éªŒè¯å¿…è¦å­—æ®µ
            if 'qa_pairs' not in qa_data:
                self.logger.warning("å“åº”ä¸­ç¼ºå°‘qa_pairså­—æ®µ")
                return None

            # è¿‡æ»¤ä½ç½®ä¿¡åº¦çš„é—®ç­”å¯¹
            filtered_pairs = []
            for qa in qa_data['qa_pairs']:
                confidence = qa.get('confidence', 0.8)
                if confidence >= 0.8:  # åªä¿ç•™é«˜ç½®ä¿¡åº¦çš„é—®ç­”å¯¹
                    filtered_pairs.append(qa)
                else:
                    self.logger.info(f"è¿‡æ»¤ä½ç½®ä¿¡åº¦é—®ç­”å¯¹: {confidence:.2f}")

            qa_data['qa_pairs'] = filtered_pairs

            return qa_data

        except json.JSONDecodeError as e:
            self.logger.error(f"JSONè§£æå¤±è´¥: {str(e)}")
            return None
        except Exception as e:
            self.logger.error(f"å“åº”è§£æå¤±è´¥: {str(e)}")
            return None

    def extract_and_save_qa_pairs(self, input_file: str) -> Dict[str, Any]:
        """
        ä»æ–‡ä»¶ä¸­æŠ½å–é—®ç­”å¯¹å¹¶ä¿å­˜åˆ°çŸ¥è¯†åº“

        Args:
            input_file: è¾“å…¥çš„markdownæ–‡ä»¶è·¯å¾„

        Returns:
            Dict[str, Any]: å¤„ç†ç»“æœ
        """
        if not os.path.exists(input_file):
            return {
                "success": False,
                "error": f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}"
            }

        try:
            # è¯»å–å¯¹è¯å†…å®¹
            with open(input_file, 'r', encoding='utf-8') as f:
                dialogue_content = f.read()

            filename = os.path.basename(input_file)
            self.logger.info(f"å¼€å§‹ä» {filename} æŠ½å–é—®ç­”å¯¹...")

            # æŠ½å–é—®ç­”å¯¹
            extraction_result = self.extract_qa_pairs_from_text(dialogue_content, filename)

            if extraction_result["success"]:
                qa_pairs = extraction_result["qa_pairs"]

                if qa_pairs:
                    # ä¿å­˜åˆ°çŸ¥è¯†åº“
                    success = self.knowledge_base.append_qa_pairs(qa_pairs)

                    if success:
                        # æ›´æ–°æ–‡ä»¶çŠ¶æ€
                        from src.core.knowledge_base import ProcessingStatus
                        self.knowledge_base.update_file_status(
                            input_file,
                            ProcessingStatus.QA_EXTRACTED,
                            {
                                'qa_count': len(qa_pairs),
                                'extraction_time': datetime.now().isoformat(),
                                'token_usage': extraction_result.get('token_usage', {})
                            }
                        )

                        self.logger.info(f"âœ… æˆåŠŸæŠ½å–å¹¶ä¿å­˜ {len(qa_pairs)} ä¸ªé—®ç­”å¯¹")

                        return {
                            "success": True,
                            "input_file": input_file,
                            "qa_count": len(qa_pairs),
                            "summary": extraction_result.get('summary', {}),
                            "token_usage": extraction_result.get('token_usage', {}),
                            "qa_pairs": qa_pairs
                        }
                    else:
                        return {
                            "success": False,
                            "error": "ä¿å­˜é—®ç­”å¯¹åˆ°çŸ¥è¯†åº“å¤±è´¥",
                            "input_file": input_file
                        }
                else:
                    self.logger.warning(f"ä» {filename} ä¸­æœªæŠ½å–åˆ°ä»»ä½•é—®ç­”å¯¹")
                    return {
                        "success": True,
                        "input_file": input_file,
                        "qa_count": 0,
                        "warning": "æœªæŠ½å–åˆ°é—®ç­”å¯¹"
                    }
            else:
                return {
                    "success": False,
                    "error": extraction_result.get("error", "æŠ½å–å¤±è´¥"),
                    "input_file": input_file
                }

        except Exception as e:
            self.logger.error(f"å¤„ç†æ–‡ä»¶ {input_file} æ—¶å‡ºé”™: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "input_file": input_file
            }

    def batch_extract_qa_pairs(self, input_dir: str = "data/output/docs") -> Dict[str, Any]:
        """
        æ‰¹é‡æŠ½å–ç›®å½•ä¸‹æ‰€æœ‰æ¸…æ´—å®Œæˆæ–‡ä»¶çš„é—®ç­”å¯¹

        Args:
            input_dir: è¾“å…¥ç›®å½•è·¯å¾„

        Returns:
            Dict[str, Any]: æ‰¹é‡å¤„ç†ç»“æœç»Ÿè®¡
        """
        if not os.path.exists(input_dir):
            return {
                "success": False,
                "error": f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}"
            }

        # è·å–æ‰€æœ‰æ¸…æ´—å®Œæˆçš„æ–‡ä»¶
        clean_finished_files = self.knowledge_base.get_clean_finished_files()

        if not clean_finished_files:
            self.logger.warning("æ²¡æœ‰æ‰¾åˆ°çŠ¶æ€ä¸ºclean_finishedçš„æ–‡ä»¶")
            return {
                "success": False,
                "error": "æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„æ–‡ä»¶"
            }

        self.logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡é—®ç­”å¯¹æŠ½å–ï¼Œå‘ç° {len(clean_finished_files)} ä¸ªå·²æ¸…æ´—æ–‡ä»¶")

        results = []
        success_count = 0
        error_count = 0
        total_qa_pairs = 0
        total_tokens = 0

        for file_path in clean_finished_files:
            self.logger.info(f"\nğŸ“„ å¤„ç†æ–‡ä»¶: {os.path.basename(file_path)}")

            result = self.extract_and_save_qa_pairs(file_path)
            results.append(result)

            if result["success"]:
                success_count += 1
                qa_count = result.get("qa_count", 0)
                total_qa_pairs += qa_count

                token_usage = result.get("token_usage", {})
                total_tokens += token_usage.get("total_tokens", 0)

                self.logger.info(f"âœ… æˆåŠŸæŠ½å– {qa_count} ä¸ªé—®ç­”å¯¹")
            else:
                error_count += 1
                self.logger.error(f"âŒ å¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

        # ç»Ÿè®¡æ€»ç»“
        self.logger.info(f"\nğŸ‰ æ‰¹é‡é—®ç­”å¯¹æŠ½å–å®Œæˆï¼")
        self.logger.info(f"âœ… æˆåŠŸ: {success_count} ä¸ªæ–‡ä»¶")
        self.logger.info(f"âŒ å¤±è´¥: {error_count} ä¸ªæ–‡ä»¶")
        self.logger.info(f"ğŸ“Š æ€»è®¡æŠ½å–: {total_qa_pairs} ä¸ªé—®ç­”å¯¹")
        self.logger.info(f"ğŸ”¢ æ€»è®¡æ¶ˆè€—: {total_tokens} tokens")

        # è·å–çŸ¥è¯†åº“ç»Ÿè®¡
        kb_stats = self.knowledge_base.get_statistics()

        return {
            "success": True,
            "total_files": len(clean_finished_files),
            "success_count": success_count,
            "error_count": error_count,
            "total_qa_pairs": total_qa_pairs,
            "total_tokens": total_tokens,
            "results": results,
            "knowledge_base_stats": kb_stats
        }

    def get_extraction_statistics(self) -> Dict[str, Any]:
        """
        è·å–é—®ç­”å¯¹æŠ½å–ç»Ÿè®¡ä¿¡æ¯

        Returns:
            Dict[str, Any]: ç»Ÿè®¡ä¿¡æ¯
        """
        # è·å–çŸ¥è¯†åº“ç»Ÿè®¡
        kb_stats = self.knowledge_base.get_statistics()

        # ç»Ÿè®¡å„ç§çŠ¶æ€çš„æ–‡ä»¶æ•°é‡
        status_counts = {}
        for file_path, file_status in self.knowledge_base.file_status_map.items():
            status = file_status.status.value
            status_counts[status] = status_counts.get(status, 0) + 1

        return {
            "knowledge_base_stats": kb_stats,
            "file_status_counts": status_counts,
            "ready_for_extraction": len(self.knowledge_base.get_clean_finished_files()),
            "total_tracked_files": len(self.knowledge_base.file_status_map)
        }